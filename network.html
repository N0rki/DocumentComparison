<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 600px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"group": "Natural Language Processing", "id": 0, "label": "HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning", "shape": "dot", "size": 10, "title": "Authors: Hanning Chen, Yang Ni, Ali Zakeri, Zhuowen Zou, Sanggeon Yun, Fei Wen, Khaleghi, Narayan Srinivasa, Hugo Latapie, Mohsen Imani\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In recent times, a plethora of hardware accelerators have\nbeen put forth for graph learning applications such as vertex\nclassification and graph classification. However, previous\nworks have paid little attention to Knowledge Graph Com-\npletion (KGC), a task that is well-known for its significantly\nhigher algorithm complexity. The state-of-the-art KGC so-\nlutions based on graph convolution neural network (GCN)\ninvolve extensive vertex/relation embedding updates and\ncomplicated score functions, which are inherently cumber-\nsome for acceleration. As a result, existing accelerator de-\nsigns are no longer optimal, and a novel algorithm-hardware\nco-design for KG reasoning is needed.\nRecently, brain-inspired HyperDimensional Computing\n(HDC) has been introduced as a promising solution for light-\nweight machine learning, particularly for graph learning\napplications. In this paper, we leverage HDC for an intrin-\nsically more efficient and acceleration-friendly KGC algo-\nrithm. We also co-design an acceleration framework named\nHDReason targeting FPGA platforms. On the algorithm level,\nHDReason achieves a balance between high reasoning ac-\ncuracy, strong model interpretability, and less computation\ncomplexity. In terms of architecture, HDReason offers re-\nconfigurability, high training throughput, and low energy\nconsumption. When compared with NVIDIA RTX 4090 GPU,\nthe proposed accelerator achieves an average 10.6\u00d7 speedup\nand 65\u00d7 energy efficiency improvement. When conducting\ncross-models and cross-platforms comparison, HDReason\nyields an average 4.2\u00d7 higher performance and 3.4\u00d7 better\nenergy efficiency with similar accuracy versus the state-of-\nthe-art FPGA-based GCN training platform."}, {"group": "History of Science", "id": 2, "label": "MicroT: Low-Energy and Adaptive Models for MCUs", "shape": "dot", "size": 10, "title": "Authors: Yushan Huang, Ranya Aloufi, Yuchen Zhao, Payam Barnaghi, Hamed Haddadi\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: We propose MicroT, a low-energy, multi-task adaptive model\nframework for resource-constrained MCUs. We divide the\noriginal model into a feature extractor and a classifier. The\nfeature extractor is obtained through self-supervised knowl-\nedge distillation and further optimized into part and full mod-\nels through model splitting and joint training. These mod-\nels are then deployed on MCUs, with classifiers added and\ntrained on local tasks, ultimately performing stage-decision\nfor joint inference. In this process, the part model initially\nprocesses the sample, and if the confidence score falls below\nthe set threshold, the full model will resume and continue the\ninference. We evaluate MicroT on two models, three datasets,\nand two MCU boards. Our experimental evaluation shows\nthat MicroT effectively improves model performance and\nreduces energy consumption when dealing with multiple\nlocal tasks. Compared to the unoptimized feature extractor,\nMicroT can improve accuracy by up to 9.87%. On MCUs,\ncompared to the standard full model inference, MicroT can\nsave up to about 29.13% in energy consumption. MicroT also\nallows users to adaptively adjust the stage-decision ratio\nas needed, better balancing model performance and energy\nconsumption. Under the standard stage-decision ratio con-\nfiguration, MicroT can increase accuracy by 5.91% and save\nabout 14.47% of energy consumption.\nCCS Concepts\n\u2022 Computer systems organization \u2192Embedded sys-\ntems; \u2022 Computing methodologies \u2192Artificial intelli-\ngence; Machine learning.\nKeywords\nResource Constraints, Machine Learning, Low-Energy, MCUs"}, {"group": "Natural Language Processing", "id": 14, "label": "Missed Connections: Lateral Thinking Puzzles for Large Language Models", "shape": "dot", "size": 10, "title": "Authors: Graham Todd, Tim Merino, Sam Earle, Julian Togelius\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014The Connections puzzle published each day by the\nNew York Times tasks players with dividing a bank of sixteen\nwords into four groups of four words that each relate to\na common theme. Solving the puzzle requires both common\nlinguistic knowledge (i.e. definitions and typical usage) as well as,\nin many cases, lateral or abstract thinking. This is because the\nfour categories ascend in complexity, with the most challenging\ncategory often requiring thinking about words in uncommon\nways or as parts of larger phrases. We investigate the capacity for\nautomated AI systems to play Connections and explore the game\u2019s\npotential as an automated benchmark for abstract reasoning\nand a way to measure the semantic information encoded by\ndata-driven linguistic systems. In particular, we study both a\nsentence-embedding baseline and modern large language models\n(LLMs). We report their accuracy on the task, measure the\nimpacts of chain-of-thought prompting, and discuss their failure\nmodes. Overall, we find that the Connections task is challenging\nyet feasible, and a strong test-bed for future work.\nIndex Terms\u2014Language models, reasoning, AI, evaluation\nI."}, {"group": "Natural Language Processing", "id": 22, "label": "P-NAL: an Effective and Interpretable Entity Alignment Method", "shape": "dot", "size": 10, "title": "Authors: Jingwei Cheng, Zhg\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Entity alignment (EA) aims to find equivalent entities between two\nKnowledge Graphs. Existing embedding-based EA methods usually\nencode entities as embeddings, triples as embeddings\u2019 constraint\nand learn to align the embeddings. The structural and side informa-\ntion are usually utilized via embedding propagation, aggregation\nor interaction. However, the details of the underlying logical in-\nference steps among the alignment process are usually omitted,\nresulting in inadequate inference process. In this paper, we intro-\nduce P-NAL, an entity alignment method that captures two types\nof logical inference paths with Non-Axiomatic Logic (NAL). Type I\nis the bridge-like inference path between to-be-aligned entity pairs,\nconsisting of two relation/attribute triples and a similarity sentence\nbetween the other two entities. Type II links the entity pair by\ntheir embeddings. P-NAL iteratively aligns entities and relations\nby integrating the conclusions of the inference paths. Moreover,\nour method is logically interpretable and extensible due to the ex-\npressiveness of NAL. Our proposed method is suitable for various\nEA settings. Experimental results show that our method outper-\nforms state-of-the-art methods in terms of Hits@1, achieving 0.98+\non all three datasets of \ud835\udc37\ud835\udc35\ud835\udc4315\ud835\udc3ewith both supervised and unsu-\npervised settings. To our knowledge, we present the first in-depth\nanalysis of entity alignment\u2019s basic principles from a unified logical\nperspective."}, {"group": "Natural Language Processing", "id": 25, "label": "Variational Multi-Modal Hypergraph Attention Network for Multi-Modal Relation Extraction", "shape": "dot", "size": 10, "title": "Authors: Jianxin Li\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Multi-modal relation extraction (MMRE) is a challenging task that\naims to identify relations between entities in text leveraging im-\nage information. Existing methods are limited by their neglect of\nthe multiple entity pairs in one sentence sharing very similar con-\ntextual information (i.e., the same text and image), resulting in\nincreased difficulty in the MMRE task. To address this limitation,\nwe propose the Variational Multi-Modal Hypergraph Attention Net-\nwork (VM-HAN) for multi-modal relation extraction. Specifically,\nwe first construct a multi-modal hypergraph for each sentence\nwith the corresponding image, to establish different high-order\nintra-/inter-modal correlations for different entity pairs in each\nsentence. We further design the Variational Hypergraph Attention\nNetworks (V-HAN) to obtain representational diversity among dif-\nferent entity pairs using Gaussian distribution and learn a better\nhypergraph structure via variational attention. VM-HAN achieves\nstate-of-the-art performance on the MMRE task, outperforming\nexisting methods in terms of accuracy and efficiency.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Information extraction.\nKEYWORDS\nMulti-modal relation extraction, hypergraph attention network.\n\u2217Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM MM, 2024, Melbourne, Australia\n\u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn"}, {"group": "Natural Language Processing", "id": 46, "label": "FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables", "shape": "dot", "size": 10, "title": "Authors: Danrui Qi, Weiling Zheng, Jiannan Wang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Feature augmentation from one-to-many relation-\nship tables is a critical but challenging problem in ML model\ndevelopment. To augment good features, data scientists need to\ncome up with SQL queries manually, which is time-consuming.\nFeaturetools [1] is a widely used tool by the data science commu-\nnity to automatically augment the training data by extracting\nnew features from relevant tables. It represents each feature\nas a group-by aggregation SQL query on relevant tables and\ncan automatically generate these SQL queries. However, it does\nnot include predicates in these queries, which significantly limits\nits application in many real-world scenarios. To overcome this\nlimitation, we propose FEATAUG, a new feature augmentation\nframework that automatically extracts predicate-aware SQL\nqueries from one-to-many relationship tables. This extension\nis not trivial because considering predicates will exponentially\nincrease the number of candidate queries. As a result, the original\nFeaturetools framework, which materializes all candidate queries,\nwill not work and needs to be redesigned. We formally define the\nproblem and model it as a hyperparameter optimization problem.\nWe discuss how the Bayesian Optimization can be applied here\nand propose a novel warm-up strategy to optimize it. To make our\nalgorithm more practical, we also study how to identify promising\nattribute combinations for predicates. We show that how the\nbeam search idea can partially solve the problem and propose\nseveral techniques to further optimize it. Our experiments on\nfour real-world datasets demonstrate that FeatAug extracts more\neffective features compared to Featuretools and other baselines.\nThe code is open-sourced at https://github.com/sfu-db/FeatAug.\nIndex Terms\u2014automatic feature augmentation, automatic fea-\nture engineering, data preparation, one-to-many relational tables\nI."}, {"group": "Natural Language Processing", "id": 48, "label": "LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries", "shape": "dot", "size": 10, "title": "Authors: Ziqi Yin\u2020, Shanshan Feng\u2021, Shang Liu\u2020, Gao Cong\u2020, Yew Soon Ong\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014With the proliferation of spatio-textual data, Top-\nk KNN spatial keyword queries (TkQs), which return a list of\nobjects based on a ranking function that evaluates both spatial\nand textual relevance, have found many real-life applications.\nExisting geo-textual indexes for TkQs use traditional retrieval\nmodels like BM25 to compute text relevance and usually exploit\na simple linear function to compute spatial relevance, but its\neffectiveness is limited. To improve effectiveness, several deep\nlearning models have recently been proposed, but they suffer\nsevere efficiency issues. To the best of our knowledge, there are\nno efficient indexes specifically designed to accelerate the top-k\nsearch process for these deep learning models.\nTo tackle these issues, we propose a novel technique, which\nLearns to Index the Spatio-Textual data for answering em-\nbedding based spatial keyword queries (called LIST). LIST\nis featured with two novel components. Firstly, we propose a\nlightweight and effective relevance model that is capable of learn-\ning both textual and spatial relevance. Secondly, we introduce a\nnovel machine learning based Approximate Nearest Neighbor\nSearch (ANNS) index, which utilizes a new learning-to-cluster\ntechnique to group relevant queries and objects together while\nseparating irrelevant queries and objects. Two key challenges\nin building an effective and efficient index are the absence of\nhigh-quality labels and unbalanced clustering results. We develop\na novel pseudo-label generation technique to address the two\nchallenges. Experimental results show that LIST significantly\noutperforms state-of-the-art methods on effectiveness, with im-\nprovements up to 19.21% and 12.79% in terms of NDCG@1\nand Recall@10, and is three orders of magnitude faster than the\nmost effective baseline.\nI."}, {"group": "Natural Language Processing", "id": 51, "label": "Query Rewriting via Large Language Models", "shape": "dot", "size": 10, "title": "Authors: Jie Liu, Barzan Mozafari\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Query rewriting is one of the most effective techniques for coping\nwith poorly written queries before passing them down to the query\noptimizer. Manual rewriting is not scalable, as it is error-prone\nand requires deep expertise. Similarly, traditional query rewriting\nalgorithms can only handle a small subset of queries: rule-based\ntechniques do not generalize to new query patterns and synthesis-\nbased techniques cannot handle complex queries. Fortunately, the\nrise of Large Language Models (LLMs), equipped with broad general\nknowledge and advanced reasoning capabilities, has created hopes\nfor solving some of these previously open problems.\nIn this paper, we present GenRewrite, the first holistic system\nthat leverages LLMs for query rewriting. We introduce the notion of\nNatural Language Rewrite Rules (NLR2s), and use them as hints to\nthe LLM but also a means for transferring knowledge from rewrit-\ning one query to another, and thus becoming smarter and more\neffective over time. We present a novel counterexample-guided\ntechnique that iteratively corrects the syntactic and semantic errors\nin the rewritten query, significantly reducing the LLM costs and the\nmanual effort required for verification. GenRewrite speeds up 22\nout of 99 TPC queries (the most complex public benchmark) by more\nthan 2x, which is 2.5x\u20133.2x higher coverage than state-of-the-art\ntraditional query rewriting and 2.1x higher than the out-of-the-box\nLLM baseline.\nPVLDB Reference Format:\nJie Liu and Barzan Mozafari. Query Rewriting via Large Language Models.\nPVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nURL_TO_YOUR_ARTIFACTS."}, {"group": "Natural Language Processing", "id": 53, "label": "Schema-Aware Multi-Task Learning for Complex Text-to-SQL", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Conventional text-to-SQL parsers are not good\nat synthesizing complex SQL queries that in-\nvolve multiple tables or columns, due to the\nchallenges inherent in identifying the correct\nschema items and performing accurate align-\nment between question and schema items. To\naddress the above issue, we present a schema-\naware multi-task learning framework (named\nMTSQL) for complicated SQL queries. Specif-\nically, we design a schema linking discrimina-\ntor module to distinguish the valid question-\nschema linkings, which explicitly instructs\nthe encoder by distinctive linking relations to\nenhance the alignment quality.\nOn the de-\ncoder side, we define 6-type relationships to\ndescribe the connections between tables and\ncolumns (e.g., WHERE_TC), and introduce\nan operator-centric triple extractor to recognize\nthose associated schema items with the prede-\nfined relationship. Also, we establish a rule set\nof grammar constraints via the predicted triples\nto filter the proper SQL operators and schema\nitems during the SQL generation. On Spider, a\ncross-domain challenging text-to-SQL bench-\nmark, experimental results indicate that MT-\nSQL is more effective than baselines, especially\nin extremely hard scenarios. Moreover, fur-\nther analyses verify that our approach leads to\npromising improvements for complicated SQL\nqueries."}, {"group": "History of Science", "id": 54, "label": "Accelerating Regular Path Queries over Graph Database with Processing-in-Memory", "shape": "dot", "size": 10, "title": "Authors: Ruoyan Ma, Shengan Zheng, Guifeng Wang, Jin Pu, Yifan Hua, Wentao Wang, Linpeng Huang, Jiao Tong University\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Regular path queries (RPQs) in graph databases are bottlenecked\nby the memory wall. Emerging processing-in-memory (PIM) tech-\nnologies offer a promising solution to dispatch and execute path\nmatching tasks in parallel within PIM modules. We present Mocto-\npus, a PIM-based data management system for graph databases that\nsupports efficient batch RPQs and graph updates. Moctopus em-\nploys a PIM-friendly dynamic graph partitioning algorithm, which\ntackles graph skewness and preserves graph locality with low over-\nhead for RPQ processing. Moctopus enables efficient graph update\nby amortizing the host CPU\u2019s update overhead to PIM modules.\nEvaluation of Moctopus demonstrates superiority over the state-of-\nthe-art traditional graph database.\nKEYWORDS\nRegular Path Query, Processing-in-Memory, path matching, graph\npartition, load balance"}, {"group": "Natural Language Processing", "id": 57, "label": "VDTuner: Automated Performance Tuning for Vector Data Management Systems", "shape": "dot", "size": 10, "title": "Authors: Tiannuo Yang, Wen Hu, Wangqi Peng, Yusen Li, Jianguo Li, Gang Wang, Xiaoguang Liu, yangtn, liyusen\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Vector data management systems (VDMSs) have\nbecome an indispensable cornerstone in large-scale information\nretrieval and machine learning systems like large language\nmodels. To enhance the efficiency and flexibility of similarity\nsearch, VDMS exposes many tunable index parameters and\nsystem parameters for users to specify. However, due to the\ninherent characteristics of VDMS, automatic performance tuning\nfor VDMS faces several critical challenges, which cannot be well\naddressed by the existing auto-tuning methods.\nIn this paper, we introduce VDTuner, a learning-based au-\ntomatic performance tuning framework for VDMS, leveraging\nmulti-objective Bayesian optimization. VDTuner overcomes the\nchallenges associated with VDMS by efficiently exploring a\ncomplex multi-dimensional parameter space without requiring\nany prior knowledge. Moreover, it is able to achieve a good\nbalance between search speed and recall rate, delivering an\noptimal configuration. Extensive evaluations demonstrate that\nVDTuner can markedly improve VDMS performance (14.12%\nin search speed and 186.38% in recall rate) compared with\ndefault setting, and is more efficient compared with state-of-the-\nart baselines (up to 3.57\u00d7 faster in terms of tuning time). In\naddition, VDTuner is scalable to specific user preference and\ncost-aware optimization objective. VDTuner is available online\nat https://github.com/tiannuo-yang/VDTuner.\nIndex\nTerms\u2014vector\ndatabase,\nparameter\ntuning,\nsearch\nspeed, recall rate, machine learning\nI."}, {"group": "History of Science", "id": 62, "label": "AI-driven Hypergraph Network of Organic Chemistry:  Network Statistics and Applications in Reaction Classification", "shape": "dot", "size": 10, "title": "Authors: Venkat Venkatasubramanian\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Rapid discovery of new reactions and molecules\nin recent years has been facilitated by the ad-\nvances in high throughput screening, accessibility\nto a highly complex chemical design space, and\nthe development of accurate molecular model-\ning frameworks. A holistic study of the grow-\ning chemistry literature is, therefore, required\nthat focuses on understanding the recent trends\nin organic chemistry and extrapolating them to\ninfer possible future trajectories. To this end, sev-\neral network theory-based studies have been re-\nported that use a directed graph representation\nof chemical reactions. Here, we perform a study\nbased on representing chemical reactions as hy-\npergraphs where the nodes represent the partici-\npating molecules and hyperedges represent reac-\ntions between nodes. We use a standard reactions\ndataset to construct a hypergraph network of or-\nganic chemistry and report its statistics such as\ndegree distribution, average path length, assorta-\ntivity or degree correlations, PageRank centrality,\nand graph-based clusters (or communities). We\nalso compute each statistic for an equivalent di-\nrected graph representation of reactions to draw\nparallels and highlight differences between the\ntwo. To demonstrate the AI applicability of hyper-\ngraph reaction representation, we generate dense\nhypergraph embeddings and use them in the reac-\ntion classi\ufb01cation problem. We conclude that the\nhypergraph representation is \ufb02exible, preserves re-\naction context, and uncovers hidden insights that\nare otherwise not apparent in a traditional directed\ngraph representation of chemical reactions.\n1Department of Chemical Engineering, Columbia University,\nNew York, USA. Correspondence to: Venkat Venkatasubramanian\n\u003cvenkat@columbia.edu\u003e.\n1."}, {"group": "Network Science", "id": 94, "label": "Neighbor2vec: an ef\ufb01cient and effective method for Graph Embedding", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Graph embedding techniques have led to signi\ufb01-\ncant progress in recent years.\nHowever, present\ntechniques are not effective enough to capture the\npatterns of networks. This paper propose neigh-\nbor2vec, a neighbor-based sampling strategy used\nalgorithm to learn the neighborhood representa-\ntions of node, a framework to gather the struc-\nture information by feature propagation between\nthe node and its neighbors. We claim that neigh-\nbor2vec is a simple and effective approach to en-\nhancing the scalability as well as equality of graph\nembedding, and it breaks the limits of the exist-\ning state-of-the-art unsupervised techniques.\nWe\nconduct experiments on several node classi\ufb01ca-\ntion and link prediction tasks for networks such\nas ogbn-arxiv, ogbn-products, ogbn-proteins, ogbl-\nppa,ogbl-collab and ogbl-citation2.\nThe result\nshows that Neighbor2vec\u2019s representations pro-\nvide an average accuracy scores up to 6.8 percent\nhigher than competing methods in node classi\ufb01ca-\ntion tasks and 3.0 percent higher in link prediction\ntasks. The neighbor2vec\u2019s representations are able\nto outperform all baseline methods and two classi-\ncal GNN models in all six experiments."}, {"group": "Network Science", "id": 97, "label": "Differentially Describing Groups of Graphs", "shape": "dot", "size": 10, "title": "Authors: Sebastian Dalleiger,*, Jilles Vreeken\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: How does neural connectivity in autistic children differ from\nneural connectivity in healthy children or autistic youths?\nWhat patterns in global trade networks are shared across\nclasses of goods, and how do these patterns change over time?\nAnswering questions like these requires us to differentially\ndescribe groups of graphs: Given a set of graphs and a parti-\ntion of these graphs into groups, discover what graphs in one\ngroup have in common, how they systematically differ from\ngraphs in other groups, and how multiple groups of graphs are\nrelated. We refer to this task as graph group analysis, which\nseeks to describe similarities and differences between graph\ngroups by means of statistically signi\ufb01cant subgraphs. To per-\nform graph group analysis, we introduce GRAGRA, which\nuses maximum entropy modeling to identify a non-redundant\nset of subgraphs with statistically signi\ufb01cant associations to\none or more graph groups. Through an extensive set of ex-\nperiments on a wide range of synthetic and real-world graph\ngroups, we con\ufb01rm that GRAGRA works well in practice."}, {"group": "Network Science", "id": 98, "label": "VGAER: Graph Neural Network Reconstruction based Community Detection", "shape": "dot", "size": 10, "title": "Authors: Chenyang Qiu, Zhaoci Huang, Wenzhe Xu, Huijia Li, cyqiu\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Community detection is a fundamental and important issue\nin network science, but there are only a few community de-\ntection algorithms based on graph neural networks, among\nwhich unsupervised algorithms are almost blank. By fus-\ning the high-order modularity information with network fea-\ntures, this paper proposes a Variational Graph AutoEncoder\nReconstruction based community detection VGAER for the\n\ufb01rst time, and gives its non-probabilistic version. They do\nnot need any prior information. We have carefully designed\ncorresponding input features, decoder, and downstream tasks\nbased on the community detection task and these designs\nare concise, natural, and perform well (NMI values under\nour design are improved by 59.1% - 565.9%). Based on\na series of experiments with wide range of datasets and\nadvanced methods, VGAER has achieved superior perfor-\nmance and shows strong competitiveness and potential with\na simpler design. Finally, we report the results of algo-\nrithm convergence analysis and t-SNE visualization, which\nclearly depicted the stable performance and powerful net-\nwork modularity ability of VGAER. Our codes are available\nat https://github.com/qcydm/VGAER."}, {"group": "Natural Language Processing", "id": 107, "label": "Predicting Research Trends in Arti\ufb01cial Intelligence with Gradient Boosting Decision Trees and Time-aware Graph Neural Networks", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014The Science4cast 2021 competition focuses on pre-\ndicting future edges in an evolving semantic network, where each\nvertex represents an arti\ufb01cial intelligence concept, and an edge\nbetween a pair of vertices denotes that the two concepts have\nbeen investigated together in a scienti\ufb01c paper. In this paper,\nwe describe our solution to this competition. We present two\ndistinct approaches: a tree-based gradient boosting approach and\na deep learning approach, and demonstrate that both approaches\nachieve competitive performance. Our \ufb01nal solution, which is\nbased on a blend of the two approaches, achieved the 1st place\namong all the participating teams. The source code for this paper\nis available at https://github.com/YichaoLu/Science4cast2021.\nIndex Terms\u2014semantic network, link prediction, network\nanalysis, feature engineering, graph neural network\nI."}, {"group": "Network Science", "id": 109, "label": "Understanding Political Polarization via Jointly Modeling Users, Connections and Multimodal Contents on Heterogeneous Graphs", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Understanding political polarization on social platforms is\nimportant as public opinions may become increasingly ex-\ntreme when they are circulated in homogeneous communi-\nties, thus potentially causing damage in the real world. Au-\ntomatically detecting the political ideology of social media\nusers can help better understand political polarization. How-\never, it is challenging due to the scarcity of ideology la-\nbels, complexity of multimodal contents, and cost of time-\nconsuming data collection process. Most previous frame-\nworks either focus on unimodal content or do not scale up\nwell. In this study, we adopt a heterogeneous graph neu-\nral network to jointly model user characteristics, multimodal\npost contents as well as user-item relations in a bipartite graph\nto learn a comprehensive and effective user embedding with-\nout requiring ideology labels. We apply our framework to on-\nline discussions about economy and public health topics. The\nlearned embeddings are then used to detect political ideology\nand understand political polarization. Our framework outper-\nforms the unimodal, early/late fusion baselines, and homoge-\nneous GNN frameworks by a margin of at least 9% absolute\ngain in the area under the receiver operating characteristic on\ntwo social media datasets. More importantly, our work does\nnot require a time-consuming data collection process, which\nallows faster detection and in turn allows the policy makers\nto conduct analysis and design policies in time to respond\nto crises. We also show that our framework learns mean-\ningful user embeddings and can help better understand po-\nlitical polarization. Notable differences in user descriptions,\ntopics, images, and levels of retweet/quote activities are ob-\nserved. Our framework for decoding user-content interaction\nshows wide applicability in understanding political polariza-\ntion. Furthermore, it can be extended to user-item bipartite\ninformation networks for other applications such as content\nand product recommendation."}, {"group": "Network Science", "id": 111, "label": "SigGAN : Adversarial Model for Learning Signed Relationships in Networks", "shape": "dot", "size": 10, "title": "Authors: Roshni Chakraborty, Joydeep Chandra\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Signed link prediction in graphs is an important problem that\nhas applications in diverse domains. It is a binary classi\ufb01cation\nproblem that predicts whether an edge between a pair of nodes\nis positive or negative. Existing approaches for link prediction\nin unsigned networks cannot be directly applied for signed link\nprediction due to their inherent di\ufb00erences. Further, additional\nstructural constraints, like, the structural balance property of the\nsigned networks must be considered for signed link prediction.\nRecent signed link prediction approaches generate node repre-\nsentations using either generative models or discriminative mod-\nels. Inspired by the recent success of Generative Adversarial Net-\nwork (GAN) based models which comprises of a discriminator\nand generator in several applications, we propose a Generative\nAdversarial Network (GAN) based model for signed networks,\nSigGAN. It considers the requirements of signed networks, such\nas, integration of information from negative edges, high imbal-\nance in number of positive and negative edges and structural\nbalance theory. Comparing the performance with state of the\nart techniques on several real-world datasets validates the e\ufb00ec-\ntiveness of SigGAN.\nKEYWORDS\nSigned Networks, Generative Adversarial Networks, Link Pre-\ndiction, Structural Awareness, Structural Balance"}, {"group": "History of Science", "id": 116, "label": "Efficient GPU Implementation of Static and Incrementally Expanding DF-P PageRank for Dynamic Graphs", "shape": "dot", "size": 10, "title": "Authors: Subhajit Sahu\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: PageRank is a widely used centrality measure that \"ranks\" vertices\nin a graph by considering the connections and their importance.\nIn this report, we first introduce one of the most efficient GPU\nimplementations of Static PageRank, which recomputes PageRank\nscores from scratch. It uses a synchronous pull-based atomics-free\nPageRank computation, with the low and high in-degree vertices\nbeing partitioned and processed by two separate kernels. Next,\nwe present our GPU implementation of incrementally expanding\n(and contracting) Dynamic Frontier with Pruning (DF-P) PageRank,\nwhich processes only a subset of vertices likely to change ranks.\nIt is based on Static PageRank, and uses an additional partitioning\nbetween low and high out-degree vertices for incremental expan-\nsion of the set of affected vertices with two additional kernels. On\na server with an NVIDIA A100 GPU, our Static PageRank outper-\nforms Hornet and Gunrock\u2019s PageRank implementations by 31\u00d7\nand 5.9\u00d7 respectively. On top of the above, DF-P PageRank outper-\nforms Static PageRank by 2.1\u00d7 on real-world dynamic graphs, and\nby 3.1\u00d7 on large static graphs with random batch updates.\nKEYWORDS\nParallel GPU-based PageRank, Dynamic Frontier approach"}, {"group": "Natural Language Processing", "id": 117, "label": "BOND: Bootstrapping From-Scratch Name Disambiguation with Multi-task Promoting", "shape": "dot", "size": 10, "title": "Authors: Yuqing Cheng\u2217\u2020, Fanjin Zhang\u2217\u2021, Jie Tang\u2021\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: From-scratch name disambiguation is an essential task for estab-\nlishing a reliable foundation for academic platforms. It involves\npartitioning documents authored by identically named individu-\nals into groups representing distinct real-life experts. Canonically,\nthe process is divided into two decoupled tasks: locally estimat-\ning the pairwise similarities between documents followed by glob-\nally grouping these documents into appropriate clusters. However,\nsuch a decoupled approach often inhibits optimal information ex-\nchange between these intertwined tasks. Therefore, we present\nBOND, which bootstraps the local and global informative signals\nto promote each other in an end-to-end regime. Specifically, BOND\nharnesses local pairwise similarities to drive global clustering, sub-\nsequently generating pseudo-clustering labels. These global signals\nfurther refine local pairwise characterizations. The experimental re-\nsults establish BOND\u2019s superiority, outperforming other advanced\nbaselines by a substantial margin. Moreover, an enhanced version,\nBOND+, incorporating ensemble and post-match techniques, rivals\nthe top methods in the WhoIsWho competition1.\nCCS CONCEPTS\n\u2022 Information systems \u2192Data extraction and integration; Entity\nresolution.\nKEYWORDS\nname disambiguation, multi-task learning\n\u2217Equal contribution.\n\u2020Work was done when Yuqing interned at Zhipu AI.\n\u2021Fanjin Zhang and Jie Tang are the corresponding authors.\n1http://whoiswho.biendata.xyz/\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW \u201924, May 13\u201317, 2024, Singapore, Singapore\n\u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0171-9/24/05...$15.00\nhttps://doi.org/10.1145/3589334.3645580\nACM Reference Format:\nYuqing Cheng, Bo Chen, Fanjin Zhang, and Jie Tang. 2024. BOND: Boot-\nstrapping From-Scratch Name Disambiguation with Multi-task Promot-\ning. In Proceedings of the ACM Web Conference 2024 (WWW \u201924), May\n13\u201317, 2024, Singapore, Singapore. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3589334.3645580"}, {"group": "Natural Language Processing", "id": 125, "label": "CORE: Data Augmentation for Link Prediction via Information Bottleneck", "shape": "dot", "size": 10, "title": "Authors: Nitesh V. Chawla\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Link prediction (LP) is a fundamental task in graph representation\nlearning, with numerous applications in diverse domains. However,\nthe generalizability of LP models is often compromised due to the\npresence of noisy or spurious information in graphs and the inher-\nent incompleteness of graph data. To address these challenges, we\ndraw inspiration from the Information Bottleneck principle and\npropose a novel data augmentation method, COmplete and REduce\n(CORE) to learn compact and predictive augmentations for LP mod-\nels. In particular, CORE aims to recover missing edges in graphs\nwhile simultaneously removing noise from the graph structures,\nthereby enhancing the model\u2019s robustness and performance. Exten-\nsive experiments on multiple benchmark datasets demonstrate the\napplicability and superiority of CORE over state-of-the-art meth-\nods, showcasing its potential as a leading approach for robust LP\nin graph representation learning.\nCCS CONCEPTS\n\u2022 Information systems \u2192Data mining; \u2022 Computing method-\nologies \u2192Regularization.\nKEYWORDS\nlink prediction, data augmentation, information bottleneck, graph\nneural networks\nACM Reference Format:\nKaiwen Dong, Zhichun Guo, and Nitesh V. Chawla. 2024. CORE: Data\nAugmentation for Link Prediction via Information Bottleneck. In . ACM,\nNew York, NY, USA, 17 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"}, {"group": "History of Science", "id": 127, "label": "A Fast Maximum Clique Algorithm Based on Network Decomposition for Large Sparse Networks", "shape": "dot", "size": 10, "title": "Authors: Tianlg Fan, Wenjun Jiang, Yi-Cheng Zhang, Linyuan L\u00fc,\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Finding maximum cliques in large networks is a challenging combinatorial problem with many real-world applications. \nWe present a fast algorithm to achieve the exact solution for the maximum clique problem in large sparse networks based \non efficient graph decomposition. A bunch of effective techniques is being used to greatly prune the graph and a novel \nconcept called Complete-Upper-Bound-Induced Subgraph (CUBIS) is proposed to ensure that the structures with the \npotential to form the maximum clique are retained in the process of graph decomposition. Our algorithm first pre-prunes \nperipheral nodes, subsequently, one or two small-scale CUBISs are constructed guided by the core number and current \nmaximum clique size. Bron-Kerbosch search is performed on each CUBIS to find the maximum clique. Experiments on \n50 empirical networks with a scale of up to 20 million show the CUBIS scales are largely independent of the original \nnetwork scale. This enables an approximately linear runtime, making our algorithm amenable for large networks. Our \nwork provides a new framework for effectively solving maximum clique problems on massive sparse graphs, which not \nonly makes the graph scale no longer the bottleneck but also shows some light on solving other clique-related problems. \nKeywords \nGraph mining, Complex network, Maximum clique problem, Graph decomposition"}, {"group": "Natural Language Processing", "id": 128, "label": "Multi-view Graph Structural Representation Learning via Graph Coarsening", "shape": "dot", "size": 10, "title": "Authors: Xiaorui Qi, Qijie Bai, Yanlong Wen\u2217, Haiwei Zhang, Xiaojie Yuan, qixiaorui, wenyl, zhhaiwei\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Graph Transformers (GTs) have made remarkable\nachievements in graph-level tasks. However, most\nexisting works regard graph structures as a form\nof guidance or bias for enhancing node represen-\ntations, which focuses on node-central perspec-\ntives and lacks explicit representations of edges and\nstructures. One natural question is, can we treat\ngraph structures node-like as a whole to learn high-\nlevel features? Through experimental analysis, we\nexplore the feasibility of this assumption. Based\non our findings, we propose a novel multi-view\ngraph structural representation learning model via\ngraph coarsening (MSLgo) on GT architecture for\ngraph classification.\nSpecifically, we build three\nunique views, original, coarsening, and conversion,\nto learn a thorough structural representation. We\ncompress loops and cliques via hierarchical heuris-\ntic graph coarsening and restrict them with well-\ndesigned constraints, which builds the coarsening\nview to learn high-level interactions between struc-\ntures. We also introduce line graphs for edge em-\nbeddings and switch to edge-central perspective to\nconstruct the conversion view. Experiments on six\nreal-world datasets demonstrate the improvements\nof MSLgo over 14 baselines from various architec-\ntures."}, {"group": "History of Science", "id": 1, "label": "Reconfigurable Intelligent Surfaces for THz: Hardware Design and Signal Processing Challenges", "shape": "dot", "size": 10, "title": "Authors: George C. Alexropoulos, Antonio Clemente, S\u00b4ergio Matos, Ryan Husbs, Sean Ahearne, Qi Luo, Ver\u00b4onica Lain-Rubio, Thomas K\u00a8urner, Lu\u00b4\u0131s M. Pessoa\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Wireless communications in the THz frequency\nband is an envisioned revolutionary technology for sixth Gener-\nation (6G) networks. However, such frequencies impose certain\ncoverage and device design challenges that need to be efficiently\novercome. To this end, the development of cost- and energy-\nefficient approaches for scaling these networks to realistic sce-\nnarios constitute a necessity. Among the recent research trends\ncontributing to these objectives belongs the technology of Recon-\nfigurable Intelligent Surfaces (RISs). In fact, several high-level\ndescriptions of THz systems based on RISs have been populating\nthe literature. Nevertheless, hardware implementations of those\nsystems are still very scarce, and not at the scale intended for most\nenvisioned THz scenarios. In this paper, we overview some of the\nmost significant hardware design and signal processing challenges\nwith THz RISs, and present a preliminary analysis of their impact\non the overall link budget and system performance, conducted\nin the framework of the ongoing TERRAMETA project.\nIndex Terms\u2014Reconfigurable intelligent surface, 6G, THz,\nbeamforming, beam squint, link budget, use cases.\nI."}, {"group": "History of Science", "id": 3, "label": "Wet TinyML: Chemical Neural Network Using Gene Regulation and Cell Plasticity", "shape": "dot", "size": 10, "title": "Authors: Samitha Somathilaka, Adrian Ratwatte, Sasitharan Balasubramaniam, Mehmet Can, Witawas Srisa\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In our earlier work, we introduced the concept of Gene Regulatory\nNeural Network (GRNN), which utilizes natural neural network-\nlike structures inherent in biological cells to perform computing\ntasks using chemical inputs. We define this form of chemical-based\nneural network as Wet TinyML. The GRNN structures are based\non the gene regulatory network and have weights associated with\neach link based on the estimated interactions between the genes.\nThe GRNNs can be used for conventional computing by employing\nan application-based search process similar to the Network Archi-\ntecture Search. This study advances this concept by incorporating\ncell plasticity, to further exploit natural cell\u2019s adaptability, in order\nto diversify the GRNN search that can match larger spectrum as\nwell as dynamic computing tasks. As an example application, we\nshow that through the directed cell plasticity, we can extract the\nmathematical regression evolution enabling it to match to dynamic\nsystem applications. We also conduct energy analysis by comparing\nthe chemical energy of the GRNN to its silicon counterpart, where\nthis analysis includes both artificial neural network algorithms\nexecuted on von Neumann architecture as well as neuromorphic\nprocessors. The concept of Wet TinyML can pave the way for the\nnew emergence of chemical-based, energy-efficient and miniature\nBiological AI.\nKEYWORDS\nBiological AI, Cell Plasticity, Biocomputing, Neuromorphic.\nACM Reference Format:\nSamitha Somathilaka, Adrian Ratwatte, Sasitharan Balasubramaniam, Mehmet\nCan Vuran, Witawas Srisa-an, and Pietro Li\u00f2. 2024. Wet TinyML: Chemical\nNeural Network Using Gene Regulation and Cell Plasticity. In Proceedings\nof tinyML Research Symposium (tinyML Research Symposium\u201924). ACM, San\nFrancisco, USA, 7 pages.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\ntinyML Research Symposium\u201924, April 2024, San Francisco, CA\n\u00a9 2024 Copyright held by the owner/author(s)."}, {"group": "History of Science", "id": 4, "label": "Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications", "shape": "dot", "size": 10, "title": "Authors: Olivia Weng, Alexander Redding, Javier Mauricio Duarte, Ryan Kastner\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014With more scientific fields relying on neural networks (NNs)\nto process data incoming at extreme throughputs and latencies, it is\ncrucial to develop NNs with all their parameters stored on-chip. In\nmany of these applications, there is not enough time to go off-chip and\nretrieve weights. Even more so, off-chip memory such as DRAM does\nnot have the bandwidth required to process these NNs as fast as the data\nis being produced (e.g., every 25 ns). As such, these extreme latency and\nbandwidth requirements have architectural implications for the hardware\nintended to run these NNs: 1) all NN parameters must fit on-chip, and 2)\ncodesigning custom/reconfigurable logic is often required to meet these\nlatency and bandwidth constraints. In our work, we show that many\nscientific NN applications must run fully on chip, in the extreme case\nrequiring a custom chip to meet such stringent constraints.\nI."}, {"group": "History of Science", "id": 56, "label": "LightningSimV2: Faster and Scalable Simulation for High-Level Synthesis via Graph Compilation and Optimization", "shape": "dot", "size": 10, "title": "Authors: Rishov Sarkar, Rachel Paul, rishov.sarkar, rachel.paul\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014High-Level Synthesis (HLS) enables rapid prototyp-\ning of complex hardware designs by translating C or C++ code to\nlow-level RTL code. However, the testing and evaluation of HLS\ndesigns still typically rely on slow RTL-level simulators that can\ntake hours to provide feedback, especially for complex designs.\nA recent work, LightningSim, helps to solve this problem by\nproviding a simulation workflow one to two orders of magnitude\nfaster than RTL simulation. However, it still exhibits inefficiencies\ndue to several types of redundant computation, making it slow for\nlarge design simulation and design space exploration. Addressing\nthese inefficiencies, we introduce LightningSimV2, a much faster\nand scalable simulation tool. LightningSimV2 features three\nmain innovations. First, we perform compile-time static analysis,\nexploiting the repetitive structures in HLS designs, e.g., loops,\nto reduce the simulation workload. Second, we propose a novel\ngraph-based simulation approach, with decoupled simulation\ngraph construction step and graph traversal step, significantly\nreducing repeated computation. Third, benefiting from the decou-\npled approach, LightningSimV2 can perform incremental stall\nanalysis extremely fast, enabling highly efficient design space\nexploration of large numbers of complex hardware parameters,\ne.g., optimal FIFO depths. Moreover, the DSE is well-suited\nfor parallel computing, further improving the DSE efficiency.\nCompared with LightningSim, LightningSimV2 achieves up to\n3.5\u00d7 speedup in full simulation and up to 577\u00d7 speed up\nfor incremental DSE. Our code is open-source on GitHub at\nhttps://github.com/sharc-lab/LightningSim/tree/v0.2.0.\nI."}, {"group": "History of Science", "id": 85, "label": "Ultra-Wideband (UWB) Positioning System Based on ESP32 and DWM3000 Modules", "shape": "dot", "size": 10, "title": "Authors: Sebastian Krebs, Tom Herter\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 In this paper, an Ultra-Wideband (UWB) position-\ning system is introduced, that leverages six identical custom-\ndesigned boards, each featuring an ESP32 microcontroller and\na DWM3000 module from Quorvo.\nThe system is capable of achieving localization with an accuracy\nof up to 10 cm, by utilizing Two-Way-Ranging (TWR) measure-\nments between one designated \u201dtag\u201d and five \u201danchor\u201d devices.\nThe gathered distance measurements are subsequently processed\nby an Extended Kalman Filter (EKF) running locally on the tag\nboard, enabling it to determine its own position, relying on fixed,\na priori known positions of the anchor boards.\nThis paper presents a comprehensive overview of the system\u2019s\narchitecture, the key components, and the capabilities it offers\nfor indoor positioning and tracking applications.\nI."}, {"group": "History of Science", "id": 119, "label": "Beam Management in Low Earth Orbit Satellite Networks with Random Traffic Arrival and Time-varying Topology", "shape": "dot", "size": 10, "title": "Authors: Jianfeng Zhu, Yaohua Sun, Mugen Peng\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Low earth orbit (LEO) satellite communication net-\nworks have been considered as promising solutions to providing\nhigh data rate and seamless coverage, where satellite beam\nmanagement plays a key role. However, due to the limitation of\nbeam resource, dynamic network topology, beam spectrum reuse,\ntime-varying traffic arrival and service continuity requirement,\nit is challenging to effectively allocate time-frequency resource\nof satellite beams to multiple cells. In this paper, aiming at\nreducing time-averaged beam revisit time and mitigate inter-\nsatellite handover, a beam management problem is formulated for\ndynamic LEO satellite communication networks, under inter-cell\ninterference and network stability constraints. Particularly, inter-\ncell interference constraints are further simplified into off-axis\nangle based constraints, which provide tractable rules for spec-\ntrum sharing between two beam cells. To deal with the long-term\nperformance optimization, the primal problem is transformed\ninto a series of single epoch problems by adopting Lyapunov\noptimization framework. Since the transformed problem is NP-\nhard, it is further divided into three subproblems, including\nserving beam allocation, beam service time allocation and serving\nsatellite allocation. With the help of conflict graphs built with\noff-axis angle based constraints, serving beam allocation and\nbeam service time allocation algorithms are developed to reduce\nbeam revisit time and cell packet queue length. Then, we further\ndevelop a satellite-cell service relationship optimization algorithm\nto better adapt to dynamic network topology. Compared with\nbaselines, numerical results show that our proposal can reduce\naverage beam revisit time by 20.8% and keep strong network\nstability with similar inter-satellite handover frequency.\nIndex Terms\u2014Low earth orbit satellite communication, beam\nmanagement, beam revisit time, inter-satellite handover, interfer-\nence mitigation.\nI."}, {"group": "History of Science", "id": 120, "label": "IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, VOL. 73, NO. 3, MARCH 2024", "shape": "dot", "size": 10, "title": "Authors: Jianfeng Zhu, Yaohua Sun, Mugen Peng\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Low earth orbit (LEO) satellite communication\nbased on 3GPP standard is seen as a promising solution to\nrolling out communication services in areas without terrestrial\nbase stations. However, due to the fast movement of satellites\nand large beam footprint size, the existing 5G timing advance\n(TA) estimation mechanism cannot be directly applied when\nglobal navigation satellite system is unavailable. In this article, an\nenhanced TA estimation approach is proposed for LEO satellite\ncommunication networks. Specifically, a user-side time-frequency\npre-compensation method is introduced at first, which leverages\nfrequency offset measurement on synchronization signal blocks\nbroadcasted by satellites in initial cell search phase. For the\nrandom access phase, the upper bound of inter-preamble inter-\nference incurred by partial-period cross-correlation operations\nis derived for a preamble format advised by 3GPP, and it is\nshown that the interference level is closely related to the square\nof the number of such operations. Inspired by this result, a cyclic\nprefix free preamble format is further designed, which features\nextended guard time, differential power allocation and flexible\npreamble structure. Numerical results show that our proposal\ncan reduce the missed detection rate of preamble within a beam.\nParticularly, the missed detection rates of preamble under 32,\n48, and 64 users are lower than 1% when SNR = -6 dB, which\nis a significant improvement compared to baselines. In addition,\nour proposal can limit the TA estimation error of the detected\nusers to the time length of 25 time-domain sampling points when\nthe subcarrier spacing is 30 kHz and operation frequency is 27\nGHz.\nIndex Terms\u2014Low earth orbit satellite, random access, timing\nadvance estimation, preamble format design.\nI."}, {"group": "History of Science", "id": 121, "label": "Beam Management in Low Earth Orbit Satellite Communication With Handover Frequency Control and Satellite-Terrestrial Spectrum Sharing", "shape": "dot", "size": 10, "title": "Authors: Yaohua Sun, Jianfeng Zhu, Mugen Peng\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014To achieve ubiquitous wireless connectivity, low\nearth orbit (LEO) satellite networks have drawn much attention.\nHowever, effective beam management is challenging due to time-\nvarying cell load, high dynamic network topology, and complex\ninterference situations. In this paper, under inter-satellite han-\ndover frequency and satellite-terrestrial/inter-beam interference\nconstraints, we formulate a practical beam management problem,\naiming to maximize the long-term service satisfaction of cells.\nParticularly, Lyapunov framework is leveraged to equivalently\ntransform the primal problem into multiple single epoch op-\ntimization problems, where virtual queue stability constraints\nreplace inter-satellite handover frequency constraints. Since each\nsingle epoch problem is NP-hard, we further decompose it into\nthree subproblems, including inter-satellite handover decision,\nbeam hopping design and satellite-terrestrial spectrum sharing.\nFirst, a proactive inter-satellite handover mechanism is developed\nto balance handover frequency and satellite loads. Subsequently,\na beam hopping design algorithm is presented based on conflict\ngraphs to achieve interference mitigation among beams, and then\na flexible satellite-terrestrial spectrum sharing algorithm is de-\nsigned to satisfy the demands of beam cells and improve spectral\nefficiency. Simulation results show that our proposal significantly\nimproves service satisfaction compared with baselines, where the\naverage data queue length of beam cells is reduced by over 50%\nwith affordable handover frequency.\nIndex Terms\u2014Low earth orbit satellite, beam management,\ninter-satellite handover, interference mitigation, spectrum shar-\ning.\nI."}, {"group": "History of Science", "id": 5, "label": "Hindawi Template version: Apr19", "shape": "dot", "size": 10, "title": "Authors: Andr\u00e9s-David Su\u00e1rez-G\u00f3mez,, ANDRES A. HERNANDEZ ORTEGA,,\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Natural Language Processing", "id": 16, "label": "REQUAL-LM: Reliability and Equity through Aggregation in Large Language Models*", "shape": "dot", "size": 10, "title": "Authors: Sana Ebrahimi, Nima Shahbazi, Abolfazl Asudeh\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The extensive scope of large language models\n(LLMs) across various domains underscores\nthe critical importance of responsibility in their\napplication, beyond natural language process-\ning. In particular, the randomized nature of\nLLMs, coupled with inherent biases and his-\ntorical stereotypes in data, raises critical con-\ncerns regarding reliability and equity. Address-\ning these challenges are necessary before using\nLLMs for applications with societal impact.\nTowards addressing this gap, we introduce\nREQUAL-LM, a novel method for finding re-\nliable and equitable LLM outputs through ag-\ngregation. Specifically, we develop a Monte-\ncarlo method based on repeated sampling to\nfind a reliable output close to the mean of the\nunderlying distribution of possible outputs. We\nformally define the terms such as reliability and\nbias, and design an equity-aware aggregation to\nminimize harmful bias while finding a highly\nreliable output. REQUAL-LM does not require\nspecialized hardware, does not impose a signifi-\ncant computing load, and uses LLMs as a black-\nbox. This design choice enables seamless scala-\nbility alongside the rapid advancement of LLM\ntechnologies. Our system does not require re-\ntraining the LLMs, which makes it deployment-\nready and easy to adapt.\nOur comprehensive experiments using various\ntasks and datasets demonstrate that REQUAL-\nLM effectively mitigates bias and selects a more\nequitable response, specifically the outputs that\nproperly represents minority groups."}, {"group": "Natural Language Processing", "id": 26, "label": "Sequential Compositional Generalization in Multimodal Models", "shape": "dot", "size": 10, "title": "Authors: Semih Yagcioglu, Osman Batur, Aykut Erdem,, Erkut Erdem, Deniz Yuret,\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The rise of large-scale multimodal models\nhas paved the pathway for groundbreaking ad-\nvances in generative modeling and reasoning,\nunlocking transformative applications in a va-\nriety of complex tasks. However, a pressing\nquestion that remains is their genuine capabil-\nity for stronger forms of generalization, which\nhas been largely underexplored in the multi-\nmodal setting. Our study aims to address this\nby examining sequential compositional gen-\neralization using COMPACT (Compositional\nActivities)1, a carefully constructed, perceptu-\nally grounded dataset set within a rich backdrop\nof egocentric kitchen activity videos. Each\ninstance in our dataset is represented with a\ncombination of raw video footage, naturally\noccurring sound, and crowd-sourced step-by-\nstep descriptions. More importantly, our setup\nensures that the individual concepts are consis-\ntently distributed across training and evaluation\nsets, while their compositions are novel in the\nevaluation set. We conduct a comprehensive as-\nsessment of several unimodal and multimodal\nmodels. Our findings reveal that bi-modal and\ntri-modal models exhibit a clear edge over their\ntext-only counterparts. This highlights the im-\nportance of multimodality while charting a tra-\njectory for future research in this domain."}, {"group": "History of Science", "id": 44, "label": "An Extensive Comparison of Static Application Security Testing Tools", "shape": "dot", "size": 10, "title": "Authors: Matteo Esposito, Valentina Falaschi, Davide Falessi\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Context: Static Application Security Testing Tools (SASTTs) iden-\ntify software vulnerabilities to support the security and reliability of\nsoftware applications. Interestingly, several studies have suggested\nthat alternative solutions may be more effective than SASTTs due\nto their tendency to generate false alarms, commonly referred to as\nlow Precision. Aim: We aim to comprehensively evaluate SASTTs,\nsetting a reliable benchmark for assessing and finding gaps in vul-\nnerability identification mechanisms based on SASTTs or alterna-\ntives. Method: Our SASTTs evaluation is based on a controlled,\nthough synthetic, Java codebase. It involves an assessment of 1.5\nmillion test executions, and it features innovative methodological\nfeatures such as effort-aware accuracy metrics and method-level\nanalysis. Results: Our findings reveal that SASTTs detect a tiny\nrange of vulnerabilities. In contrast to prevailing wisdom, SASTTs\nexhibit high Precision while falling short in Recall. Conclusions:\nOur findings suggest that enhancing Recall, alongside expanding\nthe spectrum of detected vulnerability types, should be the primary\nfocus for improving SASTTs or alternative approaches, such as\nmachine learning-based vulnerability identification solutions.\nCCS CONCEPTS\n\u2022 Security and privacy \u2192Software security engineering; Domain-\nspecific security and privacy architectures; \u2022 Software and its engi-\nneering \u2192Software defect analysis; Empirical software valida-\ntion.\nKEYWORDS\nSecurity Assessment Tool, Static Application Security Testing, Com-\nmon Vulnerability Exposure, Common Weakness Enumeration\nACM Reference Format:\nMatteo Esposito, Valentina Falaschi, and Davide Falessi . 2024. An Extensive\nComparison of Static Application Security Testing Tools. In Proceedings of\nThe 28th International Conference on Evaluation and Assessment in Software\nEngineering (EASE 2024). ACM, New York, NY, USA, 10 pages. https://doi.or\ng/XXXXXXX.XXXXXXX"}, {"group": "History of Science", "id": 52, "label": "ITERATIVE FORGETTING: ONLINE DATA STREAM REGRESSION USING DATABASE-INSPIRED ADAPTIVE GRANULATION \u2217", "shape": "dot", "size": 10, "title": "Authors: Niket Kathiriya, Hossein Haeri, Cindy Chen, Kshitij Jerath\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Many modern systems, such as financial, transportation and telecommunications systems, are time-\nsensitive in the sense that they demand low-latency predictions for real-time decision-making. Such\nsystems often have to contend with continuous unbounded data streams as well as concept drift,\nwhich are challenging requirements that traditional regression techniques are unable to cater to. There\nexists a need to create novel data stream regression methods that can handle these scenarios. We\npresent a database-inspired datastream regression model that (a) uses inspiration from R*-trees to\ncreate granules from incoming datastreams such that relevant information is retained, (b) iteratively\nforgets granules whose information is deemed to be outdated, thus maintaining a list of only recent,\nrelevant granules, and (c) uses the recent data and granules to provide low-latency predictions.\nThe R*-tree-inspired approach also makes the algorithm amenable to integration with database\nsystems. Our experiments demonstrate that the ability of this method to discard data produces a\nsignificant order-of-magnitude improvement in latency and training time when evaluated against the\nmost accurate state-of-the-art algorithms, while the R*-tree-inspired granulation technique provides\ncompetitively accurate predictions.\nKeywords Data stream, forgetting, regression, granulation, aggregation, time-sensitive systems, low latency, concept\ndrift"}, {"group": "History of Science", "id": 55, "label": "Worst-Case Convergence Time of ML Algorithms via  Extreme Value Theory", "shape": "dot", "size": 10, "title": "Authors: Sriram Sankaranarayanan\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: This paper leverages the statistics of extreme values to predict\nthe worst-case convergence times of machine learning algorithms.\nTiming is a critical non-functional property of ML systems, and\nproviding the worst-case converge times is essential to guarantee\nthe availability of ML and its services. However, timing properties\nsuch as worst-case convergence times (WCCT) are difficult to verify\nsince (1) they are not encoded in the syntax or semantics of under-\nlying programming languages of AI, (2) their evaluations depend\non both algorithmic implementations and underlying systems, and\n(3) their measurements involve uncertainty and noise. Therefore,\nprevalent formal methods and statistical models fail to provide rich\ninformation on the amounts and likelihood of WCCT.\nOur key observation is that the timing information we seek rep-\nresents the extreme tail of execution times. Therefore, extreme\nvalue theory (EVT), a statistical discipline that focuses on under-\nstanding and predicting the distribution of extreme values in the\ntail of outcomes, provides an ideal framework to model and analyze\nWCCT in the training and inference phases of ML paradigm. Build-\ning upon the mathematical tools from EVT, we propose a practical\nframework to predict the worst-case timing properties of ML. Over\na set of linear ML training algorithms, we show that EVT achieves\na better accuracy for predicting WCCTs than relevant statistical\nmethods such as the Bayesian factor. On the set of larger machine\nlearning training algorithms and deep neural network inference,\nwe show the feasibility and usefulness of EVT models to accurately\npredict WCCTs, their expected return periods, and their likelihood.\nACM Reference Format:\nSaeid Tizpaz-Niari and Sriram Sankaranarayanan. 2024. Worst-Case Con-\nvergence Time of ML Algorithms via Extreme Value Theory. In Confer-\nence on AI Engineering - Software Engineering for AI (CAIN 2024), April\n14\u201315, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3644815.3644989"}, {"group": "History of Science", "id": 58, "label": "Weighted Scaling Approach for Metabolomics Data Analysis", "shape": "dot", "size": 10, "title": "Authors: Nishith Kumar, Aminul Hoque, Ashad Alam\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 60, "label": "SFILES 2.0", "shape": "dot", "size": 10, "title": "Authors: Gabriel Vogel, Lukas Schulze, Edwin Hirtreiter, Artur M. Schweidtmann\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: SFILES is a text-based notation for chemical process \ufb02owsheets. It was originally proposed by\nd\u2019Anterroches [1] who was inspired by the text-based SMILES notation for molecules. The text-\nbased format has several advantages compared to \ufb02owsheet images regarding the storage format,\ncomputational accessibility, and eventually for data analysis and processing. However, the original\nSFILES version cannot describe essential \ufb02owsheet con\ufb01gurations unambiguously, such as the\ndistinction between top and bottom products. Neither is it capable of describing the control structure\nrequired for the safe and reliable operation of chemical processes. Also, there is no publicly\navailable software for decoding or encoding chemical process topologies to SFILES. We propose\nthe SFILES 2.0 with a complete description of the extended notation and naming conventions.\nAdditionally, we provide open-source software for the automated conversion between \ufb02owsheet\ngraphs and SFILES 2.0 strings. This way, we hope to encourage researchers and engineers to publish\ntheir \ufb02owsheet topologies as SFILES 2.0 strings. The ultimate goal is to set the standards for creating\na FAIR database of chemical process \ufb02owsheets, which would be of great value for future data\nanalysis and processing.\nKeywords Flowsheet graph \u00b7 Process \ufb02ow diagram \u00b7 Arti\ufb01cial intelligence \u00b7 FAIR data \u00b7 STRING notation"}, {"group": "History of Science", "id": 63, "label": "An Optimal Likelihood Free Method for Biological Model Selection", "shape": "dot", "size": 10, "title": "Authors: Vincent D. Zaballa, Elliot E. Hui\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Systems biology seeks to create math models of\nbiological systems to reduce inherent biological\ncomplexity and provide predictions for applica-\ntions such as therapeutic development. However,\nit remains a challenge to determine which math\nmodel is correct and how to arrive optimally at the\nanswer. We present an algorithm for automated\nbiological model selection using mathematical\nmodels of systems biology and likelihood free in-\nference methods. Our algorithm shows improved\nperformance in arriving at correct models without\na priori information over conventional heuristics\nused in experimental biology and random search.\nThis method shows promise to accelerate biologi-\ncal basic science and drug discovery.\n1."}, {"group": "History of Science", "id": 67, "label": "TripHLApan:  predicting  HLA  molecules  binding", "shape": "dot", "size": 10, "title": "Authors: Chuqi Lei, Jianxin Wang, Yaohang Li, Min Li\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Natural Language Processing", "id": 70, "label": "Modeling Diverse Chemical Reactions for Single-step Retrosynthesis via Discrete Latent Variables", "shape": "dot", "size": 10, "title": "Authors: Jie Wang\u2217\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Single-step retrosynthesis is the cornerstone of retrosynthesis plan-\nning, which is a crucial task for computer-aided drug discovery.\nThe goal of single-step retrosynthesis is to identify the possible\nreactants that lead to the synthesis of the target product in one\nreaction. By representing organic molecules as canonical strings,\nexisting sequence-based retrosynthetic methods treat the product-\nto-reactant retrosynthesis as a sequence-to-sequence translation\nproblem. However, most of them struggle to identify diverse chemi-\ncal reactions for a desired product due to the deterministic inference,\nwhich contradicts the fact that many compounds can be synthesized\nthrough various reaction types with different sets of reactants.\nIn this work, we aim to increase reaction diversity and generate\nvarious reactants using discrete latent variables. We propose a novel\nsequence-based approach, namely RetroDCVAE, which incorpo-\nrates conditional variational autoencoders into single-step retrosyn-\nthesis and associates discrete latent variables with the generation\nprocess. Specifically, RetroDCVAE uses the Gumbel-Softmax distri-\nbution to approximate the categorical distribution over potential re-\nactions and generates multiple sets of reactants with the variational\ndecoder. Experiments demonstrate that RetroDCVAE outperforms\nstate-of-the-art baselines on both benchmark dataset and home-\nmade dataset. Both quantitative and qualitative results show that\nRetroDCVAE can model the multi-modal distribution over reaction\ntypes and produce diverse reactant candidates.\n\u2217Corresponding Author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM \u201922, October 17\u201322, 2022, Atlanta, GA\n\u00a9 2022 Association for Computing Machinery.\nCCS CONCEPTS\n\u2022 Applied computing \u2192Computational biology; Chemistry; \u2022\nComputing methodologies \u2192Natural language processing; La-\ntent variable models.\nKEYWORDS\nRetrosynthesis, Variational Autoencoder, Transformer, Graph Neu-\nral Network, Discrete Latent Variable\nACM Reference Format:\nHuarui He, Jie Wang, Yunfei Liu, and Feng Wu. 2022. Modeling Diverse\nChemical Reactions for Single-step Retrosynthesis via Discrete Latent Vari-\nables. In Proceedings of Make sure to enter the correct conference title from your\nrights confirmation emai (CIKM \u201922). ACM, New York, NY, USA, 11 pages."}, {"group": "History of Science", "id": 71, "label": "Full Paper", "shape": "dot", "size": 10, "title": "Authors: Chengyu Liu, Wei Wang,, Wei Wang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Developing models with high interpretability and even deriving formulas to quantify relationships\nbetween biological data is an emerging need. We propose here a framework for ab initio derivation of\nsequence motifs and linear formula using a new approach based on the interpretable neural network\nmodel called contextual regression model. We showed that this linear model could predict gene\nexpression levels using promoter sequences with a performance comparable to deep neural network\nmodels. We uncovered a list of 300 motifs with important regulatory roles on gene expression and showed\nthat they also had significant contributions to cell-type specific gene expression in 154 diverse cell types.\nThis work illustrates the possibility of deriving formulas to represent biology laws that may not be easily\nelucidated. (https://github.com/Wang-lab-UCSD/Motif_Finding_Contextual_Regression)"}, {"group": "History of Science", "id": 73, "label": "Springer Nature 2021 LATEX template", "shape": "dot", "size": 10, "title": "Authors: Yaosen Min\u2020, Ye Wei*\u2020, Peizhuo Wang\u2020, Xiaoting, Wang, Han Li, Nian Wu, Stefan Bauer, Shuxin Zheng, Yu, Shi, Yingheng Wang, Ji Wu, Dan Zhao, JianyangZeng,, wuji\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Natural Language Processing", "id": 91, "label": "Reconfigurable Robot Identification from Motion Data", "shape": "dot", "size": 10, "title": "Authors: Yuhang Hu, Yunzhe Wang, Ruibo Liu, Zhou Shen, Hod Lipson\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Integrating Large Language Models (LLMs) and\nVision-Language Models (VLMs) with robotic systems enables\nrobots to process and understand complex natural language\ninstructions and visual information. However, a fundamental\nchallenge remains: for robots to fully capitalize on these\nadvancements, they must have a deep understanding of their\nphysical embodiment. The gap between AI models\u2019 cognitive\ncapabilities and the understanding of physical embodiment\nleads to the following question: Can a robot autonomously\nunderstand and adapt to its physical form and functionali-\nties through interaction with its environment? This question\nunderscores the transition towards developing self-modeling\nrobots without reliance on external sensory or pre-programmed\nknowledge about their structure. Here, we propose a meta-\nself-modeling that can deduce robot morphology through pro-\nprioception\u2014the robot\u2019s internal sense of its body\u2019s position\nand movement. Our study introduces a 12-DoF reconfigurable\nlegged robot, accompanied by a diverse dataset of 200k unique\nconfigurations, to systematically investigate the relationship\nbetween robotic motion and robot morphology. Utilizing a deep\nneural network model comprising a robot signature encoder\nand a configuration decoder, we demonstrate the capability\nof our system to accurately predict robot configurations from\nproprioceptive signals. This research contributes to the field of\nrobotic self-modeling, aiming to enhance robot\u2019s understanding\nof their physical embodiment and adaptability in real-world\nscenarios.\nI."}, {"group": "History of Science", "id": 76, "label": "Microsoft Word - _scientific_data__meg_masc-3.docx", "shape": "dot", "size": 10, "title": "Authors: Graham Flick, Alec Marantz, Liina Pylkka\u00a8, David, Poeppel, JeanRe\u00b4 mi King\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Natural Language Processing", "id": 6, "label": "2024 18th International Conference on Automatic Face and Gesture Recognition (FG)", "shape": "dot", "size": 10, "title": "Authors: Harry Walsh, Abolfazl Ravanshad, Mariam Rahmani, Richard Bowden\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Phonetic representations are used when record-\ning spoken languages, but no equivalent exists for recording\nsigned languages. As a result, linguists have proposed several\nannotation systems that operate on the gloss or sub-unit level;\nhowever, these resources are notably irregular and scarce.\nSign Language Production (SLP) aims to automatically\ntranslate spoken language sentences into continuous sequences\nof sign language. However, current state-of-the-art approaches\nrely on scarce linguistic resources to work. This has limited\nprogress in the field. This paper introduces an innovative solu-\ntion by transforming the continuous pose generation problem\ninto a discrete sequence generation problem. Thus, overcoming\nthe need for costly annotation. Although, if available, we\nleverage the additional information to enhance our approach.\nBy applying Vector Quantisation (VQ) to sign language\ndata, we first learn a codebook of short motions that can\nbe combined to create a natural sequence of sign. Where\neach token in the codebook can be thought of as the lexicon\nof our representation. Then using a transformer we perform\na translation from spoken language text to a sequence of\ncodebook tokens. Each token can be directly mapped to a\nsequence of poses allowing the translation to be performed\nby a single network. Furthermore, we present a sign stitching\nmethod to effectively join tokens together. We evaluate on\nthe RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the\nmore challenging meineDGST (mDGS) datasets. An extensive\nevaluation shows our approach outperforms previous methods,\nincreasing the BLEU-1 back translation score by up to 72%.\nI."}, {"group": "Natural Language Processing", "id": 9, "label": "Related Work and Citation Text Generation: A Survey", "shape": "dot", "size": 10, "title": "Authors: Jessica Ouyang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: To convince readers of the novelty of their re-\nsearch paper, authors must perform a litera-\nture review and compose a coherent story that\nconnects and relates prior works to the cur-\nrent work. This challenging nature of literature\nreview writing makes automatic related work\ngeneration (RWG) academically and compu-\ntationally interesting, and also makes it an ex-\ncellent test bed for examining the capability of\nSOTA natural language processing (NLP) mod-\nels. Since the initial proposal of the RWG task,\nits popularity has waxed and waned, following\nthe capabilities of mainstream NLP approaches.\nIn this work, we survey the zoo of RWG his-\ntorical works, summarizing the key approaches\nand task definitions and discussing the ongoing\nchallenges of RWG."}, {"group": "Network Science", "id": 28, "label": "Exploring Boundaries and Intensities in Offensive and Hate Speech: Unveiling the Complex Spectrum of Social Media Discourse", "shape": "dot", "size": 10, "title": "Authors: Abew Ali Ayele,, Esubalew Alemneh Jalew, Adem Chanie Ali, Seid Muhie Yimam, Chris Biemann\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The prevalence of digital media and evolving sociopolitical dynamics have significantly amplified the dissemination\nof hateful content. Existing studies mainly focus on classifying texts into binary categories, often overlooking the\ncontinuous spectrum of offensiveness and hatefulness inherent in the text. In this research, we present an extensive\nbenchmark dataset for Amharic, comprising 8,258 tweets annotated for three distinct tasks: category classification,\nidentification of hate targets, and rating offensiveness and hatefulness intensities. Our study highlights that a\nconsiderable majority of tweets belong to the less offensive and less hate intensity levels, underscoring the need for\nearly interventions by stakeholders. The prevalence of ethnic and political hatred targets, with significant overlaps in\nour dataset, emphasizes the complex relationships within Ethiopia\u2019s sociopolitical landscape. We build classification\nand regression models and investigate the efficacy of models in handling these tasks. Our results reveal that hate and\noffensive speech can not be addressed by a simplistic binary classification, instead manifesting as variables across a\ncontinuous range of values. The Afro-XLMR-large model exhibits the best performances achieving F1-scores of\n75.30%, 70.59%, and 29.42% for the category, target, and regression tasks, respectively. The 80.22% correlation\ncoefficient of the Afro-XLMR-large model indicates strong alignments.\nKeywords: Intensity, Hatefulness, Offensiveness, Rating scale\n1."}, {"group": "History of Science", "id": 47, "label": "SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions", "shape": "dot", "size": 10, "title": "Authors: Gang Liao, Yonghua Ding, Le Cai, Jianjun Chen, ye.liu, yonghua.ding, le.cai\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The ubiquity of variable-length integers in data storage and\ncommunication necessitates efficient decoding techniques. In\nthis paper, we present SFVInt, a simple and fast approach to\ndecode the prevalent Little Endian Base-128 (LEB128) varints.\nOur approach, distilled into a mere 500 lines of code, effec-\ntively utilizes the Bit Manipulation Instruction Set 2 (BMI2)\nin modern Intel and AMD processors, achieving significant\nperformance improvement while maintaining simplicity and\navoiding overengineering. SFVInt, with its generic design,\neffectively processes both 32-bit and 64-bit unsigned inte-\ngers using a unified code template, marking a significant\nleap forward in varint decoding efficiency. We thoroughly\nevaluate SFVInt\u2019s performance across various datasets and\nscenarios, demonstrating that it achieves up to a 2x increase\nin decoding speed when compared to varint decoding meth-\nods used in established frameworks like Facebook Folly and\nGoogle Protobuf."}, {"group": "History of Science", "id": 64, "label": "2 Non-Gaussian displacement distributions in models of 2 0 2", "shape": "dot", "size": 10, "title": "Authors: Igor M. Sokolov, Ralf Metzler\u2020,\u266f, Aleksei V. Chechk\u2020,\u2021,\u00a7\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: .\nWe study the e\ufb00ect of randomly distributed di\ufb00usivities and speeds in\ntwo models for active particle dynamics with active and passive \ufb02uctuations.\nWe\ndemonstrate how non-Gaussian displacement distributions emerge in these models in\nthe long time limit, including Cauchy-type and exponential (Laplace) shapes. Notably\nthe resulting shapes of the displacement distributions with distributed di\ufb00usivities for\nthe active models considered here are in striking contrast to passive di\ufb00usion models.\nFor the active motion models our discussion points out the di\ufb00erences between active-\nand passive-noise. Speci\ufb01cally, we demonstrate that the case with active-noise is in\nnice agreement with measured data for the displacement distribution of social amoeba.\n1."}, {"group": "History of Science", "id": 68, "label": " Clustering Optimisation Method for Highly Connected Biological Data ", "shape": "dot", "size": 10, "title": "Authors: Richard Tj\u00f6rnhammara\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Currently, data-driven discovery in biological sciences resides in \ufb01nding seg-\nmentation strategies in multivariate data that produce sensible descriptions of the\ndata. Clustering is but one of several approaches and sometimes falls short because\nof dif\ufb01culties in assessing reasonable cutoffs, the number of clusters that need to\nbe formed or that an approach fails to preserve topological properties of the orig-\ninal system in its clustered form. In this work, we show how a simple metric for\nconnectivity clustering evaluation leads to an optimised segmentation of biological\ndata.\nThe novelty of the work resides in the creation of a simple optimisation method\nfor clustering crowded data. The resulting clustering approach only relies on met-\nrics derived from the inherent properties of the clustering. The new method facili-\ntates knowledge for optimised clustering, which is easy to implement.\nWe discuss how the clustering optimisation strategy corresponds to the viable\ninformation content yielded by the \ufb01nal segmentation. We further elaborate on\nhow the clustering results, in the optimal solution, corresponds to prior knowledge\nof three different data sets.\nKeywords: Clustering, Connectivity, Unimodal Optimisation, Dimensionality\nReduction, Statistical Learning, Hierarchical Agglomerative Clustering\n1."}, {"group": "History of Science", "id": 74, "label": "Predicting microsatellite instability and key biomarkers in colorectal cancer", "shape": "dot", "size": 10, "title": "Authors: Xgyu Li, Jitendra Jonnagaddala, Hong Zhang, Xu Steven Xu, Jitendra Jonnagaddala, Hong Zhang, Xu Steven Xu\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 78, "label": "Simulating how animals learn: a new modelling framework", "shape": "dot", "size": 10, "title": "Authors: Peter R. Thompson,\u2217M\u00b4elodie Kunegel-Lion\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 81, "label": "Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)", "shape": "dot", "size": 10, "title": "Authors: Jeongeun Park, Taemoon Jeong, Hyeonseong Kim, Taehyun Byun, Seungyoon Sh, Keunjun Choi, Jaewoon Kwon, Taeyoon Lee, Matthew Pan, Sungjoon Choi\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 This paper presents the design and development\nof an innovative interactive robotic system to enhance au-\ndience engagement using character-like personas. Built upon\nthe foundations of persona-driven dialog agents, this work\nextends the agent\u2019s application to the physical realm, employing\nrobots to provide a more immersive and interactive experience.\nThe proposed system, named the Masquerading Animated\nSocial Kinematics (MASK), leverages an anthropomorphic\nrobot which interacts with guests using non-verbal interactions,\nincluding facial expressions and gestures. A behavior generation\nsystem based upon a finite-state machine structure effectively\nconditions robotic behavior to convey distinct personas The\nMASK framework integrates a perception engine, a behav-\nior selection engine, and a comprehensive action library to\nenable real-time, dynamic interactions with minimal human\nintervention in behavior design. Throughout the user subject\nstudies, we examined whether the users could recognize the\nintended character in film-character-based persona conditions.\nWe conclude by discussing the role of personas in interactive\nagents and the factors to consider for creating an engaging user\nexperience.\nI."}, {"group": "Network Science", "id": 96, "label": "Format guide for AIRCC", "shape": "dot", "size": 10, "title": "Authors: David C. Wyld, Emmanuel Etuh, Francis S. Bakpo, Eneh Agozie H\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: We live in a virtual world where actual lifestyles are replicated.  The growing reliance on the \nuse of social media networks worldwide has resulted in great concern for information security. \nOne of the factors popularizing the social media platforms is how they connect people \nworldwide to interact, share content, and engage in mutual interactions of common interest that \ncut across geographical boundaries. Behind all these incredible gains are digital crime \nequivalence that threatens the physical socialization. Criminal minded elements and hackers \nare exploiting Social Media Platforms (SMP) for many nefarious activities to harm others. As \ndetection tools are developed to control these crimes, hackers\u2019 tactics and techniques are \nconstantly evolving. Hackers are constantly developing new attacking tools and hacking \nstrategies to gain malicious access to systems and attack social media network thereby making \nit difficult for security administrators and organizations to develop and implement the proper \npolicies and procedures necessary to prevent the hackers\u2019 attacks. The increase in cyber-attacks \non the social media platforms calls for urgent and more intelligent security measures to \nenhance the effectiveness of social media platforms. This paper explores the mode and tactics of \nhackers\u2019 mode of attacks on social media and ways of preventing their activities against users to \nensure secure social cyberspace and enhance virtual socialization. Social media platforms are \nbriefly categorized, the various types of attacks are also highlighted with current state-of-the-art  \npreventive mechanisms to overcome the attacks as proposed in research works, finally, social \nmedia intrusion detection mechanism is suggested as a second line of defence to combat \ncybercrime on social media networks \n \nKEYWORDS \n \nIntrusion Detection System, Data Warehouse, Machine Learning, Hackers, Social Media \nPlatform, Online Social Network Intrusion. \n \n1."}, {"group": "Network Science", "id": 99, "label": "Understanding how people consume low quality and extreme news using web traf\ufb01c data", "shape": "dot", "size": 10, "title": "Authors: Zhouhan Chen, Juliana Freire, Jonathan Nagler, Joshua A. Tucker, Haohan Chen\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: To mitigate the spread of fake news, researchers need to un-\nderstand who visit fake new sites, what brings people to those\nsites, where visitors come from, and what content they pre-\nfer to consume. In this paper, we analyze web traf\ufb01c data\nfrom The Gateway Pundit (TGP), a popular far-right website\nthat is known for repeatedly sharing false information that\nhas made its web traf\ufb01c available to the general public. We\ncollect data on 68 million web traf\ufb01c visits to the site over\na month period and analyze how people consume news via\nmultiple features. Our traf\ufb01c analysis shows that search en-\ngines and social media platforms are main drivers of traf\ufb01c;\nour geo-location analysis reveals that TGP is more popular in\ncounties that voted for Trump in 2020; and our topic analy-\nsis shows that conspiratorial articles receive more visits than\nfactual articles.\nDue to the inability to observe direct website traf\ufb01c, exist-\ning research uses alternative data source such as engage-\nment signals from social media posts. To validate if social\nmedia engagement signals correlate with actual web visit\ncounts, we collect all Facebook and Twitter posts with URLs\nfrom TGP during the same time period. We show that all en-\ngagement signals positively correlate with web visit counts,\nbut with varying correlation strengths. Metrics based on Face-\nbook posts correlate better than metrics based on Twitter. Our\nunique web traf\ufb01c data set and insights can help researchers\nto better measure the impact of far-right and fake news URLs\non social media platforms."}, {"group": "Network Science", "id": 101, "label": "No Community Can Do Everything: Why People Participate in Similar Online Communities", "shape": "dot", "size": 10, "title": "Authors: NATHAN TEBLUNTHUIS, CHARLES KIENE, ISABELLA BROWN, LAURA (ALIA) LEVI, USANICOLE MCGINNIS, BENJAMIN MAKO HILL, Nathan TeBlunthuis, Charles Kiene, Isabella Brown, Laura (Alia) Levi, Nicole McGnis, Benjam Mako, Hill\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Network Science", "id": 104, "label": "This Must Be the Place: Predicting Engagement of Online Communities in a Large-scale Distributed Campaign", "shape": "dot", "size": 10, "title": "Authors: Abrhm Isreli, Alexnder Kreminsky, Oren Tsur\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Understanding collective decision making at a large-scale, and elu-\ncidating how community organization and community dynamics\nshape collective behavior are at the heart of social science research.\nIn this work we study the behavior of thousands of communities\nwith millions of active members. We define a novel task: predict-\ning which community will undertake an unexpected, large-scale,\ndistributed campaign. To this end, we develop a hybrid model, com-\nbining textual cues, community meta-data, and structural proper-\nties. We show how this multi-faceted model can accurately predict\nlarge-scale collective decision-making in a distributed environment.\nWe demonstrate the applicability of our model through Reddit\u2019s\nr/place \u2013 a large-scale online experiment in which millions of\nusers, self-organized in thousands of communities, clashed and\ncollaborated in an effort to realize their agenda.\nOur hybrid model achieves a high F1 prediction score of 0.826.\nWe find that coarse meta-features are as important for prediction\naccuracy as fine-grained textual cues, while explicit structural fea-\ntures play a smaller role. Interpreting our model, we provide and\nsupport various social insights about the unique characteristics of\nthe communities that participated in the r/place experiment.\nOur results and analysis shed light on the complex social dy-\nnamics that drive collective behavior, and on the factors that pro-\npel user coordination. The scale and the unique conditions of the\nr/place experiment suggest that our findings may apply in broader\ncontexts, such as online activism, (countering) the spread of hate\nspeech and reducing political polarization. The broader applica-\nbility of the model is demonstrated through an extensive analysis\nof the WallStreetBets community, their role in r/place and four\nyears later, in the GameStop short squeeze campaign of 2021.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Neural networks; \u2022 Informa-\ntion systems \u2192Social networks; \u2022 Applied computing \u2192Ethnog-\nraphy; Sociology; \u2022 Social and professional topics \u2192Cultural\ncharacteristics;\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW \u201922, April 25\u201329, 2022, Virtual Event, Lyon, France\n\u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9096-5/22/04...$15.00\nhttps://doi.org/10.1145/3485447.3512238\nKEYWORDS\nOnline Communities, Natural Language Processing, Social Net-\nworks, Computational Social Science, Reddit, rPlace, wallStreetBets,\nGameStop\nACM Reference Format:\nAbraham Israeli, Alexander Kremiansky, Oren Tsur. 2022. This Must Be\nthe Place: Predicting Engagement of Online Communities in a Large-scale\nDistributed Campaign. In Proceedings of the ACM Web Conference 2022\n(WWW \u201922), April 25\u201329, 2022, Virtual Event, Lyon, France. ACM, New York,\nNY, USA, 12 pages. https://doi.org/10.1145/3485447.3512238"}, {"group": "Network Science", "id": 105, "label": "NCPAM16_313", "shape": "dot", "size": 10, "title": "Authors: B Jaganathan, Pankaj Shukla\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014The purpose of the research is to find a centrality \nmeasure that can be used in place of PageRank and to find out \nthe conditions where we can use it in place of PageRank. After \nanalysis and comparison of graphs with a large number of nodes \nusing Spearman\u2019s Rank Coefficient Correlation, the conclusion is \nevident that Eigenvector can be safely used in place of PageRank \nin directed networks to improve the performance in terms of the \ntime complexity.\nKeywords\u2014 PageRank, Centrality, Eigenvector, Webgraph\nI."}, {"group": "Network Science", "id": 108, "label": "\u201cI Can\u2019t Keep It Up.\u201d A Dataset from the Defunct Voat.co News Aggregator", "shape": "dot", "size": 10, "title": "Authors: min Mekacher\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Voat.co was a news aggregator website that shut down on De-\ncember 25, 2020. The site had a troubled history and was\nknown for hosting various banned subreddits.\nThis paper\npresents a dataset with over 2.3M submissions and 16.2M\ncomments posted from 113K users in 7.1K subverses (the\nequivalent of subreddit for Voat).\nOur dataset covers the\nwhole lifetime of Voat, from its developing period starting\non November 8, 2013, the day it was founded, April 2014, up\nuntil the day it shut down (December 25, 2020).\nThis work presents the largest and most complete publicly\navailable Voat dataset, to the best of our knowledge. Along\nwith the release of this dataset, we present a preliminary anal-\nysis covering posting activity and daily user and subverse reg-\nistration on the platform so that researchers interested in our\ndataset can know what to expect.\nOur data may prove helpful to false news dissemination\nstudies as we analyze the links users share on the platform,\n\ufb01nding that many communities rely on alternative news press,\nlike Breitbart and GatewayPundit, for their daily discussions.\nIn addition, we perform network analysis on user interactions\n\ufb01nding that many users prefer not to interact with subverses\noutside their narrative interests, which could be helpful to re-\nsearchers focusing on polarization and echo chambers. Also,\nsince Voat was one of the platforms banned Reddit commu-\nnities migrated to, we are con\ufb01dent our dataset will motivate\nand assist researchers studying deplatforming. Finally, many\nhateful and conspiratorial communities were very popular on\nVoat, which makes our work valuable for researchers focus-\ning on toxicity, conspiracy theories, cross-platform studies of\nsocial networks, and natural language processing."}, {"group": "Network Science", "id": 112, "label": "FacebookConfounding_v1.docx", "shape": "dot", "size": 10, "title": "Authors: Alexander Ruch, Yujia Zhang, Michael Macy\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Lifestyle politics emerge when activities that have no substantive relevance to ideology\nbecome politically aligned and polarized. Homophily and social influence are able\ngenerate these fault lines on their own; however, social identities from demographics may\nserve as coordinating mechanisms through which lifestyle politics are mobilized are\nspread. Using a dataset of 137,661,886 observations from 299,327 Facebook interests\naggregated across users of different racial/ethnic, education, age, gender, and income\ndemographics, we find that the most extreme instances of lifestyle politics are those\nwhich are highly confounded by demographics such as race/ethnicity (e.g., Black artists\nand performers). After adjusting political alignment for demographic effects, lifestyle\npolitics decreased by 27.36% toward the political \u201ccenter\u201d and demographically\nconfounded interests were no longer among the most polarized interests. Instead, after\ndemographic deconfounding, we found that the most liberal interests included electric\ncars, Planned Parenthood, and liberal satire while the most conservative interests included\nthe Republican Party and conservative commentators. We validate our measures of\npolitical alignment and lifestyle politics using the General Social Survey and find similar\ndemographic entanglements with lifestyle politics existed before social media such as\nFacebook were ubiquitous, giving us strong confidence that our results are not due to\necho chambers or filter bubbles. Likewise, since demographic characteristics exist prior\nto ideological values, we argue that the demographic confounding we observe is causally\nresponsible for the extreme instances of lifestyle politics that we find among the\naggregated interests. We conclude our paper by relating our results to Simpson\u2019s paradox,\ncultural omnivorousness, and network autocorrelation.\nKeywords: social networks, social media, polarization, lifestyle politics, demographics."}, {"group": "History of Science", "id": 122, "label": "Rethinking the Graph Polynomial Filter via Positive and Negative Coupling Analysis", "shape": "dot", "size": 10, "title": "Authors: Haodong Wen\u2217, Deyu Meng\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Network Science", "id": 126, "label": "X-posing Free Speech: Examining the Impact of Moderation Relaxation on Online Social Networks", "shape": "dot", "size": 10, "title": "Authors: Saurav Chhatani, Pnurangam Kumaraguru\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: We investigate the impact of free speech and\nthe relaxation of moderation on online social\nmedia platforms using Elon Musk\u2019s takeover\nof Twitter as a case study.\nBy curating a\ndataset of over 10 million tweets, our study\nemploys a novel framework combining content\nand network analysis. Our findings reveal a sig-\nnificant increase in the distribution of certain\nforms of hate content, particularly targeting the\nLGBTQ+ community and liberals. Network\nanalysis reveals the formation of cohesive hate\ncommunities facilitated by influential bridge\nusers, with substantial growth in interactions\nhinting at increased hate production and dif-\nfusion. By tracking the temporal evolution of\nPageRank, we identify key influencers, primar-\nily self-identified far-right supporters dissem-\ninating hate against liberals and woke culture.\nIronically, embracing free speech principles ap-\npears to have enabled hate speech against the\nvery concept of freedom of expression and free\nspeech itself. Our findings underscore the del-\nicate balance platforms must strike between\nopen expression and robust moderation to curb\nthe proliferation of hate online."}, {"group": "History of Science", "id": 137, "label": "Commentaries on the publication entitled", "shape": "dot", "size": 10, "title": "Authors: Hgyi Li, Hgyi Li, Ernest Starling, Guyt, Levick, Kihara, Catchpole, G. Hauck\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Natural Language Processing", "id": 8, "label": "Quantifying Multilingual Performance of Large Language Models Across Languages", "shape": "dot", "size": 10, "title": "Authors: Zihao Li\u2217, Yucheng Shi, Zirui Liu, Fan Yang, Ninghao Liu, Mengnan Du\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The training process of Large Language Mod-\nels (LLMs) requires extensive text corpus.\nHowever, these data are often unevenly dis-\ntributed in different languages. As a result,\nLLMs perform well on common languages,\nsuch as English, German, and French, but per-\nform poorly on low-resource languages. How-\never, currently there is no work to quantita-\ntively measure the performance of LLMs in\nlow-resource languages. To fill this gap, we\nproposed the Language Ranker that aims to\nbenchmark and rank different languages ac-\ncording to the performance of LLMs on those\nlanguages. We employ the LLM\u2019s performance\non the English corpus as a baseline to compare\nthe performances of different languages and\nEnglish. We have the following three findings:\n1. The performance rankings of different LLMs\nin all languages are roughly the same. 2. LLMs\nwith different sizes have the same partial order\nof performance. 3. There is a strong correlation\nbetween LlaMa2\u2019s performance in different lan-\nguages and the proportion of the pre-training\ncorpus. These findings illustrate that the Lan-\nguage Ranker can be used as an indicator to\nmeasure the language performance of LLMs."}, {"group": "Natural Language Processing", "id": 10, "label": "ADVANCING SPEECH TRANSLATION: A CORPUS OF MANDARIN-ENGLISH CONVERSATIONAL TELEPHONE SPEECH", "shape": "dot", "size": 10, "title": "Authors: Shannon Wotherspoon, William Hartmann, Matthew Snover, william.hartmann\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: This paper introduces a set of English translations for a\n123-hour subset of the CallHome Mandarin Chinese data\nand the HKUST Mandarin Telephone Speech data for the\ntask of speech translation.\nPaired source-language speech\nand target-language text is essential for training end-to-end\nspeech translation systems and can provide substantial perfor-\nmance improvements for cascaded systems as well, relative to\ntraining on more widely available text data sets. We demon-\nstrate that \ufb01ne-tuning a general-purpose translation model to\nour Mandarin-English conversational telephone speech train-\ning set improves target-domain BLEU by more than 8 points,\nhighlighting the importance of matched training data.\nIndex Terms\u2014 speech translation, corpus, conversational\ntelephone speech\n1."}, {"group": "Natural Language Processing", "id": 11, "label": "MEMLLM: Finetuning LLMs to Use An Explicit Read-Write Memory", "shape": "dot", "size": 10, "title": "Authors: Abdullatif K\u00f6ksal,, Ayyoob Imani,, Mohsen Fayyaz\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: While current large language models (LLMs)\ndemonstrate some capabilities in knowledge-\nintensive tasks, they are limited by relying on\ntheir parameters as an implicit storage mecha-\nnism. As a result, they struggle with infrequent\nknowledge and temporal degradation. In addi-\ntion, the uninterpretable nature of parametric\nmemorization makes it challenging to under-\nstand and prevent hallucination. Parametric\nmemory pools and model editing are only par-\ntial solutions. Retrieval Augmented Generation\n(RAG) \u2013 though non-parametric \u2013 has its own\nlimitations: it lacks structure, complicates in-\nterpretability and makes it hard to effectively\nmanage stored knowledge. In this paper, we\nintroduce MEMLLM, a novel method of en-\nhancing LLMs by integrating a structured and\nexplicit read-and-write memory module. MEM-\nLLM tackles the aforementioned challenges by\nenabling dynamic interaction with the memory\nand improving the LLM\u2019s capabilities in using\nstored knowledge. Our experiments indicate\nthat MEMLLM enhances the LLM\u2019s perfor-\nmance and interpretability, in language model-\ning in general and knowledge-intensive tasks in\nparticular. We see MEMLLM as an important\nstep towards making LLMs more grounded and\nfactual through memory augmentation."}, {"group": "Natural Language Processing", "id": 20, "label": "SKIP: Skill-Localized Prompt Tuning for Inference Speed Boost-Up", "shape": "dot", "size": 10, "title": "Authors: Junseok Kim, Jiwon Moon, Yunah Jang, Kyomin Jung\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Prompt-tuning methods have shown compa-\nrable performance as parameter-efficient fine-\ntuning (PEFT) methods in various natural lan-\nguage understanding tasks. However, existing\nprompt tuning methods still utilize the entire\nmodel architecture; thus, they fail to accel-\nerate inference speed in the application. In\nthis paper, we propose a novel approach called\nSKIll-localized Prompt tuning (SKIP), which\nis extremely efficient in inference time. Our\nmethod significantly enhances inference effi-\nciency by investigating and utilizing a skill-\nlocalized subnetwork in a language model. Sur-\nprisingly, our method improves the inference\nspeed up to 160% times while pruning 52% of\nthe parameters. Furthermore, we demonstrate\nthat our method is applicable across various\ntransformer-based architectures, thereby con-\nfirming its practicality and scalability."}, {"group": "Natural Language Processing", "id": 21, "label": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment", "shape": "dot", "size": 10, "title": "Authors: Bin Wang, Zhengyuan Liu, Nancy F. Chen\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Multilingual proficiency presents a significant\nchallenge for large language models (LLMs).\nEnglish-centric models are usually suboptimal\nin other languages, particularly those that are\nlinguistically distant from English. This per-\nformance discrepancy mainly stems from the\nimbalanced distribution of training data across\nlanguages during pre-training and instruction\ntuning stages. To address this problem, we pro-\npose a novel approach called CrossIn, which\nutilizes a mixed composition of cross-lingual\ninstruction tuning data.\nOur method lever-\nages the compressed representation shared by\nvarious languages to efficiently enhance the\nmodel\u2019s task-solving capabilities and multilin-\ngual proficiency within a single process. In\naddition, we introduce a multi-task and multi-\nfaceted benchmark to evaluate the effective-\nness of CrossIn. Experimental results demon-\nstrate that our method substantially improves\nperformance across tasks and languages, and\nwe provide extensive insights into the impact of\ncross-lingual data volume and the integration\nof translation data on enhancing multilingual\nconsistency and accuracy.1"}, {"group": "Natural Language Processing", "id": 23, "label": "Aligning Language Models to Explicitly Handle Ambiguity", "shape": "dot", "size": 10, "title": "Authors: Hyuhng Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sang-goo Lee, Taeuk Kim, juny\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In spoken languages, utterances are often\nshaped to be incomplete or vague for efficiency.\nThis can lead to varying interpretations of the\nsame input, based on different assumptions\nabout the context.\nTo ensure reliable user-\nmodel interactions in such scenarios, it is cru-\ncial for models to adeptly handle the inherent\nambiguity in user queries. However, conversa-\ntional agents built upon even the most recent\nlarge language models (LLMs) face challenges\nin processing ambiguous inputs, primarily due\nto the following two hurdles: (1) LLMs are not\ndirectly trained to handle inputs that are too am-\nbiguous to be properly managed; (2) the degree\nof ambiguity in an input can vary according to\nthe intrinsic knowledge of the LLMs, which\nis difficult to investigate. To address these is-\nsues, this paper proposes a method to align\nLLMs to explicitly handle ambiguous inputs.\nSpecifically, we introduce a proxy task that\nguides LLMs to utilize their intrinsic knowl-\nedge to self-disambiguate a given input. We\nquantify the information gain from the disam-\nbiguation procedure as a measure of the extent\nto which the models perceive their inputs as\nambiguous. This measure serves as a cue for\nselecting samples deemed ambiguous from the\nmodels\u2019 perspectives, which are then utilized\nfor alignment. Experimental results from sev-\neral question-answering datasets demonstrate\nthat the LLMs fine-tuned with our approach are\ncapable of handling ambiguous inputs while\nstill performing competitively on clear ques-\ntions within the task."}, {"group": "Natural Language Processing", "id": 24, "label": "Token-level Direct Preference Optimization", "shape": "dot", "size": 10, "title": "Authors: Yongcheng Zeng, Guoqing Liu, Ning Yang, Haifeng Zhang, Jun Wang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Fine-tuning pre-trained Large Language Models\n(LLMs) is essential to align them with human\nvalues and intentions.\nThis process often uti-\nlizes methods like pairwise comparisons and KL\ndivergence against a reference LLM, focusing\non the evaluation of full answers generated by\nthe models. However, the generation of these\nresponses occurs in a token level, following a\nsequential, auto-regressive fashion. In this pa-\nper, we introduce Token-level Direct Preference\nOptimization (TDPO), a novel approach to align\nLLMs with human preferences by optimizing pol-\nicy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency,\nTDPO incorporates forward KL divergence con-\nstraints for each token, improving alignment and\ndiversity. Utilizing the Bradley-Terry model for\na token-based reward system, TDPO enhances\nthe regulation of KL divergence, while preserv-\ning simplicity without the need for explicit re-\nward modeling. Experimental results across vari-\nous text tasks demonstrate TDPO\u2019s superior per-\nformance in balancing alignment with genera-\ntion diversity. Notably, fine-tuning with TDPO\nstrikes a better balance than DPO in the controlled\nsentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of\ngenerated responses compared to both DPO and\nPPO-based RLHF methods. Our code is open-\nsourced at https://github.com/Vance0124/Token-\nlevel-Direct-Preference-Optimization.\n1."}, {"group": "Natural Language Processing", "id": 29, "label": "Constituents Correspond to Word Sequence Patterns among Sentences with Equivalent Predicate-Argument Structures: Unsupervised Constituency Parsing by Span Matching", "shape": "dot", "size": 10, "title": "Authors: Junjie Chen, Xiangheng He, Danushka Bollegala, Yusuke Miyao\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Unsupervised constituency parsing is about\nidentifying word sequences that form a syn-\ntactic unit (i.e., constituents) in a target sen-\ntence. Linguists identify the constituent by\nevaluating a set of Predicate-Argument Struc-\nture (PAS) equivalent sentences where we find\nthe constituent corresponds to frequent word\nsequences. However, such information is un-\navailable to previous parsing methods which\nidentify the constituent by observing sentences\nwith diverse PAS. In this study, we empirically\nverify that constituents correspond to word\nsequence patterns in the PAS-equivalent sen-\ntence set.\nWe propose a frequency-based\nmethod span-overlap, applying the word se-\nquence pattern to computational unsupervised\nparsing for the first time.\nParsing experi-\nments show that the span-overlap parser out-\nperforms state-of-the-art parsers in eight out\nof ten languages. Further discrimination analy-\nsis confirms that the span-overlap method can\nnon-trivially separate constituents from non-\nconstituents. This result highlights the util-\nity of the word sequence pattern. Addition-\nally, we discover a multilingual phenomenon:\nparticipant-denoting constituents are more\nfrequent than event-denoting constituents.\nThe phenomenon indicates a behavioral differ-\nence between the two constituent types, laying\nthe foundation for future labeled unsupervised\nparsing."}, {"group": "Natural Language Processing", "id": 30, "label": "TIMIT Speaker Profiling: A Comparison of Multi-task learning and Single-task learning Approaches", "shape": "dot", "size": 10, "title": "Authors: Rong Wang, Kun Sun\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: This study employs deep learning techniques to ex-\nplore four speaker profiling tasks on the TIMIT\ndataset, namely gender classification, accent clas-\nsification, age estimation, and speaker identification,\nhighlighting the potential and challenges of multi-\ntask learning versus single-task models. The motiva-\ntion for this research is twofold: firstly, to empirically\nassess the advantages and drawbacks of multi-task\nlearning over single-task models in the context of\nspeaker profiling; secondly, to emphasize the undi-\nminished significance of skillful feature engineering\nfor speaker recognition tasks. The findings reveal\nchallenges in accent classification, and multi-task\nlearning is found advantageous for tasks of similar\ncomplexity. Non-sequential features are favored for\nspeaker recognition, but sequential ones can serve as\nstarting points for complex models. The study under-\nscores the necessity of meticulous experimentation\nand parameter tuning for deep learning models."}, {"group": "Natural Language Processing", "id": 35, "label": "EuSQuAD: Automatically Translated and Aligned SQuAD2.0 for Basque", "shape": "dot", "size": 10, "title": "Authors: Euskera, Aitor Garc\u00eda-Pablos,\u2217Naiara, Jaione Bengoetxea\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: : The widespread availability of Question Answering (QA) datasets in\nEnglish has greatly facilitated the advancement of the Natural Language Processing\n(NLP) field. However, the scarcity of such resources for minority languages, such\nas Basque, poses a substantial challenge for these communities. In this context, the\ntranslation and alignment of existing QA datasets plays a crucial role in narrowing\nthis technological gap. This work presents EuSQuAD, the first initiative dedicated\nto automatically translating and aligning SQuAD2.0 into Basque, resulting in more\nthan 142k QA examples.\nWe demonstrate EuSQuAD\u2019s value through extensive\nqualitative analysis and QA experiments supported with EuSQuAD as training data.\nThese experiments are evaluated with a new human-annotated dataset.\nKeywords: question answering, reading comprehension, Basque, SQuAD\nResumen: La amplia disponibilidad de conjuntos de datos de preguntas y respues-\ntas en ingl\u00e9s ha facilitado en gran medida el avance del campo de Procesamiento\nde Lenguaje Natural (PLN). Sin embargo, la escasez de tales recursos para idiomas\nminoritarios, como el euskera, plantea un desaf\u00edo sustancial para estas comunida-\ndes. En este contexto, la traducci\u00f3n y alineaci\u00f3n de conjuntos de datos desempe\u00f1a\nun papel crucial en la reducci\u00f3n de esta brecha tecnol\u00f3gica. Este trabajo presen-\nta EuSQuAD, la primera iniciativa dedicada a traducir y alinear autom\u00e1ticamente\nSQuAD2.0 al euskera. Demostramos el valor de EuSQuAD a trav\u00e9s de un extenso\nan\u00e1lisis cualitativo y experimentos de QA, para los cuales se ha creado adem\u00e1s un\nnuevo dataset anotado por humanos.\nPalabras clave: pregunta-respuesta, comprensi\u00f3n lectora, euskera, SQuAD"}, {"group": "Natural Language Processing", "id": 37, "label": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing", "shape": "dot", "size": 10, "title": "Authors: Bairu Hou\u2217, Zhen Zhang\u2217, Guanhua Zhang\u2217, Wenqi Fan, Qing Li, Gaowen Liu, Sijia Liu, Shiyu Chang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Although large language models (LLMs) have\nachieved significant success, their vulnerabil-\nity to adversarial perturbations, including re-\ncent jailbreak attacks, has raised considerable\nconcerns.\nHowever, the increasing size of\nthese models and their limited access make\nimproving their robustness a challenging task.\nAmong various defense strategies, random-\nized smoothing has shown great potential for\nLLMs, as it does not require full access to the\nmodel\u2019s parameters or fine-tuning via adver-\nsarial training. However, randomized smooth-\ning involves adding noise to the input before\nmodel prediction, and the final model\u2019s ro-\nbustness largely depends on the model\u2019s per-\nformance on these noise corrupted data. Its\neffectiveness is often limited by the model\u2019s\nsub-optimal performance on noisy data. To\naddress this issue, we propose to leverage the\nmultitasking nature of LLMs to first denoise\nthe noisy inputs and then to make predictions\nbased on these denoised versions.\nWe call\nthis procedure self-denoised smoothing. Un-\nlike previous denoised smoothing techniques\nin computer vision, which require training a\nseparate model to enhance the robustness of\nLLMs, our method offers significantly better\nefficiency and flexibility. Our experimental\nresults indicate that our method surpasses ex-\nisting methods in both empirical and certified\nrobustness in defending against adversarial at-\ntacks for both downstream tasks and human\nalignments (i.e., jailbreak attacks). Our code\nis publicly available at https://github.\ncom/UCSB-NLP-Chang/SelfDenoise."}, {"group": "Natural Language Processing", "id": 38, "label": "Resilience through Scene Context in Visual Referring Expression Generation", "shape": "dot", "size": 10, "title": "Authors: Sa Zarrie\u00df\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Scene context is well known to facilitate hu-\nmans\u2019 perception of visible objects. In this pa-\nper, we investigate the role of context in Refer-\nring Expression Generation (REG) for objects\nin images, where existing research has often fo-\ncused on distractor contexts that exert pressure\non the generator. We take a new perspective\non scene context in REG and hypothesize that\ncontextual information can be conceived of as a\nresource that makes REG models more resilient\nand facilitates the generation of object descrip-\ntions, and object types in particular. We train\nand test Transformer-based REG models with\ntarget representations that have been artificially\nobscured with noise to varying degrees. We\nevaluate how properties of the models\u2019 visual\ncontext affect their processing and performance.\nOur results show that even simple scene con-\ntexts make models surprisingly resilient to per-\nturbations, to the extent that they can identify\nreferent types even when visual information\nabout the target is completely missing."}, {"group": "Natural Language Processing", "id": 39, "label": "Augmenting emotion features in irony detection with", "shape": "dot", "size": 10, "title": "Authors: Yucheng L, Yuhan Xia\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: . This study introduces a novel method for irony detection, applying \nLarge Language Models (LLMs) with prompt-based learning to facilitate \nemotion-centric text augmentation. Traditional irony detection techniques \ntypically fall short due to their reliance on static linguistic features and predefined \nknowledge bases, often overlooking the nuanced emotional dimensions integral \nto irony. In contrast, our methodology augments the detection process by \nintegrating subtle emotional cues, augmented through LLMs, into three \nbenchmark pre-trained NLP models\u2014BERT, T5, and GPT-2\u2014which are widely \nrecognized as foundational in irony detection. We assessed our method using the \nSemEval-2018 Task 3 dataset and observed substantial enhancements in irony \ndetection capabilities. \nKeywords: Irony Detection, Text Augmentation, Large Language Modeling"}, {"group": "Natural Language Processing", "id": 40, "label": "Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair", "shape": "dot", "size": 10, "title": "Authors: Mana Makae\u2217, Hidetaka Kamigaito\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In Simultaneous Machine Translation (SiMT)\nsystems, training with a simultaneous interpre-\ntation (SI) corpus is an effective method for\nachieving high-quality yet low-latency systems.\nHowever, it is very challenging to curate such\na corpus due to limitations in the abilities of\nannotators, and hence, existing SI corpora are\nlimited. Therefore, we propose a method to\nconvert existing speech translation corpora into\ninterpretation-style data, maintaining the origi-\nnal word order and preserving the entire source\ncontent using Large Language Models (LLM-\nSI-Corpus). We demonstrate that fine-tuning\nSiMT models in text-to-text and speech-to-text\nsettings with the LLM-SI-Corpus reduces laten-\ncies while maintaining the same level of qual-\nity as the models trained with offline datasets.\nThe LLM-SI-Corpus is available at https://\ngithub.com/yusuke1997/LLM-SI-Corpus."}, {"group": "Natural Language Processing", "id": 41, "label": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "shape": "dot", "size": 10, "title": "Authors: Zhaofeng Wu\u00e3, Ananth Balashankar\u00e5, Jacob Eisenstein\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Aligning language models (LMs) based on\nhuman-annotated preference data is a crucial\nstep in obtaining practical and performant LM-\nbased systems. However, multilingual human\npreference data are difficult to obtain at scale,\nmaking it challenging to extend this frame-\nwork to diverse languages. In this work, we\nevaluate a simple approach for zero-shot cross-\nlingual alignment, where a reward model is\ntrained on preference data in one source lan-\nguage and directly applied to other target lan-\nguages. On summarization and open-ended\ndialog generation, we show that this method is\nconsistently successful under comprehensive\nevaluation settings, including human evalua-\ntion: cross-lingually aligned models are pre-\nferred by humans over unaligned models on up\nto \u003e70% of evaluation instances. We moreover\nfind that a different-language reward model\nsometimes yields better aligned models than\na same-language reward model. We also iden-\ntify best practices when there is no language-\nspecific data for even supervised finetuning,\nanother component in alignment."}, {"group": "History of Science", "id": 86, "label": "SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy", "shape": "dot", "size": 10, "title": "Authors: Alison Bartsch, Arvind Car, Charlotte Avra, Amir Barati Farimani\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Manipulating\ndeformable\nobjects\nremains\na\nchallenge within robotics due to the difficulties of state\nestimation, long-horizon planning, and predicting how the\nobject will deform given an interaction. These challenges\nare\nthe\nmost\npronounced\nwith\n3D\ndeformable\nobjects.\nWe propose SculptDiff, a goal-conditioned diffusion-based\nimitation learning framework that works with point cloud\nstate observations to directly learn clay sculpting policies for a\nvariety of target shapes. To the best of our knowledge this is the\nfirst real-world method that successfully learns manipulation\npolicies for 3D deformable objects. For sculpting videos and\naccess to our dataset and hardware CAD models, see the project\nwebsite:\nhttps://sites.google.com/andrew.cmu.edu/imitation-\nsculpting/home\nI."}, {"group": "History of Science", "id": 87, "label": "H-MaP: An Iterative and Hybrid Sequential Manipulation Planner", "shape": "dot", "size": 10, "title": "Authors: Cankut Bora Tuncer, Busenaz Kerimgil, Ozgur S. Oguz\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 This study introduces the Hybrid Sequential Ma-\nnipulation Planner (H-MaP), a novel approach that iteratively\ndoes motion planning using contact points and waypoints for\ncomplex sequential manipulation tasks in robotics. Combining\noptimization-based methods for generalizability and sampling-\nbased methods for robustness, H-MaP enhances manipulation\nplanning through active contact mode switches and enables\ninteractions with auxiliary objects and tools. This framework,\nvalidated by a series of diverse physical manipulation tasks and\nreal-robot experiments, offers a scalable and adaptable solution\nfor complex real-world applications in robotic manipulation.\nhttps://sites.google.com/view/h-map/\nI."}, {"group": "History of Science", "id": 90, "label": "Lifelong LERF: Local 3D Semantic Inventory Monitoring Using FogROS2", "shape": "dot", "size": 10, "title": "Authors: Adam Rashid, Chung Min Kim, Justin Kerr, Letian Fu, Kush Hari, Ayah Ahmad, Kaiyuan Chen, Huang Huang, Marcus Gualtieri, Michael Wang, Christian Juette, Nan Tian, Liu Ren, Ken Goldberg\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Inventory monitoring in homes, factories, and\nretail stores relies on maintaining data despite objects being\nswapped, added, removed, or moved. We introduce Lifelong\nLERF, a method that allows a mobile robot with minimal\ncompute to jointly optimize a dense language and geometric\nrepresentation of its surroundings. Lifelong LERF maintains\nthis representation over time by detecting semantic changes\nand selectively updating these regions of the environment,\navoiding the need to exhaustively remap. Human users can\nquery inventory by providing natural language queries and\nreceiving a 3D heatmap of potential object locations. To\nmanage the computational load, we use Fog-ROS2, a cloud\nrobotics platform, to offload resource-intensive tasks. Lifelong\nLERF obtains poses from a monocular RGBD SLAM backend,\nand uses these poses to progressively optimize a Language\nEmbedded Radiance Field (LERF) for semantic monitoring.\nExperiments with 3-5 objects arranged on a tabletop and a\nTurtlebot with a RealSense camera suggest that Lifelong LERF\ncan persistently adapt to changes in objects with up to 91%\naccuracy.\nI."}, {"group": "Natural Language Processing", "id": 7, "label": "Evaluating Span Extraction in Generative Paradigm: A Reflection on Aspect-Based Sentiment Analysis", "shape": "dot", "size": 10, "title": "Authors: Soyoung Yang, W Ik Cho\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In the era of rapid evolution of generative lan-\nguage models within the realm of natural lan-\nguage processing, there is an imperative call\nto revisit and reformulate evaluation method-\nologies, especially in the domain of aspect-\nbased sentiment analysis (ABSA). This paper\naddresses the emerging challenges introduced\nby the generative paradigm, which has moder-\nately blurred traditional boundaries between\nunderstanding and generation tasks.\nBuild-\ning upon prevailing practices in the field, we\nanalyze the advantages and shortcomings as-\nsociated with the prevalent ABSA evaluation\nparadigms.\nThrough an in-depth examina-\ntion, supplemented by illustrative examples, we\nhighlight the intricacies involved in aligning\ngenerative outputs with other evaluative met-\nrics, specifically those derived from other tasks,\nincluding question answering. While we steer\nclear of advocating for a singular and defini-\ntive metric, our contribution lies in paving the\npath for a comprehensive guideline tailored for\nABSA evaluations in this generative paradigm.\nIn this position paper, we aim to provide prac-\ntitioners with profound reflections, offering in-\nsights and directions that can aid in navigating\nthis evolving landscape, ensuring evaluations\nthat are both accurate and reflective of genera-\ntive capabilities."}, {"group": "Natural Language Processing", "id": 12, "label": "How often are errors in natural language reasoning due to paraphrastic variability?", "shape": "dot", "size": 10, "title": "Authors: Neha Srikanth, Mare Carpuat, Rachel Rudger\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Large language models have been shown\nto behave inconsistently in response to\nmeaning-preserving paraphrastic inputs. At\nthe same time, researchers evaluate the\nknowledge and reasoning abilities of these\nmodels with test evaluations that do not dis-\naggregate the effect of paraphrastic variabil-\nity on performance. We propose a metric,\nPC, for evaluating the paraphrastic consis-\ntency of natural language reasoning models\nbased on the probability of a model achiev-\ning the same correctness on two paraphrases\nof the same problem. We mathematically\nconnect this metric to the proportion of a\nmodel\u2019s variance in correctness attributable\nto paraphrasing. To estimate PC, we col-\nlect PARANLU, a dataset of 7,782 human-\nwritten and validated paraphrased reason-\ning problems constructed on top of exist-\ning benchmark datasets for defeasible and\nabductive natural language inference. Us-\ning PARANLU, we measure the paraphras-\ntic consistency of several model classes\nand show that consistency dramatically in-\ncreases with pretraining but not fine-tuning.\nAll models tested exhibited room for im-\nprovement in paraphrastic consistency."}, {"group": "Natural Language Processing", "id": 13, "label": "Investigating Gender Bias in Turkish Language Models", "shape": "dot", "size": 10, "title": "Authors: Orhun Caglidil, Malte Ostendorff\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Language models are trained mostly on Web\ndata, which often contains social stereotypes\nand biases that the models can inherit. This has\npotentially negative consequences, as models\ncan amplify these biases in downstream tasks\nor applications. However, prior research has\nprimarily focused on the English language, es-\npecially in the context of gender bias. In partic-\nular, grammatically gender-neutral languages\nsuch as Turkish are underexplored despite rep-\nresenting different linguistic properties to lan-\nguage models with possibly different effects on\nbiases. In this paper, we fill this research gap\nand investigate the significance of gender bias\nin Turkish language models. We build upon\nexisting bias evaluation frameworks and extend\nthem to the Turkish language by translating\nexisting English tests and creating new ones\ndesigned to measure gender bias in the con-\ntext of T\u00fcrkiye. Specifically, we also evaluate\nTurkish language models for their embedded\nethnic bias toward Kurdish people. Based on\nthe experimental results, we attribute possible\nbiases to different model characteristics such as\nthe model size, their multilingualism, and the\ntraining corpora. We make the Turkish gender\nbias dataset publicly available."}, {"group": "Natural Language Processing", "id": 17, "label": "Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point Generation and Introducing an Automatic Coverage Evaluation Metric", "shape": "dot", "size": 10, "title": "Authors: Chenyg Hug\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The proliferation of social media platforms has\ngiven rise to the amount of online debates and\narguments. Consequently, the need for auto-\nmatic summarization methods for such debates\nis imperative, however this area of summariza-\ntion is rather understudied.\nThe Key Point\nAnalysis (KPA) task formulates argument sum-\nmarization as representing the summary of a\nlarge collection of arguments in the form of\nconcise sentences in bullet-style format, called\nkey points. A sub-task of KPA, called Key\nPoint Generation (KPG), focuses on generating\nthese key points given the arguments. This pa-\nper introduces a novel extractive approach for\nkey point generation, that outperforms previ-\nous state-of-the-art methods for the task. Our\nmethod utilizes an extractive clustering based\napproach that offers concise, high quality gen-\nerated key points with higher coverage of refer-\nence summaries, and less redundant outputs. In\naddition, we show that the existing evaluation\nmetrics for summarization such as ROUGE are\nincapable of differentiating between generated\nkey points of different qualities. To this end, we\npropose a new evaluation metric for assessing\nthe generated key points by their coverage. Our\ncode can be accessed online.1"}, {"group": "Natural Language Processing", "id": 18, "label": "AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence", "shape": "dot", "size": 10, "title": "Authors: Hwaran Lee,, minbeomkim\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: As the integration of large language models\ninto daily life is on the rise, there is a clear\ngap in benchmarks for advising on subjective\nand personal dilemmas. To address this, we\nintroduce AdvisorQA, the first benchmark de-\nveloped to assess LLMs\u2019 capability in offering\nadvice for deeply personalized concerns, uti-\nlizing the LifeProTips subreddit forum. This\nforum features a dynamic interaction where\nusers post advice-seeking questions, receiving\nan average of 8.9 advice per query, with 164.2\nupvotes from hundreds of users, embodying a\ncollective intelligence framework. Therefore,\nwe\u2019ve completed a benchmark encompassing\ndaily life questions, diverse corresponding re-\nsponses, and majority vote ranking to train our\nhelpfulness metric. Baseline experiments val-\nidate the efficacy of AdvisorQA through our\nhelpfulness metric, GPT-4, and human evalua-\ntion, analyzing phenomena beyond the trade-off\nbetween helpfulness and harmlessness. Advi-\nsorQA marks a significant leap in enhancing\nQA systems for providing personalized, empa-\nthetic advice, showcasing LLMs\u2019 improved un-\nderstanding of human subjectivity."}, {"group": "Natural Language Processing", "id": 27, "label": "Enhance Robustness of Language Models Against Variation Attack through Graph Integration", "shape": "dot", "size": 10, "title": "Authors: Lizhi Qing, Yangyang Kang, Jiawei Liu, Hongsong Li, Changlong Sun, Xiaozhong Liu, Wei Lu, yangyang.kangyy\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The widespread use of pre-trained language models (PLMs) in natural language processing (NLP) has greatly\nimproved performance outcomes. However, these models\u2019 vulnerability to adversarial attacks (e.g., camouflaged\nhints from drug dealers), particularly in the Chinese language with its rich character diversity/variation and complex\nstructures, hatches vital apprehension.\nIn this study, we propose a novel method, CHinese vAriatioN Graph\nEnhancement (CHANGE), to increase the robustness of PLMs against character variation attacks in Chinese\ncontent. CHANGE presents a novel approach for incorporating a Chinese character variation graph into the PLMs.\nThrough designing different supplementary tasks utilizing the graph structure, CHANGE essentially enhances PLMs\u2019\ninterpretation of adversarially manipulated text. Experiments conducted in a multitude of NLP tasks show that\nCHANGE outperforms current language models in combating against adversarial attacks and serves as a valuable\ncontribution to robust language model research. These findings contribute to the groundwork on robust language\nmodels and highlight the substantial potential of graph-guided pre-training strategies for real-world applications.\nKeywords: PLMs, Chinese adversarial attacks, variation graph\n1."}, {"group": "Network Science", "id": 33, "label": "Stance Detection on Social Media with Fine-Tuned Large Language Models", "shape": "dot", "size": 10, "title": "Authors: Ilker G\u00fcl, EPFL, R\u00e9mi Lebret, Karl Aberer\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Stance detection, a key task in natural language\nprocessing, determines an author\u2019s viewpoint\nbased on textual analysis. This study evaluates\nthe evolution of stance detection methods,\ntransitioning from early machine learning\napproaches to the groundbreaking BERT model,\nand eventually to modern Large Language Mod-\nels (LLMs) such as ChatGPT, LLaMa-2, and\nMistral-7B. While ChatGPT\u2019s closed-source\nnature and associated costs present challenges,\nthe open-source models like LLaMa-2 and\nMistral-7B offers an encouraging alternative.\nInitially, our research focused on fine-tuning\nChatGPT, LLaMa-2, and Mistral-7B using sev-\neral publicly available datasets. Subsequently,\nto provide a comprehensive comparison, we\nassess the performance of these models in\nzero-shot and few-shot learning scenarios. The\nresults underscore the exceptional ability of\nLLMs in accurately detecting stance, with all\ntested models surpassing existing benchmarks.\nNotably, LLaMa-2 and Mistral-7B demonstrate\nremarkable efficiency and potential for stance\ndetection, despite their smaller sizes compared\nto ChatGPT. This study emphasizes the\npotential of LLMs in stance detection and calls\nfor more extensive research in this field."}, {"group": "Natural Language Processing", "id": 34, "label": "Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?", "shape": "dot", "size": 10, "title": "Authors: Laura Majer, Jan \u0160najder, laura.majer\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The increasing threat of disinformation calls\nfor automating parts of the fact-checking\npipeline.\nIdentifying text segments requir-\ning fact-checking is known as claim detec-\ntion (CD) and claim check-worthiness detection\n(CW), the latter incorporating complex domain-\nspecific criteria of worthiness and often framed\nas a ranking task. Zero- and few-shot LLM\nprompting is an attractive option for both tasks,\nas it bypasses the need for labeled datasets and\nallows verbalized claim and worthiness criteria\nto be directly used for prompting. We evaluate\nthe LLMs\u2019 predictive and calibration accuracy\non five CD/CW datasets from diverse domains,\neach utilizing a different worthiness criterion.\nWe investigate two key aspects: (1) how best\nto distill factuality and worthiness criteria into\na prompt and (2) what amount of context to\nprovide for each claim. To this end, we experi-\nment with varying the level of prompt verbosity\nand the amount of contextual information pro-\nvided to the model. Our results show that op-\ntimal prompt verbosity is domain-dependent,\nadding context does not improve performance,\nand confidence scores can be directly used to\nproduce reliable check-worthiness rankings."}, {"group": "Natural Language Processing", "id": 36, "label": "Length Generalization of Causal Transformers without Position Encoding", "shape": "dot", "size": 10, "title": "Authors: Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang,, Xiaoling Wang\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Generalizing to longer sentences is important\nfor recent Transformer-based language mod-\nels. Besides algorithms manipulating explicit\nposition features, the success of Transform-\ners without position encodings (NoPE) pro-\nvides a new way to overcome the challenge.\nIn this paper, we study the length generaliza-\ntion property of NoPE. We find that although\nNoPE can extend to longer sequences than\nthe commonly used explicit position encod-\nings, it still has a limited context length. We\nidentify a connection between the failure of\nNoPE\u2019s generalization and the distraction of at-\ntention distributions. We propose a parameter-\nefficient tuning for searching attention heads\u2019\nbest temperature hyper-parameters, which sub-\nstantially expands NoPE\u2019s context size. Experi-\nments on long sequence language modeling, the\nsynthetic passkey retrieval task and real-world\nlong context tasks show that NoPE can achieve\ncompetitive performances with state-of-the-art\nlength generalization algorithms. The source\ncode is publicly accessible1."}, {"group": "Network Science", "id": 92, "label": "Semi-supervised Stance Detection of Tweets Via Distant Network Supervision", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Detecting and labeling stance in social media text is strongly moti-\nvated by hate speech detection, poll prediction, engagement fore-\ncasting, and concerted propaganda detection. Today\u2019s best neural\nstance detectors need large volumes of training data, which is dif-\nficult to curate given the fast-changing landscape of social media\ntext and issues on which users opine. Homophily properties over\nthe social network provide strong signal of coarse-grained user-\nlevel stance. But semi-supervised approaches for tweet-level stance\ndetection fail to properly leverage homophily. In light of this, We\npresent SANDS, a new semi-supervised stance detector. SANDS starts\nfrom very few labeled tweets. It builds multiple deep feature views\nof tweets. It also uses a distant supervision signal from the social net-\nwork to provide a surrogate loss signal to the component learners.\nWe prepare two new tweet datasets comprising over 236,000 politi-\ncally tinted tweets from two demographics (US and India) posted\nby over 87,000 users, their follower-followee graph, and over 8,000\ntweets annotated by linguists. SANDS achieves a macro-F1 score of\n0.55 (0.49) on US (India)-based datasets, outperforming 17 baselines\n(including variants of SANDS) substantially, particularly for minor-\nity stance labels and noisy text. Numerous ablation experiments on\nSANDS disentangle the dynamics of textual and network-propagated\nstance signals.\nACM Reference Format:\n1Subhabrata Dutta, 2Samiya Caur, 3Soumen Chakrabarti, 2Tanmoy Chakraborty.\n2022. Semi-supervised Stance Detection of Tweets Via Distant Network\nSupervision. In Proceedings of the Fifteenth ACM International Conference on\nWeb Search and Data Mining (WSDM \u201922), February 21\u201325, 2022, Tempe, AZ,\nUSA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3488560.\n3498511"}, {"group": "Network Science", "id": 106, "label": "A Fine-Grained Analysis of Public Opinion toward Chinese Technology Companies on Reddit", "shape": "dot", "size": 10, "title": "Authors: Enting Zhou, Yurg Liu, Hanjia Lyu, Jiebo Luo\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In the face of the growing global in\ufb02uence and prevalence\nof Chinese technology companies, governments worldwide\nhave expressed concern and mistrust toward these compa-\nnies. There is a scarcity of research that speci\ufb01cally exam-\nines the widespread public response to this phenomenon on a\nlarge scale. This study aims to \ufb01ll in the gap in understanding\npublic opinion toward Chinese technology companies using\nReddit data, a popular news-oriented social media platform.\nWe employ the state-of-the-art transformer model to build\na reliable sentiment classi\ufb01er. We then use LDA to extract\nthe topics associated with positive and negative comments.\nWe also conduct content analysis by studying the changes in\nthe semantic meaning of the companies\u2019 names over time.\nOur main \ufb01ndings include the following: 1) Notable differ-\nence exists in the proportions of positive comments (8.42%)\nand negative comments (14.12%); 2) Positive comments are\nmostly associated with the companies\u2019 consumer products,\nsuch as smartphones, laptops, and wearable electronics. Neg-\native comments have a more diverse topic distribution (no-\ntable topics include criticism toward the platform, dissatisfac-\ntion with the companies\u2019 smartphone products, companies\u2019\nties to the Chinese government, data security concerns, 5G\nconstruction, and general political discussions); and 3) Char-\nacterization of each technology company is usually centered\naround a particular predominant theme related to the com-\npany, while real-world political events may trigger drastic\nchanges in users\u2019 characterization."}, {"group": "Network Science", "id": 118, "label": "Targeted aspect-based emotion analysis to detect opportunities and precaution in financial Twitter messages", "shape": "dot", "size": 10, "title": "Authors: Silvia Garc\u00b4\u0131a-M\u00b4endeza,\u2217, Francisco de Arriba-P\u00b4ereza, Ana Barros-Vilaa, Francisco J. Gonz\u00b4alez-Casta\u02dcnoaaInformation\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Microblogging platforms, of which Twitter is a representative example, are valuable information sources for\nmarket screening and financial models. In them, users voluntarily provide relevant information, including\neducated knowledge on investments, reacting to the state of the stock markets in real-time and, often,\ninfluencing this state. We are interested in the user forecasts in financial, social media messages expressing\nopportunities and precautions about assets. We propose a novel Targeted Aspect-Based Emotion Analysis\n(tabea) system that can individually discern the financial emotions (positive and negative forecasts) on the\ndifferent stock market assets in the same tweet (instead of making an overall guess about that whole tweet).\nIt is based on Natural Language Processing (nlp) techniques and Machine Learning streaming algorithms.\nThe system comprises a constituency parsing module for parsing the tweets and splitting them into simpler\ndeclarative clauses; an offline data processing module to engineer textual, numerical and categorical features\nand analyse and select them based on their relevance; and a stream classification module to continuously\nprocess tweets on-the-fly. Experimental results on a labelled data set endorse our solution. It achieves over\n90% precision for the target emotions, financial opportunity, and precaution on Twitter. To the best of\nour knowledge, no prior work in the literature has addressed this problem despite its practical interest in\ndecision-making, and we are not aware of any previous nlp nor online Machine Learning approaches to\ntabea.\nKeywords:\nAspect-Based Emotion Analysis, Machine Learning, Natural Language Processing, Opinion\nMining, Personal Finance Management, Portfolio Optimisation\n1."}, {"group": "Network Science", "id": 130, "label": "Stance Detection on Social Media with Fine-Tuned Large Language Models", "shape": "dot", "size": 10, "title": "Authors: Ilker G\u00fcl, EPFL, R\u00e9mi Lebret, Karl Aberer\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Stance detection, a key task in natural language\nprocessing, determines an author\u2019s viewpoint\nbased on textual analysis. This study evaluates\nthe evolution of stance detection methods,\ntransitioning from early machine learning\napproaches to the groundbreaking BERT model,\nand eventually to modern Large Language Mod-\nels (LLMs) such as ChatGPT, LLaMa-2, and\nMistral-7B. While ChatGPT\u2019s closed-source\nnature and associated costs present challenges,\nthe open-source models like LLaMa-2 and\nMistral-7B offers an encouraging alternative.\nInitially, our research focused on fine-tuning\nChatGPT, LLaMa-2, and Mistral-7B using sev-\neral publicly available datasets. Subsequently,\nto provide a comprehensive comparison, we\nassess the performance of these models in\nzero-shot and few-shot learning scenarios. The\nresults underscore the exceptional ability of\nLLMs in accurately detecting stance, with all\ntested models surpassing existing benchmarks.\nNotably, LLaMa-2 and Mistral-7B demonstrate\nremarkable efficiency and potential for stance\ndetection, despite their smaller sizes compared\nto ChatGPT. This study emphasizes the\npotential of LLMs in stance detection and calls\nfor more extensive research in this field."}, {"group": "Natural Language Processing", "id": 32, "label": "FECTEK: ENHANCING TERM WEIGHT IN LEXICON-BASED RETRIEVAL WITH FEATURE CONTEXT AND TERM-LEVEL KNOWLEDGE", "shape": "dot", "size": 10, "title": "Authors: ShenZhen, ShenZhen, Shen, ShenZhen, Ye, ShenZhen\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Lexicon-based retrieval has gained siginificant popularity in text retrieval due to its efficient and\nrobust performance. To further enhance performance of lexicon-based retrieval, researchers have\nbeen diligently incorporating state-of-the-art methodologies like Neural retrieval and text-level\ncontrastive learning approaches. Nonetheless, despite the promising outcomes, current lexicon-based\nretrieval methods have received limited attention in exploring the potential benefits of feature context\nrepresentations and term-level knowledge guidance. In this paper, we introduce an innovative method\nby introducing FEature Context and TErm-level Knowledge modules(FecTek). To effectively enrich\nthe feature context representations of term weight, the Feature Context Module (FCM) is introduced,\nwhich leverages the power of BERT\u2019s representation to determine dynamic weights for each element\nin the embedding. Additionally, we develop a term-level knowledge guidance module (TKGM) for\neffectively utilizing term-level knowledge to intelligently guide the modeling process of term weight.\nEvaluation of the proposed method on MS Marco benchmark demonstrates its superiority over the\nprevious state-of-the-art approaches."}, {"group": "Network Science", "id": 15, "label": "Mapping Violence: Developing an Extensive Framework to Build a Bangla Sectarian Expression Dataset from Social Media Interactions Sujan Sen Gupta* University of Dhaka", "shape": "dot", "size": 10, "title": "Authors: Nzi Tsnim, Suj  Gupt, Istik Hossin Shihb, Ftih Islm Juee, Arunim Thsin, Prim Ghum, Kij Ftem, Mrshi Hque, Wsem Frz, Prionti Nsir, Ashique KhudBukhsh, Frig Sdeque\u2020, AIAsif Sushmit\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Communal violence in online forums has\nbecome extremely prevalent in South Asia,\nwhere many communities of different cultures\ncoexist and share resources. These societies\nexhibit a phenomenon characterized by strong\nbonds within their own groups and animos-\nity towards others, leading to con\ufb02icts that\nfrequently escalate into violent confrontations.\nTo address this issue, we have developed the\n\ufb01rst comprehensive framework for the auto-\nmatic detection of communal violence mark-\ners in online Bangla content accompanying the\nlargest collection (13K raw sentences) of so-\ncial media interactions that fall under the de\ufb01-\nnition of four major violence class and their 16\ncoarse expressions. Our work\ufb02ow introduces a\n7-step expert annotation process incorporating\ninsights from social scientists, linguists, and\npsychologists.\nBy\npresenting\ndata\nstatistics\nand\nbench-\nmarking performance using this dataset1, we\nhave determined that, aside from the cat-\negory of Non-communal violence, Religio-\ncommunal violence is particularly pervasive\nin Bangla text. Moreover, we have substanti-\nated the effectiveness of \ufb01ne-tuning language\nmodels in identifying violent comments by\nconducting preliminary benchmarking on the\nstate-of-the-art Bangla deep learning model."}, {"group": "Natural Language Processing", "id": 102, "label": "A simple article template", "shape": "dot", "size": 10, "title": "Authors: Ran Ziv, Ilan Gronau, Michael Fire\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Entity Matching is an essential part of all real-world systems that take in structured and un-\nstructured data coming from di\ufb00erent sources. Typically no common key is available for connecting\nrecords. Massive data cleaning and integration processes require completion before any data ana-\nlytics, or further processing can be performed. Although record linkage is frequently regarded as a\nsomewhat tedious but necessary step, it reveals valuable insights, supports data visualization, and\nguides further analytic approaches to the data. Here, we focus on organization entity matching. We\nintroduce CompanyName2Vec, a novel algorithm to solve company entity matching (CEM) using a\nneural network model to learn company name semantics from a job ad corpus, without relying on\nany information on the matched company besides its name. Based on a real-world data, we show\nthat CompanyName2Vec outperforms other evaluated methods and solves the CEM challenge with\nan average success rate of 89.3%.\nKeywords: Entity Matching, Organization Name Matching, LSTM, CompanyName2Vec"}, {"group": "Network Science", "id": 113, "label": "CHAPTER 4", "shape": "dot", "size": 10, "title": "Authors: Alexander Ruch, Ari Decter-Frain, Raghav Batra\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Polarization in America has reached a high point as markets are also becoming polarized. \nExisting research, however, focuses on specific market segments and products and has not \nevaluated this trend\u2019s full breadth. If such fault lines do spread into other segments that are \nnot explicitly political, it would indicate the presence of lifestyle politics \u2013 when ideas and \nbehaviors not inherently political become politically aligned through their connections with \nexplicitly political things. We study the pervasiveness of polarization and lifestyle politics \nover different product segments in a diverse market and test the extent to which consumer- \nand platform-level network effects and morality may explain lifestyle politics. Specifically, \nusing graph and language data from Amazon (82.5M reviews of 9.5M products and product \nand category metadata from 1996\u20132014), we sample 234.6 million relations among 21.8 \nmillion market entities to find product categories that are most politically relevant, aligned, \nand polarized. We then extract moral values present in reviews\u2019 text and use these data and \nother reviewer-, product-, and category-level data to test whether individual- and platform-\nlevel network factors explain lifestyle politics better than products\u2019 implicit morality. We \nfind pervasive lifestyle politics. Cultural products are 4 times more polarized than any other \nsegment, products\u2019 political attributes have up to 3.7 times larger associations with lifestyle \npolitics than author-level covariates, and morality has statistically significant but relatively \nsmall correlations with lifestyle politics. Examining lifestyle politics in these contexts helps \nus better understand the extent and root of partisan differences, why Americans may be so \npolarized, and how this polarization affects market systems. \n \nKeywords: markets, networks, polarization, lifestyle politics, morality."}, {"group": "Natural Language Processing", "id": 31, "label": "Enhancing Suicide Risk Assessment: A Speech-Based Automated Approach in Emergency Medicine", "shape": "dot", "size": 10, "title": "Authors: Maurice Gerczuk, Justa Lutz, Wolfgang Strube, lkomiet Hasan,, lexander Kathan, Bj\u00a8orn W. Schuller,,,,\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 The delayed access to specialized psychiatric\nassessments and care for patients at risk of suicidal tendencies\nin emergency departments creates a notable gap in timely\nintervention, hindering the provision of adequate mental health\nsupport during critical situations. To address this, we present\na non-invasive, speech-based approach for automatic suicide\nrisk assessment. For our study, we have collected a novel\ndataset of speech recordings from 20 patients from which we\nextract three sets of features, including wav2vec, interpretable\nspeech and acoustic features, and deep learning-based spectral\nrepresentations. We proceed by conducting a binary classification\nto assess suicide risk in a leave-one-subject-out fashion. Our\nmost effective speech model achieves a balanced accuracy of\n66.2 %. Moreover, we show that integrating our speech model\nwith a series of patients\u2019 metadata, such as the history of suicide\nattempts or access to firearms, improves the overall result. The\nmetadata integration yields a balanced accuracy of 94.4 %,\nmarking an absolute improvement of 28.2 %, demonstrating the\nefficacy of our proposed approaches for automatic suicide risk\nassessment in emergency medicine.\nI."}, {"group": "Natural Language Processing", "id": 61, "label": "Amino Acid Classi\ufb01cation in 2D NMR Spectra via Acoustic Signal Embeddings", "shape": "dot", "size": 10, "title": "Authors: Jia Qi Yip\u00a7\u2020, Dianwen Ng\u00a7\u2020, B Ma\u00a7, Konstant Pervush\u2020, Eng Siong Chng\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Nuclear Magnetic Resonance (NMR) is used in\nstructural biology to experimentally determine the structure of\nproteins, which is used in many areas of biology and is an\nimportant part of drug development. Unfortunately, NMR data\ncan cost thousands of dollars per sample to collect and it can take\na specialist weeks to assign the observed resonances to speci\ufb01c\nchemical groups. There has thus been growing interest in the\nNMR community to use deep learning to automate NMR data\nannotation. Due to similarities between NMR and audio data, we\npropose that methods used in acoustic signal processing can be\napplied to NMR as well. Using a simulated amino acid dataset,\nwe show that by swapping out \ufb01lter banks with a trainable\nconvolutional encoder, acoustic signal embeddings from speaker\nveri\ufb01cation models can be used for amino acid classi\ufb01cation\nin 2D NMR spectra by treating each amino acid as a unique\nspeaker. On an NMR dataset comparable in size with of 46 hours\nof audio, we achieve a classi\ufb01cation performance of 97.7% on a\n20-class problem. We also achieve a 23% relative improvement\nby using an acoustic embedding model compared to an existing\nNMR-based model.\nI."}, {"group": "Network Science", "id": 19, "label": "Challenging Negative Gender Stereotypes: A Study on the Effectiveness of Automated Counter-Stereotypes", "shape": "dot", "size": 10, "title": "Authors: Isar Nejadgholi,, Kathleen C. Fraser,, nna Kerkh, Svetlana Kiritchenko, isar.nejadgholi, kathleen.fraser\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Content Warning: This paper presents examples of gender stereotypes that may be offensive or upsetting.\nGender stereotypes are pervasive beliefs about individuals based on their gender that play a significant\nrole in shaping societal attitudes, behaviours, and even opportunities. Recognizing the negative implications of\ngender stereotypes, particularly in online communications, this study investigates eleven strategies to automatically\ncounteract and challenge these views. We present AI-generated gender-based counter-stereotypes to (self-identified)\nmale and female study participants and ask them to assess their offensiveness, plausibility, and potential effectiveness.\nThe strategies of counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of\ngroup membership) emerged as the most robust approaches, while humour, perspective-taking, counter-examples,\nand empathy for the speaker were perceived as less effective. Also, the differences in ratings were more pronounced\nfor stereotypes about the different targets than between the genders of the raters. Alarmingly, many AI-generated\ncounter-stereotypes were perceived as offensive and/or implausible. Our analysis and the collected dataset offer\nfoundational insight into counter-stereotype generation, guiding future efforts to develop strategies that effectively\nchallenge gender stereotypes in online interactions.\nKeywords: Gender Stereotypes, Counter-Stereotypes, Social Influence, Online Conversations\n1."}, {"group": "Network Science", "id": 110, "label": "Taking sides: Public Opinion over the Israel-Palestine Con\ufb02ict in 2021", "shape": "dot", "size": 10, "title": "Authors: Arsal Imtiaz, Danish Khan, Hanjia Lyu, Jiebo Luo, aimtiaz, dkhan\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The Israel-Palestine Con\ufb02ict, one of the most enduring con-\n\ufb02icts in history, dates back to the start of 20th century, with\nthe establishment of the British Mandate in Palestine and has\ndeeply rooted complex issues in politics, demography, reli-\ngion, and other aspects, making it harder to attain resolve. To\nunderstand the con\ufb02ict in 2021, we devise an observational\nstudy to aggregate stance held by English-speaking countries.\nWe collect Twitter data using popular hashtags around and\nspeci\ufb01c to the con\ufb02ict portraying opinions neutral or partial to\nthe two parties. We use different tools and methods to classify\ntweets into pro-Palestinian, pro-Israel, or neutral. This paper\nfurther describes the implementation of data mining method-\nologies to obtain insights and reason the stance held by citi-\nzens around the con\ufb02ict."}, {"group": "Natural Language Processing", "id": 42, "label": "Language-Based Augmentation to Address Shortcut Learning in Object-Goal Navigation", "shape": "dot", "size": 10, "title": "Authors: Dennis Hoftijzer, Gertjan Burghouts, Luuk Spreeuwers\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Deep Reinforcement Learning (DRL) has shown\ngreat potential in enabling robots to find certain objects (e.g.,\n\u2018find a fridge\u2019) in environments like homes or schools. This task\nis known as Object-Goal Navigation (ObjectNav). DRL methods\nare predominantly trained and evaluated using environment\nsimulators. Although DRL has shown impressive results, the\nsimulators may be biased or limited. This creates a risk of\nshortcut learning, i.e., learning a policy tailored to specific\nvisual details of training environments. We aim to deepen our\nunderstanding of shortcut learning in ObjectNav, its implications\nand propose a solution. We design an experiment for inserting\na shortcut bias in the appearance of training environments.\nAs a proof-of-concept, we associate room types to specific wall\ncolors (e.g., bedrooms with green walls), and observe poor\ngeneralization of a state-of-the-art (SOTA) ObjectNav method\nto environments where this is not the case (e.g., bedrooms with\nblue walls). We find that shortcut learning is the root cause: the\nagent learns to navigate to target objects, by simply searching\nfor the associated wall color of the target object\u2019s room. To\nsolve this, we propose Language-Based (L-B) augmentation. Our\nkey insight is that we can leverage the multimodal feature\nspace of a Vision-Language Model (VLM) to augment visual\nrepresentations directly at the feature-level, requiring no changes\nto the simulator, and only an addition of one layer to the model.\nWhere the SOTA ObjectNav method\u2019s success rate drops 69%,\nour proposal has only a drop of 23%. Code is available at\nhttps://github.com/Dennishoftijzer/L-B Augmentation\nIndex Terms\u2014Vision-based Navigation, Deep Reinforcement\nLearning, Vision-Language\nI."}, {"group": "Natural Language Processing", "id": 84, "label": "Do Visual-Language Maps Capture Latent Semantics?", "shape": "dot", "size": 10, "title": "Authors: Matti Pekkanen, Tsvetomila Mihaylova, Francesco Verdoja, Ville Kyrki\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Visual-language models (VLMs) have recently\nbeen introduced in robotic mapping by using the latent rep-\nresentations, i.e., embeddings, of the VLMs to represent the\nnatural language semantics in the map. The main benefit is\nmoving beyond a small set of human-created labels toward\nopen-vocabulary scene understanding. While there is anecdotal\nevidence that maps built this way support downstream tasks,\nsuch as navigation, rigorous analysis of the quality of the\nmaps using these embeddings is lacking. We investigate two\ncritical properties of map quality: queryability and consistency.\nThe evaluation of queryability addresses the ability to retrieve\ninformation from the embeddings. We investigate two aspects of\nconsistency: intra-map consistency and inter-map consistency.\nIntra-map consistency captures the ability of the embeddings to\nrepresent abstract semantic classes, and inter-map consistency\ncaptures the generalization properties of the representation.\nIn this paper, we propose a way to analyze the quality of\nmaps created using VLMs, which forms an open-source bench-\nmark to be used when proposing new open-vocabulary map\nrepresentations. We demonstrate the benchmark by evaluating\nthe maps created by two state-of-the-art methods, VLMaps\nand OpenScene, using two encoders, LSeg and OpenSeg, using\nreal-world data from the Matterport3D data set. We find that\nOpenScene outperforms VLMaps with both encoders, and LSeg\noutperforms OpenSeg with both methods.\nI."}, {"group": "Network Science", "id": 45, "label": "Gun Culture in Fringe Social Media", "shape": "dot", "size": 10, "title": "Authors: Fatemeh Tahmasbi, Aakarsha Chug, Barry Bradlyn, Jeremy Blackburn\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The increasing frequency of mass shootings in the United\nStates has, unfortunately, become a norm. While the issue of\ngun control in the US involves complex legal concerns, there\nare also societal issues at play. One such social issue is so-\ncalled \u201cgun culture,\u201d i.e., a general set of beliefs and actions\nrelated to gun ownership. However relatively little is known\nabout gun culture, and even less is known when it comes to\nfringe online communities. This is especially worrying con-\nsidering the aforementioned rise in mass shootings and nu-\nmerous instances of shooters being radicalized online.\nTo address this gap, we explore gun culture on /k/, 4chan\u2019s\nweapons board. More specifically, using a variety of quanti-\ntative techniques, we examine over 4M posts on /k/ and posi-\ntion their discussion within the larger body of theoretical un-\nderstanding of gun culture. Among other things, our findings\nsuggest that gun culture on /k/ covers a relatively diverse set\nof topics (with a particular focus on legal discussion), some\nof which are signals of fetishism."}, {"group": "Network Science", "id": 103, "label": "An Intelligent System for Multi-topic Social Spam", "shape": "dot", "size": 10, "title": "Authors: Bilal Abu, Dana Al Qudah, Malak Al-Hassan, Seyed Mohssen Ghafari, Tomayess Issa, Ibrahim Aljarah, Amin Beheshti, Sulaiman Alqahtani\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: : The communication revolution has perpetually reshaped the means through which people send and \nreceive information. Social media is an important pillar of this revolution and has brought profound changes to \nvarious aspects of our lives. However, the open environment and popularity of these platforms inaugurate \nwindows of opportunities for various cyber threats, thus social networks have become a fertile venue for \nspammers and other illegitimate users to execute their malicious activities. These activities include phishing hot \nand trendy topics and posting a wide range of contents in many topics. Hence, it is crucial to continuously \nintroduce new techniques and approaches to detect and stop this category of users. This paper proposes a \nnovel and effective approach to detect social spammers. An investigation into several attributes to measure \ntopic-dependent and topic-independent users\u2019 behaviours on Twitter is carried out. The experiments of this \nstudy are undertaken on various machine learning classifiers. The performance of these classifiers are compared \nand their effectiveness is measured via a number of robust evaluation measures. Further, the proposed approach \nis benchmarked against state-of-the-art social spam and anomalous detection techniques. These experiments \nreport the effectiveness and utility of the proposed approach and embedded modules. \nKeywords: Social Spammers, Online Social Networks, Machine Learning, Social Credibility, Semantic Analysis, \nCyber Threats. \n1."}, {"group": "Network Science", "id": 114, "label": "Toxic Synergy Between Hate Speech and Fake News Exposure", "shape": "dot", "size": 10, "title": "Authors: Filippo Menczer\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Hate speech on social media is a pressing concern. Under-\nstanding the factors associated with hate speech may help\nmitigate it. Here we explore the association between hate\nspeech and exposure to fake news by studying the correla-\ntion between exposure to news from low-credibility sources\nthrough following connections and the use of hate speech on\nTwitter. Using news source credibility labels and a dataset\nof posts with hate speech targeting various populations, we\nfind that hate speakers are exposed to lower percentages of\nposts linking to credible news sources. When taking the tar-\nget population into account, we find that this association is\nmainly driven by anti-semitic and anti-Muslim content. We\nalso observe that hate speakers are more likely to be exposed\nto low-credibility news with low popularity. Finally, while\nhate speech is associated with low-credibility news from par-\ntisan sources, we find that those sources tend to skew to the\npolitical left for antisemitic content and to the political right\nfor hate speech targeting Muslim and Latino populations. Our\nresults suggest that mitigating fake news and hate speech may\nhave synergistic effects."}, {"group": "History of Science", "id": 50, "label": "arXiv:2403.08375v1  [cs.DB]  13 Mar 2024", "shape": "dot", "size": 10, "title": "Authors: Salwa Alamir, Xiaomo Liu\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Migrations of systems from on-site premises to the cloud has been\na fundamental endeavor by many industrial institutions. A crucial\ncomponent of such cloud migrations is the transition of databases\nto be hosted online. In this work, we consider the di\ufb03culties of this\nmigration for SQL databases. While SQL is one of the prominent\nmethods for storing database procedures, there are a plethora of\ndi\ufb00erent SQL dialects (e.g., MySQL, Postgres, etc.) which can com-\nplicate migrations when the on-premise SQL dialect di\ufb00ers to the\ndialect hosted on the cloud. Tools exist by common cloud provides\nsuch as AWS and Azure to aid in translating between dialects in\norder to mitigate the majority of the di\ufb03culties. However, these\ntools do not successfully translate 100% of the code. Consequently,\nsoftware engineers must manually convert the remainder of the\nuntranslated database. For large organizations, this task quickly\nbecomes intractable and so more innovative solutions are required.\nWe consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud mi-\ngrations. Furthermore, we introduce potential avenues of research\nto tackle this challenge that have yielded promising preliminary\nresults.\nCCS CONCEPTS\n\u2022 Theory of computation \u2192Database query processing and opti-\nmization (theory); \u2022 Information systems \u2192Cloud based storage;\n\u2022 Computing methodologies \u2192Machine translation.\nKEYWORDS\nCloud Migration, SQL, Code Translation\nACM Reference Format:\nRan Zmigrod, Salwa Alamir, and Xiaomo Liu. 2024. Translating between\nSQL Dialects for Cloud Migration. In 46th International Conference on Soft-\nware Engineering: Software Engineering in Practice (ICSE-SEIP \u201924), April 14\u2013\n20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3639477.3639727"}, {"group": "Network Science", "id": 66, "label": "NRBdMF: Recommendation Algorithm for Drug Effects Prediction Considering Directionality", "shape": "dot", "size": 10, "title": "Authors: Iori Azuma, Tadahaya Mizuno, Hiroyuki Kusuhara, Tadahaya Mizuno\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "Network Science", "id": 93, "label": "On the Persistence of Higher-Order Interactions in Real-World Hypergraphs", "shape": "dot", "size": 10, "title": "Authors: Hyunj Choo\u2217, Kijung Sh\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: A hypergraph, which generalizes an ordinary graph, naturally represents group interactions as hyperedges\n(i.e., arbitrary-sized subsets of nodes). Such group interactions are ubiquitous: the sender and receivers of an\nemail, the co-authors of a publication, and the items co-purchased by a customer, to name a few. A higher-\norder interaction (HOI) in a hypergraph is de\ufb01ned as the co-appearance of a set of nodes in any hyperedge.\nOur focus is the persistence of HOIs repeated over time, which is naturally interpreted as the strength of\ngroup relationships, aiming at answering three questions: (a) How do HOIs in real-world hypergraphs persist\nover time? (b) What are the key factors governing the persistence? (c) How accurately can we predict the\npersistence?\nIn order to answer the questions above, we investigate the persistence of HOIs in 13 real-world hypergraphs\nfrom six domains. First, we de\ufb01ne how to measure the persistence of HOIs. Then, we examine global patterns\nand anomalies in the persistence, revealing a power-law relationship. After that, we study the relations between\nthe persistence and 16 structural features of HOIs, some of which are closely related to the persistence. Lastly,\nbased on the 16 structural features, we assess the predictability of the persistence under various settings and\n\ufb01nd strong predictors. Note that predicting the persistence of HOIs has many potential applications, such as\nrecommending items to be purchased together and predicting missing recipients of emails.\nKeywords: Temporal Hypergraph, Higher-Order Interaction, Persistence, Predictability"}, {"group": "Network Science", "id": 124, "label": "Community detection and anomaly prediction in dynamic networks", "shape": "dot", "size": 10, "title": "Authors: Catera De Bacco\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Anomaly detection is an essential task in the analysis of dynamic networks, as it can provide early warning of\npotential threats or abnormal behavior. We present a principled approach to detect anomalies in dynamic networks\nthat integrates community structure as a foundational model for regular behavior. Our model identifies anomalies as\nirregular edges while capturing structural changes. Leveraging a Markovian approach for temporal transitions and\nincorporating structural information via latent variables for communities and anomaly detection, our model infers these\nhidden parameters to pinpoint abnormal interactions within the network. Our approach is evaluated on both synthetic\nand real-world datasets. Real-world network analysis shows strong anomaly detection across diverse scenarios. In a\nmore specific study of transfers of professional male football players, we observe various types of unexpected patterns\nand investigate how the country and wealth of clubs influence interactions. Additionally, we identify anomalies between\nclubs with incompatible community memberships, but also instances of anomalous transactions between clubs with\nsimilar memberships. The latter is due in particular to the dynamic nature of the transactions, as we find that the\nfrequency of transfers results in anomalous behaviors that are otherwise expected to interact as they belong to similar\ncommunities.\nI."}, {"group": "History of Science", "id": 65, "label": "Causality in Cardiorespiratory Signals in Pediatric Cardiac Patients", "shape": "dot", "size": 10, "title": "Authors: Granger\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Four different Granger causality-based methods -\none linear and three nonlinear (Granger Causality, Kernel \nGranger Causality, large-scale Nonlinear Granger Causality, \nand Neural Network Granger Causality) were used for \nassessment and causal-based quantification of the respiratory \nsinus arrythmia (RSA) in the group of pediatric cardiac patients, \nbased on the single-lead ECG and impedance pneumography \nsignals (the latter as the tidal volume curve equivalent). Each \nmethod was able to detect the dependency (in terms of causal \ninference) between respiratory and cardiac signals. The \ncorrelations between quantified RSA and the demographic \nparameters were also studied, but the results differ for each \nmethod. \n \nClinical relevance\u2014 The presented methods (among which \nNNGC seems to be the most valid) allow for quantification of \nRSA and study of dependency between tidal volume and RR \nintervals, which may help to better understand association \nbetween respiratory and cardiovascular systems in different \npopulations.  \nI."}, {"group": "Network Science", "id": 95, "label": "Noname manuscript No. (will be inserted by the editor)", "shape": "dot", "size": 10, "title": "Authors: Wei Chen, Weiqing Wang, Hongzhi Yin, Lei Zhao(B, Xiaofang Zhou\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Sources of complementary information are\nconnected when we link user accounts belonging to the\nsame user across di\ufb00erent platforms or devices. The\nexpanded information promotes the development of a\nwide range of applications, such as cross-platform pre-\ndiction, cross-platform recommendation, and advertise-\nment. Due to the signi\ufb01cance of user account linkage\nand the widespread popularization of GPS-enabled mo-\nbile devices, there are increasing research studies on\nlinking user account with spatio-temporal data across\nlocation-aware social networks. Being di\ufb00erent from most\nexisting studies in this domain that only focus on the\ne\ufb00ectiveness, we propose a novel framework entitled\nHFUL (A Hybrid Framework for User Account Linkage\nacross Location-Aware Social Networks), where e\ufb03ciency,\ne\ufb00ectiveness, scalability, robustness, and application of\nuser account linkage are considered. Speci\ufb01cally, to im-\nprove the e\ufb03ciency, we develop a comprehensive in-\ndex structure from the spatio-temporal perspective, and\ndesign novel pruning strategies to reduce the search\nWei Chen\nrobertchen@suda.edu.cn\nWeiqing Wang\nteresa.wang@monash.edu\nHongzhi Yin\ny.yin1@uq.edu.au\nLei Zhao\nzhaol@suda.edu.cn\nXiaofang Zhou\nzxf@cse.ust.hk\n1Institute of Arti\ufb01cial Intelligence, School of Computer Sci-\nence and Technology, Soochow University, China\n2Faculty of Information Technology, Monash University, Mel-\nbourne, Australia\n3School of ITEE, The University of Queensland, Bribane,\nAustralia\n4The Hong Kong University of Science and Technology, Hong\nKong, China\nspace. To improve the e\ufb00ectiveness, a kernel density\nestimation-based method has been proposed to alle-\nviate the data sparsity problem in measuring users\u2019\nsimilarities. Additionally, we investigate the application\nof HFUL in terms of user prediction, time prediction,\nand location prediction. The extensive experiments con-\nducted on three real-world datasets demonstrate the su-\nperiority of HFUL in terms of e\ufb00ectiveness, e\ufb03ciency,\nscalability, robustness, and application compared with\nthe state-of-the-art methods.\nKeywords User Account Linkage \u00b7 Social Networks \u00b7\nLocation Data"}, {"group": "History of Science", "id": 123, "label": "INTELLIGENT INFORMATION PROCESSING LABORATORY, SCHOOL OF EIE, BEIJING JIAOTONG UNIVERSITY", "shape": "dot", "size": 10, "title": "Authors: Mingze Sun\u2217, Yiqing Wang\u2020, Zhenyi Zhao\u2021\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014A novel crowd stampede detection and prediction\nalgorithm based on Deformable DETR is proposed to address the\nchallenges of detecting a large number of small targets and target\nocclusion in crowded airport and train station environments.\nIn terms of model design, the algorithm incorporates a multi-\nscale feature fusion module to enlarge the receptive field and en-\nhance the detection capability of small targets. Furthermore, the\ndeformable attention mechanism is improved to reduce missed\ndetections and false alarms for critical targets [1]. Additionally,\na new algorithm is innovatively introduced for stampede event\nprediction and visualization. Experimental evaluations on the\nPKX-LHR dataset demonstrate that the enhanced algorithm\nachieves an 34% performance in small target detection accuracy\nwhile maintaining the original detection speed.\nIndex Terms\u2014Multi Scale Deformable Attention; small target\ndetection; stampede event prediction; multi-scale feature fusion\nmodule.\nI."}, {"group": "History of Science", "id": 82, "label": "HeR-DRL:Heterogeneous Relational Deep Reinforcement Learning for Decentralized Multi-Robot Crowd Navigation", "shape": "dot", "size": 10, "title": "Authors: Songhao Piao\u2217, Wenzheng Chi, Liguo Chen, Wei Li\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Crowd navigation has received significant research\nattention in recent years, especially DRL-based methods. While\nsingle-robot crowd scenarios have dominated research, they\noffer limited applicability to real-world complexities. The het-\nerogeneity of interaction among multiple agent categories, like\nin decentralized multi-robot pedestrian scenarios, are frequently\ndisregarded. This \u201dinteraction blind spot\u201d hinders generaliz-\nability and restricts progress towards robust navigation algo-\nrithms. In this paper, we propose a heterogeneous relational\ndeep reinforcement learning(HeR-DRL), based on customised\nheterogeneous GNN, in order to improve navigation strategies in\ndecentralized multi-robot crowd navigation. Firstly, we devised a\nmethod for constructing robot-crowd heterogenous relation graph\nthat effectively simulates the heterogeneous pair-wise interaction\nrelationships. We proposed a new heterogeneous graph neural\nnetwork for transferring and aggregating the heterogeneous state\ninformation. Finally, we incorporate the encoded information into\ndeep reinforcement learning to explore the optimal policy. HeR-\nDRL are rigorously evaluated through comparing it to state-\nof-the-art algorithms in both single-robot and multi-robot circle\ncrowssing scenario. The experimental results demonstrate that\nHeR-DRL surpasses the state-of-the-art approaches in overall\nperformance, particularly excelling in safety and comfort metrics.\nThis underscores the significance of interaction heterogeneity for\ncrowd navigation. The source code will be publicly released in\nhttps://github.com/Zhouxy-Debugging-Den/HeR-DRL.\nI."}, {"group": "History of Science", "id": 88, "label": "Online Concurrent Multi-Robot Coverage Path Planning", "shape": "dot", "size": 10, "title": "Authors: Indranil Saha\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 Recently,\ncentralized\nreceding\nhorizon\nonline\nmulti-robot coverage path planning algorithms have shown\nremarkable scalability in thoroughly exploring large, complex,\nunknown workspaces with many robots. In a horizon, the path\nplanning and the path execution interleave, meaning when the\npath planning occurs for robots with no paths, the robots\nwith outstanding paths do not execute, and subsequently, when\nthe robots with new or outstanding paths execute to reach\nrespective goals, path planning does not occur for those robots\nyet to get new paths, leading to wastage of both the robotic\nand the computation resources. As a remedy, we propose a\ncentralized algorithm that is not horizon-based. It plans paths\nat any time for a subset of robots with no paths, i.e., who\nhave reached their previously assigned goals, while the rest\nexecute their outstanding paths, thereby enabling concurrent\nplanning and execution. We formally prove that the proposed\nalgorithm ensures complete coverage of an unknown workspace\nand analyze its time complexity. To demonstrate scalability, we\nevaluate our algorithm to cover eight large 2D grid bench-\nmark workspaces with up to 512 aerial and ground robots,\nrespectively. A comparison with a state-of-the-art horizon-based\nalgorithm shows its superiority in completing the coverage with\nup to 1.6\u00d7 speedup. For validation, we perform ROS + Gazebo\nsimulations in six 2D grid benchmark workspaces with 10\nQuadcopters and TurtleBots, respectively. We also successfully\nconducted one outdoor experiment with three quadcopters and\none indoor with two TurtleBots.\nI."}, {"group": "History of Science", "id": 43, "label": "MetroGNN: Metro Network Expansion with Reinforcement Learning", "shape": "dot", "size": 10, "title": "Authors: Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li\u2217\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Selecting urban regions for metro network expansion to meet maxi-\nmal transportation demands is crucial for urban development, while\ncomputationally challenging to solve. The expansion process re-\nlies not only on complicated features like urban demographics\nand origin-destination (OD) flow but is also constrained by the\nexisting metro network and urban geography. In this paper, we\nintroduce a reinforcement learning framework to address a Markov\ndecision process within an urban heterogeneous multi-graph. Our\napproach employs an attentive policy network that intelligently\nselects nodes based on information captured by a graph neural net-\nwork. Experiments on real-world urban data demonstrate that our\nproposed methodology substantially improve the satisfied trans-\nportation demands by over 30% when compared with state-of-the-\nart methods. Codes are published at https://github.com/tsinghua-\nfib-lab/MetroGNN.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Planning and scheduling;\nReinforcement learning.\nKEYWORDS\nmetro network, reinforcement learning, graph neural networks\nACM Reference Format:\nHongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, and Yong Li. 2024.\nMetroGNN: Metro Network Expansion with Reinforcement Learning. In\nCompanion Proceedings of the ACM Web Conference 2024 (WWW \u201924 Com-\npanion), May 13\u201317, 2024, Singapore, Singapore. ACM, New York, NY, USA,\n4 pages. https://doi.org/10.1145/3589335.3651536\n\u2217Corresponding author (liyong07@tsinghua.edu.cn).\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nWWW \u201924 Companion, May 13\u201317, 2024, Singapore, Singapore\n\u00a9 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0172-6/24/05.\nhttps://doi.org/10.1145/3589335.3651536"}, {"group": "History of Science", "id": 49, "label": "Efficient Fault Tolerance for Pipelined Query Engines via Write-ahead Lineage", "shape": "dot", "size": 10, "title": "Authors: Ziheng Wang, Alex Aiken\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Modern distributed pipelined query engines either\ndo not support intra-query fault tolerance or employ high-\noverhead approaches such as persisting intermediate outputs or\ncheckpointing state. In this work, we present write-ahead lineage,\na novel fault recovery technique that combines Spark\u2019s lineage-\nbased replay and write-ahead logging. Unlike Spark, where\nthe lineage is determined before query execution, write-ahead\nlineage persistently logs lineage at runtime to support dynamic\ntask dependencies in pipelined query engines. Since only KB-\nsized lineages are persisted instead of MB-sized intermediate\noutputs, the normal execution overhead is minimal compared\nto spooling or checkpointing based approaches. To ensure fast\nfault recovery times, tasks only consume intermediate outputs\nwith persisted lineage, preventing global rollbacks upon failure.\nIn addition, lost tasks from different stages can be recovered in\na pipelined parallel manner. We implement write-ahead lineage\nin a distributed pipelined query engine called Quokka. We show\nthat Quokka is around 2x faster than SparkSQL on the TPC-H\nbenchmark with similar fault recovery performance.\nI."}, {"group": "History of Science", "id": 59, "label": "Stochastic failure of cell infection post viral entry: Implications for infection outcomes and antiviral therapy", "shape": "dot", "size": 10, "title": "Authors: Christian Quirouette, Daniel Cresta, Jizhou Li, Kathleen P. Wilkie, Haozhao Liang,\u00a4, Catherine A.A. Beauchemin,,\u2217\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 80, "label": "epl draft", "shape": "dot", "size": 10, "title": "Authors: Senador Salgado Filho\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2013 We investigate a cyclic game system where organisms face an epidemic beyond being\nthreatened by natural enemies.\nAs a survival strategy, individuals of one out of the species\nusually safeguard themselves by approaching the enemies of their enemies and performing social\ndistancing to escape contamination when an outbreak a\ufb00ects the neighbourhood. We simulate\nhow the survival movement strategy to local epidemic surges must adapt if a pathogen mutation\nmakes the disease deadlier. We study the spatial distribution of local outbreaks and observe the\nin\ufb02uence of disease mortality on individuals\u2019 spatial organisation. We show that adapting the\nsurvival movement strategy for a high mortality disease demands an altruistic behaviour of the\norganisms since their death risk increases. Despite weakening the disease transmission chain, which\nbene\ufb01ts the species, abandoning refuges provided by safeguarding social interaction increases the\nvulnerability to being eliminated in the cyclic game. Considering that not all individuals exhibit\naltruism, we \ufb01nd the relative growth in the species density as a function of the proportion of\nindividuals behaving altruistically. Our results may be helpful for biologists and data scientists to\nunderstand how adaptive altruistic processes can a\ufb00ect population dynamics in complex systems."}, {"group": "History of Science", "id": 100, "label": "Generating Connected, Simple, and Realistic Cyber Graphs for Smart Grids", "shape": "dot", "size": 10, "title": "Authors: Katherine Davis\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014Smart grids integrate communication systems with\npower networks to enable power grids operation and command\nthrough real-time data collection and control signals. Designing,\nanalyzing, and simulating smart grid infrastructures as well\nas predicting the impact of power network failures strongly\ndepend on the topologies of the underlying power network\nand communication system. Despite the substantial impact that\nthe communication systems bring to smart grid operation, the\ntopology of communication systems employed in smart grids\nwas less studied. The power community lacks realistic generative\ncommunication system models that can be calibrated to match\nreal-world data. To address this issue, this paper proposes a\nframework to generate the underlying topological graphs for\nthe communication systems deployed in smart grids by mim-\nicking the topology of real-world smart grids. In this regard,\nwe have updated the Chung-Lu algorithm to guarantee the\ncommunication network connectivity and to match the degree\ndistribution of a real-world smart grid rather than following\nan expected degree distribution. In addition, key characteristics\nof communication systems such as diameter, average shortest\npaths, clustering coef\ufb01cients, assortativity, and spectral gap were\ntaken into consideration to generate the most similar real-world\ncommunication network for smart grid studies. We believe that\nthe proposed algorithm to generate realistic cyber graphs for\nsmart grid studies will bene\ufb01t the power community.\nNOMENCLATURE\nG, V, E\nGraph, set of nodes, set of edges\nn, m P Z\n|V|, |E|\nu.d P R\nDegree of a node u\nS P Zn\nDegree sequence\nd P Z\nMax. degree\nx\np\u00a8q\nNormalization operation s.t. \u0159n\n1 x\np\u00a8q \u201c 1\nD P Zd\nDegree vector D \u201c r1, . . . , ds\npK P Rd\nNormalized frequency vector\nG.g P t0, 1u G is graphical (no self loops and parallel edges)\nG.c P t0, 1u G is connected\n\u03c1 P R\nDensity as m{n\nI P R\nDiameter as the longest shortest path\nrsp P R\nAverage shortest path\ncc P R\nClustering coef\ufb01cient\na P R\nAssortativity as the Pearson correlation coef\ufb01-\ncient of degrees between pairs of linked nodes\n\u03bb P R\nSpectral gap as the minimum non-zero eigen-\nvalue of the normalized Laplacian\nThis work was supported by NSF under Award Number 1808064.\nI."}, {"group": "Network Science", "id": 115, "label": "Interest Maximization in Social Networks", "shape": "dot", "size": 10, "title": "Authors: Rahul Kumar Gautam, Anjeneya Swami Kare, S. Durga Bhavani\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: . Nowadays, organizations use viral marketing strategies to promote their products\nthrough social networks. It is expensive to directly send the product promotional information\nto all the users in the network. In this context, Kempe et al. [10] introduced the Influence\nMaximization (IM) problem, which identifies k most influential nodes (spreader nodes), such\nthat the maximum number of people in the network adopts the promotional message.\nMany variants of the IM problem have been studied in the literature, namely, Perfect Evan-\ngelising Set (PES), Perfect Awareness Problem (PAP), etc. In this work, we propose a maxi-\nmization version of PAP called the Interest Maximization problem. Different people have\ndifferent levels of interest in a particular product. This is modeled by assigning an interest\nvalue to each node in the network. Then, the problem is to select k initial spreaders such\nthat the sum of the interest values of the people (nodes) who become aware of the message\nis maximized.\nWe study the Interest Maximization problem under two popular diffusion models: the\nLinear Threshold Model (LTM) and the Independent Cascade Model (ICM). We show that\nthe Interest Maximization problem is NP-Hard under LTM. We give linear programming\nformulation for the problem under LTM. We propose four heuristic algorithms for the In-\nterest Maximization problem: Level Based Greedy Heuristic (LBGH), Maximum Degree\nFirst Heuristic (MDFH), Profit Based Greedy Heuristic (PBGH), and Maximum Profit Based\nGreedy Heuristic (MPBGH). Extensive experimentation has been carried out on many real-\nworld benchmark data sets for both the diffusion models. The results show that among the\nproposed heuristics, MPBGH performs better in maximizing the interest value."}, {"group": "History of Science", "id": 129, "label": "Effective Individual Fairest Community Search over Heterogeneous Information Networks", "shape": "dot", "size": 10, "title": "Authors: Ningning Cui, Wei Luo\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Community search over heterogeneous information networks has\nbeen applied to wide domains, such as activity organization and\nteam formation. From these scenarios, the members of a group\nwith the same treatment often have different levels of activity and\nworkloads, which causes unfairness in the treatment between ac-\ntive members and inactive members (called individual unfairness).\nHowever, existing works do not pay attention to individual fairness\nand do not sufficiently consider the rich semantics of HINs (e.g.,\nhigh-order structure), which disables complex queries. To fill the\ngap, we formally define the issue of individual fairest community\nsearch over HINs (denoted as IFCS), which aims to find a set of\nvertices from the HIN that own the same type, close relationships,\nand small difference of activity level and has been demonstrated to\nbe NP-hard. To do this, we first develop an exploration-based filter\nthat reduces the search space of the community effectively. Further,\nto avoid repeating computation and prune unfair communities in\nadvance, we propose a message-based scheme and a lower bound-\nbased scheme. At last, we conduct extensive experiments on four\nreal-world datasets to demonstrate the effectiveness and efficiency\nof our proposed algorithms, which achieve at least \u00d73 times faster\nthan the baseline solution.\nCCS CONCEPTS\n\u2022 Do Not Use This Code \u2192Generate the Correct Terms for\nYour Paper; Generate the Correct Terms for Your Paper; Generate\nthe Correct Terms for Your Paper; Generate the Correct Terms for\nYour Paper.\nACM Reference Format:\nTaige Zhao, Jianxin Li, Ningning Cui, and Wei Luo. 2018. Effective Individual\nFairest Community Search over Heterogeneous Information Networks. In\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nProceedings of Make sure to enter the correct conference title from your rights\nconfirmation emai (Conference acronym \u2019XX). ACM, New York, NY, USA,\n13 pages. https://doi.org/XXXXXXX.XXXXXXX"}, {"group": "History of Science", "id": 132, "label": "The resonant acoustic signatures of lithic debitage", "shape": "dot", "size": 10, "title": "Authors: Margaret A. Morris a, Petr Krysl b, Isabel C. Rivera-Collazo\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Acoustic methods to search for submerged archaeological sites have shown that concentrations of knapped flint \nproduce a visible acoustic response in chirp sonar profiles in a variety of geographical settings. Field tests and \nsimulations have suggested that the submerged lithic signal is due to acoustic resonances of the flaked stone. We \nmodel and measure the resonant acoustic signatures of chert, obsidian, metavolcanic, and granitic lithic debit\u00ad\nage. Struck lithics produce multiple resonant peaks under 30 kHz, with high quality factors that decrease with \nmaterial coarseness. We use a combination of the finite element and boundary element methods to model the \nnatural vibrations of lithic debitage in both air and water. Direct measurement of lithic material density and \nadjustment of the Young\u2019s modulus and Poisson\u2019s ratio provide excellent correspondence between measured and \nmodeled resonances. Using a coupled finite element and boundary integral method, we model the acoustic \nscattering return of individual lithics in water as a function of frequency and incidence angle. We find the \nstrongest resonant signal between 8 and 16 kHz for a collection of lithic debitage. Results indicate that the lithic \nresonance signal is highly directional, with target strength up to \u221220 dB when excited at optimal angles. For a \nflat-laying lithic, target strength at normal incidence is, on average, 10 dB lower than the strongest signal, \ntypically found 55\u25e6\u00b1 18\u25e6from normal incidence. We suggest that the best way to detect submerged lithics may \nnot be through standard mono-static sub-bottom profiling with a direct downward pulse, but with a chirp pulse \nsent and received at an angle with respect to the sea bottom.   \n1."}, {"group": "History of Science", "id": 138, "label": "On Stress Driven Diffusion in Bone - An Experimental Study", "shape": "dot", "size": 10, "title": "Authors: Gustav Ldbergaand, Per St\u00e5hleb\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The transport of nutrients or signal constituents that stimulate growth of bone\ntissue is supposed to be a\ufb00ected by a static mechanical load. It follows from\nbasic thermodynamical principles that constituents causing volumetric change\nare dragged along the gradients of hydrostatic stress. The present preliminary\nstudy examines the behaviour of iodine present in the medullary cavity of a\nbovine long bone exposed to mechanical load.\nA section of the bone is x-\nray scanned with the static load present, with and without the iodine. The\nresulting distribution in a selected 2D plane is numerically evaluated using a\ndiscrete Radon\u2019s inverse transform. The result suggests that iodine is a useful\nconstituent with a good attenuation e\ufb00ect on the x-ray beam and clearly reveals\nthe temporal distribution of its transport through the bone. It further result\nshows some indication that stress does a\ufb00ect the iodine distribution. .\nKeywords:\nStress-driven di\ufb00usion, tomography, bone, experimental, Radon\u2019s\nintegral\n1."}, {"group": "History of Science", "id": 69, "label": "Bridging the gap between target-based and cell-based drug", "shape": "dot", "size": 10, "title": "Authors: Fan Hu, Dongqi Wang, Huazhen Huang, Yishen Hu, Peng Yin\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 72, "label": "Genome-wide Nucleotide-resolution Model of Single-strand Break Site Reveals Species Evolutionary Hierarchy", "shape": "dot", "size": 10, "title": "Authors: Sheng Xu, Junkang Wei, Yu Li\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Single-strand breaks (SSBs) are the major DNA damage in the genome arising spontaneously as\nthe outcome of genotoxins and intermediates of DNA transactions. SSBs play a crucial role in\nvarious biological processes and show a non-random distribution in the genome. Several SSB\ndetection approaches such as S1 END-seq and SSiNGLe-ILM emerged to characterize the genomic\nlandscape of SSB with nucleotide resolution. However, these sequencing-based methods are costly\nand unfeasible for large-scale analysis of diverse species. Thus, we proposed the \ufb01rst computational\napproach, SSBlazer, which is an explainable and scalable deep learning framework for genome-wide\nnucleotide-resolution SSB site prediction. We demonstrated that SSBlazer can accurately predict\nSSB sites and suf\ufb01ciently alleviate false positives by constructing an imbalanced dataset to simulate\nthe realistic SSB distribution. The model interpretation analysis reveals that SSBlazer captures\nthe pattern of individual CpG in genomic context and the motif of TGCC in the center region as\ncritical features. Besides, SSBlazer is a lightweight model with robust cross-species generalization\nability in the cross-species evaluation, which enables the large-scale genome-wide application in\ndiverse species. Strikingly, the putative SSB genomic landscapes of 216 vertebrates reveal a negative\ncorrelation between SSB frequency and evolutionary hierarchy, suggesting that the genome tends to\nbe integrity during evolution."}, {"group": "History of Science", "id": 77, "label": "Prognostic Significance of Tumor-Infiltrating Lymphocytes Determined Using Deep Learning on Colorectal Cancer Pathology Images", "shape": "dot", "size": 10, "title": "Authors: Anran Liu, Xingyu Li, Hgyi Wu, Bangwei Guo, Jitendra Jnagaddala, Hg Zhang, Xu Steven Xu\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Nothing was found"}, {"group": "History of Science", "id": 79, "label": "Microsoft Word - Paper-Zahra-InfiltrationArxiv.docx", "shape": "dot", "size": 10, "title": "Authors: Drew Parker,, Hamed Akbari,, Spyridon Bakas,,, Ronald L. Wolf, Steven, Brem, Ragini Verma,, Zahra Riahi Samani\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: In malignant primary brain tumors, cancer cells infiltrate into the peritumoral brain structures which \nresults in inevitable recurrence. Quantitative assessment of infiltrative heterogeneity in the peritumoral \nregion, the area where biopsy or resection can be hazardous, is important for clinical decision making. \nPrevious work on characterizing the infiltrative heterogeneity in the peritumoral region used various \nimaging modalities, but information of extracellular free water movement restriction has been limitedly \nexplored. Here, we derive a unique set of Artificial Intelligence (AI)-based markers capturing the \nheterogeneity of tumor infiltration, by characterizing free water movement restriction in the peritumoral \nregion using Diffusion Tensor Imaging (DTI)-based free water volume fraction maps. A novel voxel-wise \ndeep learning-based peritumoral microenvironment index (PMI) is first extracted by leveraging the widely \ndifferent water diffusivity properties of glioblastomas and brain metastases as regions with and without \ninfiltrations in the peritumoral tissue. Descriptive characteristics of locoregional hubs of uniformly high \nPMI values are extracted as AI-based markers to capture distinct aspects of infiltrative heterogeneity. The \nproposed markers are applied to two clinical use cases on an independent population of 275 adult-type \ndiffuse gliomas (CNS WHO grade 4), analyzing the duration of survival among Isocitrate-Dehydrogenase \n1 (IDH1)-wildtypes and the differences with IDH1-mutants. Our findings provide a panel of markers as \nsurrogates of infiltration that captures unique insight about underlying biology of peritumoral \nmicrostructural heterogeneity, establishing them as biomarkers of prognosis pertaining to survival and \nmolecular stratification, with potential applicability in clinical decision making."}, {"group": "History of Science", "id": 136, "label": "Deep Learning models for benign and malign Ocular Tumor Growth Estimation", "shape": "dot", "size": 10, "title": "Authors: \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Relatively abundant availability of medical imaging data has provided significant support in the \ndevelopment and testing of Neural Network based image processing methods. Clinicians often face issues \nin selecting suitable image processing algorithm for medical imaging data. A strategy for the selection of a \nproper model is presented here. The training data set comprises optical coherence tomography (OCT) and \nangiography (OCT-A) images of 50 mice eyes with more than 100 days follow-up. The data contains \nimages from treated and untreated mouse eyes. \nFour deep learning variants are tested for automatic (a) differentiation of tumor region with healthy retinal \nlayer and (b) segmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of deep learning \nmodels is performed with respect to the number of training and testing images using 8 eight performance \nindices to study accuracy, reliability/reproducibility, and speed. U-net with UVgg16 is best for malign \ntumor data set with treatment (having considerable variation) and U-net with Inception backbone for benign \ntumor data (with minor variation). Loss value and root mean square error (R.M.S.E.) are found most and \nleast sensitive performance indices, respectively. The performance (via indices) is found to be exponentially \nimproving regarding a number of training images. The segmented OCT-Angiography data shows that \nneovascularization drives the tumor volume. Image analysis shows that photodynamic imaging-assisted \ntumor treatment protocol is transforming an aggressively growing tumor into a cyst. \nAn empirical expression is obtained to help medical professionals to choose a particular model given the \nnumber of images and types of characteristics.  \nWe recommend that the presented exercise should be taken as standard practice before employing a \nparticular deep learning model for biomedical image analysis. \n \nKeywords: Deep CNN, OCT Imaging, OCT \nAngiography, Image Segmentation, Cancer \ngrowth. \n1."}, {"group": "History of Science", "id": 83, "label": "Autonomous Monitoring of Pharmaceutical R\u0026D Laboratories with 6 Axis Arm Equipped Quadruped Robot and Generative AI: A Preliminary Study", "shape": "dot", "size": 10, "title": "Authors: Shunichi Hato, Nozomi Ogawa\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014This paper presents a proof-of-concept study that\nexamines the utilization of generative AI and mobile robotics for\nautonomous laboratory monitoring in the pharmaceutical R\u0026D\nlaboratory. The study investigates the potential advantages of\nanomaly detection and automated reporting by multi-modal model\nand Vision Foundation Model (VFM), which have the potential\nto enhance compliance and safety in laboratory environments.\nAdditionally, the paper discusses the current limitations of the\ngenerative AI approach and proposes future directions for its\napplication in lab monitoring.\nIndex Terms\u2014Quadruped, Mobile Robotics, Autonomous In-\nspection, Laboratory Automation\nI."}, {"group": "History of Science", "id": 89, "label": "Stimulate the Potential of Robots via Competition", "shape": "dot", "size": 10, "title": "Authors: Kangyao Huang, Di Guo, Xinyu Zhang, Xiangyang Ji, Huaping Liu\u2020\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: \u2014 It is common for us to feel pressure in a competi-\ntion environment, which arises from the desire to obtain success\ncomparing with other individuals or opponents. Although we\nmight get anxious under the pressure, it could also be a drive\nfor us to stimulate our potentials to the best in order to keep\nup with others. Inspired by this, we propose a competitive\nlearning framework which is able to help individual robot to\nacquire knowledge from the competition, fully stimulating its\ndynamics potential in the race. Specifically, the competition\ninformation among competitors is introduced as the additional\nauxiliary signal to learn advantaged actions. We further build\na Multiagent-Race environment, and extensive experiments are\nconducted, demonstrating that robots trained in competitive\nenvironments outperform ones that are trained with SoTA\nalgorithms in single robot environment.\nI."}, {"group": "History of Science", "id": 141, "label": "Water penetrating radar (WPR) in archaeology: A crannog case study", "shape": "dot", "size": 10, "title": "Authors: lastair Ruffell\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: WPR has had limited use in freshwater archaeology and can provide superb 2D data over sites with appropriate \nwater type and depth, such as exists in this case study at Castlewellan Lake, Northern Ireland. Here, a submerged \ncrannog (Prehistoric, usually Bronze Age to Medieval human structure in water) is only shown on one historic \nmap, yet is not presently visible nor on other historic/recent maps. This work used a desktop study, sonar and \nWPR to characterise the submerged structure as a crannog. The asymmetry, bathymetric position, upper surface \n(rock slabs) and general topography of the crannog are determined, yet the variable makeup of its interior and \nsides allow numerous theories and possibilities for further, probably non-invasive investigation. The limits, \nproblems, data processing and uses of WPR for archaeology in this environment are described, some of which \nmay be useful in other studies of freshwater archaeology, including crannogs, flooded dwellings, walkways/ \npiers/jetties and military archaeology.   \n1."}, {"group": "History of Science", "id": 142, "label": "Who\u2019s eating pork? Investigating pig breeding and consumption in Byzantine, Islamic and Norman/Aragonese Sicily (7th-14th c. AD)", "shape": "dot", "size": 10, "title": "Authors: Veronica Aniceti, Umberto Albarella\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: This paper investigates the culture of pork consumption in Sicily by examining a number of archaeological faunal \nassemblages dated to chronological phases spanning from the Byzantine to the Norman/Aragonese period (7th- \n14th c. AD). Zooarchaeological analyses reveal substantial diachronic changes in the use of the main domesti\u00ad\ncates, particularly concerning pig frequencies. In the Islamic period (9th-11th c. AD), pig is poorly represented at \nurban sites; this is likely to reflect a socio-cultural acceptance of the Islamic religious precepts forbidding pork \nconsumption. By contrast, and in continuity with the Roman and Byzantine periods, pigs are well-represented in \nrural settlements, thus indicating a more resilient attitude of these communities toward newly imported religious \ntraditions. In the later Norman/Aragonese period, the frequency of pig increases at some urban sites, reflecting \nthe fact that that pork prohibition had been lifted and that new food production and consumption practices were \ndeveloped. Pig continues, however, to be almost absent at a number of urban sites and castles/fortified villages; \nthis may suggest the persistence of Islamised communities in Sicily after the end of Islamic rule.   \n1."}, {"group": "History of Science", "id": 131, "label": "The morphological variability of Maltese \u2018cart ruts\u2019 and its implications", "shape": "dot", "size": 10, "title": "Authors: Huw S. Groucutt\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Hundreds of \u2018cart ruts\u2019 \u2013 pairs of incised parallel grooves in the bedrock \u2013 are found across the Maltese Archi\u00ad\npelago in the central Mediterranean. The age, functional association, formation processes, and taphonomic \nalteration of these ruts, which occur here with a globally unrivalled frequency, has been much debated. \nGenerally seen as being created by erosion from vehicles such as wheeled carts, or alternatively being cut into the \nrock to facilitate movement of such vehicles, specific models range from the use of carts to move soil in the \nNeolithic to them reflecting classical era stone quarrying, and many other possibilities. One interesting aspect \nconcerns the morphological variability of the cart ruts, such as the notion that they have a standard gauge (width \nbetween ruts), and that this gauge is very similar to that of modern railway tracks. Evaluating the morphological \nvariability of the cart ruts contributes to an understanding of the phenomenon, as, for instance, we might expect \nthat if they date to different periods, with different functions, and/or were extensively modified by geomor\u00ad\nphological processes this will be reflected in the character of their morphological variability. The analysis sug\u00ad\ngests that cart ruts are fairly standardised in terms of basic measurements such as widths and depth, perhaps \nsuggesting that they are of a consistent age and function. This study identified a need for definitional clarity as \nthe commonly cited gauge measurements are not taken in the same way as gauge is defined for railway tracks. \nThere are hints of rut shape changes reflecting extensive use and or processes such as limestone dissolution, \nwhich give insights into their formation histories.   \n1."}, {"group": "History of Science", "id": 133, "label": "The search for a needle in a haystack \u2013 New studies on plant use during the Mesolithic in southwest Central Europe", "shape": "dot", "size": 10, "title": "Authors: Stefnie Jcomet, Ptrici Vndorpe\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The existence of Mesolithic agriculture is a subject of debate in the archaeobotanist community. So far, reliable \nand AMS-dated on-site evidence of cereal macro remains are lacking to support the hypothesis. Archaeobotanical \nanalysis of two rock shelters with Mesolithic occupation layers in NE-France and SW-Switzerland, namely Lutter, \nAbri St. Joseph (FR) and Arconciel, La Souche (CH), revealed the presence of cereal remains within the Meso\u00ad\nlithic deposits. They gave rise to a possible answer to the question, however direct dating of the individual cereal \ngrains revealed their intrusive nature in the older deposits. The main aim of this paper is to place the newly \nacquired data from both sites in the broader framework of archaeobotanical research on Mesolithic sites in \nCentral Europe with a special focus on methodological and taphonomic issues often encountered at such sites.   \n1."}, {"group": "History of Science", "id": 134, "label": "Three-colored Sancai glazed ceramics excavated from Bohai sites in Primorye (Russia)", "shape": "dot", "size": 10, "title": "Authors: Buravlev, E.G. Lapo, V.A. Pimenov, A.V. Martynenko,  (Balhae) , \u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: This paper presents the analytical results of polychrome and monochrome lead-glazed ceramic classified as \nSancai that was excavated from the Bohai sites of Kraskino and Gorbatka walled towns, and the Chernyatino-2 \nsettlement (Bohai layer) (Primorye, Russia). Thirty ceramic fragments were analyzed using non-destructive X-ray \nfluorescence analysis (pXRF), optical microscopy, and scanning electron microscopy (SEM) with energy- \ndispersive x-ray spectroscopy (EDS). This work presents new data on the elemental composition of lead glazes \nand discusses some aspects of their technology, including assumptions about the medieval potters\u2019 skill level and \ntechnological awareness. Data pertaining to the sample\u2019s elemental composition indicates their origin from \nvarious workshops practicing different approaches to pottery production. These ceramics have no slip layer with \nthe glaze coating having been applied directly onto the surface of the ceramic vessels. The intensity of the lead \ncomponent in the glaze composition varies significantly, in the range of concentration between 36 and 79 wt%. \nSurface features of filling and shine, as well as the nature of glaze defects demonstrates a low probability of the \npeak firing approach that was applied in the technological process. The results presented in this paper contribute \nto expanding our understanding of the Bohai (CE 698\u2013926) and Tang (CE 618\u2013907) periods for Sancai ceramics.   \n1."}, {"group": "History of Science", "id": 139, "label": "Uniform in diversity: Typological and technological analysis of Bronze Age fine ware from Kakucs-Turj\u00e1n", "shape": "dot", "size": 10, "title": "Authors: Attila Kreiter c, Gabriella Kulcs\u00b4ar d\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The ceramic variability of fine ware pottery in Early and Middle Bronze Age Carpathian Basin (2600/ \n2500\u20131500/1450 BCE) has been used as an argument for the existence of distinct, large-scale communities. The \nrecent technological studies of coarse ware ceramics have shown that different stylistic groups are characterized \nby uniform technological traits, suggesting that current narratives result from an a priori classification of material \nculture. To verify whether fine ware ceramics were characterized by similar traits, a sample of 33 vessels from a \nmulti-layered, fortified settlement of Kakucs-Turj\u00b4an (Hungary) was analyzed in terms of vessel forms and un\u00ad\nderlying production processes. The analyses compared the classical local and non-local provenance objects to \ndetermine whether the impressionistic categorization of vessels is supported by discrete parameters. The results \nindicate that similarly to coarse ware, Early and Middle Bronze Age ceramics in the Carpathian Basin was \ncharacterized by uniform production processes suggesting the impact of multi-directional and persisting inter\u00ad\naction across the Carpathian Basin.   \n1."}, {"group": "History of Science", "id": 140, "label": "Velika Pe\u0107ina: Zooarchaeology, taphonomy and technology of a LGM Upper Paleolithic site in the central Balkans (Serbia)", "shape": "dot", "size": 10, "title": "Authors:  Pe\u00b4c, Mry C. Ster, Vesn Dimitrijevi\u00b4c b, Dusn Mihilovi\u00b4c b, Steven L. Kuhn\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The Last Glacial Maximum (MIS 2) was a period of rapid and extreme environmental change, prompting \nremarkable human adaptive responses across the world. While large parts of the temperate zone were unin\u00ad\nhabitable during this interval, other areas served as refugia for plant, animal and human populations. The \nBalkans region is identified as a biotic refugium zone for many species during high glacial intervals, but until \nrecently there has been little archaeological evidence that it was also a refugium for Paleolithic human pop\u00ad\nulations. This paper reports archaeological findings from the cave site of Velika Pe\u00b4cina (\u02c7Zagubica, Serbia), which \npreserves evidence for multiple episodes of human presence between 24,000 and 20,500 cal BP. Species com\u00ad\nbinations within the macro- and microfaunas attest to a mosaic environment with high habitat and species di\u00ad\nversity over relatively small areas in the periods leading into, during, and following the LGM. Humans hunted a \nwide spectrum of large mammals, birds, and small carnivores while occupying Velika Pe\u00b4cina. Other mammalian \nand avian carnivores used the cave when humans were not present. Lithic and osseous artifact assemblages \nindicate that a range of manufacturing activities took place in the cave, despite the relatively low density of \noccupational debris. The late Upper Paleolithic groups used the cave as a residential base during a series of brief, \nprobably seasonal visits. Human populations were small in the study area, but their presence was not diminished \nduring episodes of extreme variation in global climate.   \n1."}, {"group": "History of Science", "id": 135, "label": "IMAGING DYNAMICS BENEATH TURBID MEDIA VIA PARALLELIZED SINGLE-PHOTON DETECTION", "shape": "dot", "size": 10, "title": "Authors: Shiqi Xu, Xi Yang, Wenhui Liu,, Joakim Jonsson, Ruobing Qian, Pavan Chandra Konda, Kevin C. Zhou, Lucas, Kreiss, Qionghai Dai, Haoqian Wang, Edouard Berrocal, Roarke Horstmeyer,,\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: Noninvasive optical imaging through dynamic scattering media has numerous important biomedical\napplications but still remains a challenging task. While standard diffuse imaging methods measure\noptical absorption or \ufb02uorescent emission, it is also well-established that the temporal correlation of\nscattered coherent light diffuses through tissue much like optical intensity. Few works to date, however,\nhave aimed to experimentally measure and process such temporal correlation data to demonstrate\ndeep-tissue video reconstruction of decorrelation dynamics. In this work, we utilize a single-photon\navalanche diode (SPAD) array camera to simultaneously monitor the temporal dynamics of speckle\n\ufb02uctuations at the single-photon level from 12 different phantom tissue surface locations delivered\nvia a customized \ufb01ber bundle array. We then apply a deep neural network to convert the acquired\nsingle-photon measurements into video of scattering dynamics beneath rapidly decorrelating tissue\nphantoms. We demonstrate the ability to reconstruct images of transient (0.1-0.4s) dynamic events\noccurring up to 8 mm beneath a decorrelating tissue phantom with millimeter-scale resolution, and\nhighlight how our model can \ufb02exibly extend to monitor \ufb02ow speed within buried phantom vessels."}, {"group": "History of Science", "id": 75, "label": "Revised calculation of the coefficient of parentage in plant breeding", "shape": "dot", "size": 10, "title": "Authors: Carlos Hernandez-Suareza,\u2217\u003cbr\u003eYear: 2023\u003cbr\u003eAbstract: The Coe\ufb03cient of Parentage (COP) between two individuals is the expected\ninbreeding of their o\ufb00spring. Originally exploited by animal breeders, is now a\nroutine calculation among plant breeders as part of crop improvement programs.\nHere we show that the COP between strains requires a di\ufb00erent calculation than\nthe used to calculate the COP between individuals. Failure to do so may result\nin an overestimation of the amount of inbreeding. Here we provide a simple\nmethodology to calculate the correct coe\ufb03cient of parentage between strains.\nKeywords:\nCoe\ufb03cient of parentage, Coe\ufb03cient of inbreeding, Coe\ufb03cient of\nkinship, Coe\ufb03cient of coancestry, COP\n1."}]);
                  edges = new vis.DataSet([{"from": 0, "to": 2, "width": 0.7005070447921753}, {"from": 0, "to": 14, "width": 0.7656683921813965}, {"from": 0, "to": 22, "width": 0.7500103712081909}, {"from": 0, "to": 25, "width": 0.7601374387741089}, {"from": 0, "to": 46, "width": 0.7241954207420349}, {"from": 0, "to": 48, "width": 0.7182976007461548}, {"from": 0, "to": 51, "width": 0.7129238247871399}, {"from": 0, "to": 53, "width": 0.7328237295150757}, {"from": 0, "to": 54, "width": 0.7078771591186523}, {"from": 0, "to": 57, "width": 0.7159292697906494}, {"from": 0, "to": 62, "width": 0.7894225120544434}, {"from": 0, "to": 94, "width": 0.790229082107544}, {"from": 0, "to": 97, "width": 0.727338433265686}, {"from": 0, "to": 98, "width": 0.7713004350662231}, {"from": 0, "to": 107, "width": 0.8551312685012817}, {"from": 0, "to": 109, "width": 0.7361226081848145}, {"from": 0, "to": 111, "width": 0.7117290496826172}, {"from": 0, "to": 116, "width": 0.7011836767196655}, {"from": 0, "to": 117, "width": 0.7048107385635376}, {"from": 0, "to": 125, "width": 0.8473817110061646}, {"from": 0, "to": 127, "width": 0.7549234628677368}, {"from": 0, "to": 128, "width": 0.8613513708114624}, {"from": 1, "to": 3, "width": 0.7115730047225952}, {"from": 1, "to": 4, "width": 0.7138692140579224}, {"from": 1, "to": 56, "width": 0.7588350772857666}, {"from": 1, "to": 85, "width": 0.7224966883659363}, {"from": 1, "to": 119, "width": 0.704635500907898}, {"from": 1, "to": 120, "width": 0.7926202416419983}, {"from": 1, "to": 121, "width": 0.7373119592666626}, {"from": 2, "to": 3, "width": 0.7128222584724426}, {"from": 2, "to": 4, "width": 0.7190906405448914}, {"from": 2, "to": 5, "width": 0.7014995217323303}, {"from": 2, "to": 16, "width": 0.7113189697265625}, {"from": 2, "to": 26, "width": 0.700336754322052}, {"from": 2, "to": 44, "width": 0.7053134441375732}, {"from": 2, "to": 46, "width": 0.7195660471916199}, {"from": 2, "to": 52, "width": 0.7496715784072876}, {"from": 2, "to": 55, "width": 0.7400968074798584}, {"from": 2, "to": 56, "width": 0.7128645777702332}, {"from": 2, "to": 58, "width": 0.747205376625061}, {"from": 2, "to": 60, "width": 0.7201595306396484}, {"from": 2, "to": 63, "width": 0.7097615003585815}, {"from": 2, "to": 67, "width": 0.7092193365097046}, {"from": 2, "to": 85, "width": 0.705226719379425}, {"from": 2, "to": 127, "width": 0.7374018430709839}, {"from": 2, "to": 128, "width": 0.7058278918266296}, {"from": 3, "to": 4, "width": 0.7755472660064697}, {"from": 3, "to": 52, "width": 0.7204928398132324}, {"from": 3, "to": 55, "width": 0.7175453305244446}, {"from": 3, "to": 60, "width": 0.7279660701751709}, {"from": 3, "to": 62, "width": 0.7081743478775024}, {"from": 3, "to": 63, "width": 0.7060847282409668}, {"from": 3, "to": 70, "width": 0.7045652270317078}, {"from": 3, "to": 71, "width": 0.7802740335464478}, {"from": 3, "to": 73, "width": 0.7249463200569153}, {"from": 3, "to": 91, "width": 0.718287467956543}, {"from": 4, "to": 55, "width": 0.7448180317878723}, {"from": 4, "to": 56, "width": 0.7155433297157288}, {"from": 4, "to": 76, "width": 0.7237293124198914}, {"from": 5, "to": 6, "width": 0.7207870483398438}, {"from": 5, "to": 9, "width": 0.7261705994606018}, {"from": 5, "to": 14, "width": 0.7238669395446777}, {"from": 5, "to": 16, "width": 0.7462702989578247}, {"from": 5, "to": 26, "width": 0.7011455297470093}, {"from": 5, "to": 28, "width": 0.7166668176651001}, {"from": 5, "to": 47, "width": 0.7257634401321411}, {"from": 5, "to": 52, "width": 0.7178769111633301}, {"from": 5, "to": 58, "width": 0.7832194566726685}, {"from": 5, "to": 60, "width": 0.7448093295097351}, {"from": 5, "to": 63, "width": 0.72060227394104}, {"from": 5, "to": 64, "width": 0.7538884878158569}, {"from": 5, "to": 67, "width": 0.7633352279663086}, {"from": 5, "to": 68, "width": 0.7360793352127075}, {"from": 5, "to": 71, "width": 0.703722357749939}, {"from": 5, "to": 73, "width": 0.8288863897323608}, {"from": 5, "to": 74, "width": 0.7143709659576416}, {"from": 5, "to": 76, "width": 0.7851179242134094}, {"from": 5, "to": 78, "width": 0.7712805271148682}, {"from": 5, "to": 81, "width": 0.7074008584022522}, {"from": 5, "to": 96, "width": 0.7289050817489624}, {"from": 5, "to": 97, "width": 0.7222207188606262}, {"from": 5, "to": 99, "width": 0.7124583721160889}, {"from": 5, "to": 101, "width": 0.8075414896011353}, {"from": 5, "to": 104, "width": 0.7332797050476074}, {"from": 5, "to": 105, "width": 0.7556223273277283}, {"from": 5, "to": 108, "width": 0.7232534885406494}, {"from": 5, "to": 112, "width": 0.7236979007720947}, {"from": 5, "to": 122, "width": 0.7479240298271179}, {"from": 5, "to": 126, "width": 0.728095531463623}, {"from": 5, "to": 137, "width": 0.8715769648551941}, {"from": 6, "to": 8, "width": 0.7212448120117188}, {"from": 6, "to": 10, "width": 0.7713900208473206}, {"from": 6, "to": 11, "width": 0.7053035497665405}, {"from": 6, "to": 14, "width": 0.7168337106704712}, {"from": 6, "to": 16, "width": 0.7036150097846985}, {"from": 6, "to": 20, "width": 0.7516472935676575}, {"from": 6, "to": 21, "width": 0.7033727765083313}, {"from": 6, "to": 23, "width": 0.7209944725036621}, {"from": 6, "to": 24, "width": 0.7264865040779114}, {"from": 6, "to": 26, "width": 0.756230354309082}, {"from": 6, "to": 29, "width": 0.7457616925239563}, {"from": 6, "to": 30, "width": 0.7427605390548706}, {"from": 6, "to": 35, "width": 0.7034532427787781}, {"from": 6, "to": 37, "width": 0.7647640705108643}, {"from": 6, "to": 38, "width": 0.7139567136764526}, {"from": 6, "to": 39, "width": 0.722314715385437}, {"from": 6, "to": 40, "width": 0.7592249512672424}, {"from": 6, "to": 41, "width": 0.702201247215271}, {"from": 6, "to": 47, "width": 0.7814292311668396}, {"from": 6, "to": 51, "width": 0.7036794424057007}, {"from": 6, "to": 57, "width": 0.7056999802589417}, {"from": 6, "to": 73, "width": 0.7075566649436951}, {"from": 6, "to": 76, "width": 0.7094377279281616}, {"from": 6, "to": 81, "width": 0.7535110712051392}, {"from": 6, "to": 86, "width": 0.7153946161270142}, {"from": 6, "to": 87, "width": 0.7040237784385681}, {"from": 6, "to": 90, "width": 0.7208794355392456}, {"from": 6, "to": 91, "width": 0.767060399055481}, {"from": 6, "to": 111, "width": 0.7260429859161377}, {"from": 6, "to": 122, "width": 0.7062897682189941}, {"from": 6, "to": 137, "width": 0.714572012424469}, {"from": 7, "to": 9, "width": 0.7824301719665527}, {"from": 7, "to": 11, "width": 0.7564005851745605}, {"from": 7, "to": 12, "width": 0.7218037843704224}, {"from": 7, "to": 13, "width": 0.7277019023895264}, {"from": 7, "to": 14, "width": 0.8010493516921997}, {"from": 7, "to": 16, "width": 0.7676562070846558}, {"from": 7, "to": 17, "width": 0.8134251832962036}, {"from": 7, "to": 18, "width": 0.7553820013999939}, {"from": 7, "to": 20, "width": 0.7319670915603638}, {"from": 7, "to": 22, "width": 0.7078818082809448}, {"from": 7, "to": 24, "width": 0.7941956520080566}, {"from": 7, "to": 25, "width": 0.7298389673233032}, {"from": 7, "to": 26, "width": 0.7863892316818237}, {"from": 7, "to": 27, "width": 0.7627383470535278}, {"from": 7, "to": 28, "width": 0.7239347696304321}, {"from": 7, "to": 29, "width": 0.7665678262710571}, {"from": 7, "to": 33, "width": 0.7880319356918335}, {"from": 7, "to": 34, "width": 0.7170777320861816}, {"from": 7, "to": 35, "width": 0.7186928391456604}, {"from": 7, "to": 36, "width": 0.7130212783813477}, {"from": 7, "to": 39, "width": 0.8062121868133545}, {"from": 7, "to": 41, "width": 0.7581733465194702}, {"from": 7, "to": 46, "width": 0.7038884162902832}, {"from": 7, "to": 51, "width": 0.7066386938095093}, {"from": 7, "to": 92, "width": 0.7381773591041565}, {"from": 7, "to": 106, "width": 0.7345713376998901}, {"from": 7, "to": 107, "width": 0.7670900821685791}, {"from": 7, "to": 108, "width": 0.7223641872406006}, {"from": 7, "to": 109, "width": 0.7563246488571167}, {"from": 7, "to": 118, "width": 0.8240016102790833}, {"from": 7, "to": 126, "width": 0.7214686870574951}, {"from": 7, "to": 128, "width": 0.7034142017364502}, {"from": 7, "to": 130, "width": 0.7880319356918335}, {"from": 7, "to": 137, "width": 0.7085524201393127}, {"from": 8, "to": 9, "width": 0.741937518119812}, {"from": 8, "to": 10, "width": 0.7316457033157349}, {"from": 8, "to": 11, "width": 0.7727239727973938}, {"from": 8, "to": 12, "width": 0.723707914352417}, {"from": 8, "to": 13, "width": 0.839687705039978}, {"from": 8, "to": 14, "width": 0.7958500981330872}, {"from": 8, "to": 16, "width": 0.8220951557159424}, {"from": 8, "to": 17, "width": 0.7141262292861938}, {"from": 8, "to": 20, "width": 0.7538357973098755}, {"from": 8, "to": 21, "width": 0.8263856172561646}, {"from": 8, "to": 23, "width": 0.8017693758010864}, {"from": 8, "to": 24, "width": 0.7863483428955078}, {"from": 8, "to": 27, "width": 0.7497707605361938}, {"from": 8, "to": 30, "width": 0.7328680753707886}, {"from": 8, "to": 32, "width": 0.7820318937301636}, {"from": 8, "to": 33, "width": 0.7549055814743042}, {"from": 8, "to": 35, "width": 0.7464873194694519}, {"from": 8, "to": 36, "width": 0.7369365692138672}, {"from": 8, "to": 37, "width": 0.7896233201026917}, {"from": 8, "to": 39, "width": 0.8079659938812256}, {"from": 8, "to": 40, "width": 0.7943710088729858}, {"from": 8, "to": 41, "width": 0.8190602660179138}, {"from": 8, "to": 48, "width": 0.7089495658874512}, {"from": 8, "to": 51, "width": 0.721956193447113}, {"from": 8, "to": 57, "width": 0.7667569518089294}, {"from": 8, "to": 76, "width": 0.7240335941314697}, {"from": 8, "to": 92, "width": 0.7085517644882202}, {"from": 8, "to": 101, "width": 0.7191782593727112}, {"from": 8, "to": 104, "width": 0.7099974751472473}, {"from": 8, "to": 108, "width": 0.7088452577590942}, {"from": 8, "to": 117, "width": 0.7457253336906433}, {"from": 8, "to": 130, "width": 0.7549055814743042}, {"from": 8, "to": 137, "width": 0.7076736688613892}, {"from": 9, "to": 11, "width": 0.8086326122283936}, {"from": 9, "to": 12, "width": 0.7324328422546387}, {"from": 9, "to": 13, "width": 0.7092750668525696}, {"from": 9, "to": 14, "width": 0.8174556493759155}, {"from": 9, "to": 15, "width": 0.7149839997291565}, {"from": 9, "to": 16, "width": 0.7554581165313721}, {"from": 9, "to": 17, "width": 0.8177513480186462}, {"from": 9, "to": 18, "width": 0.7339906692504883}, {"from": 9, "to": 20, "width": 0.75288987159729}, {"from": 9, "to": 21, "width": 0.7084529995918274}, {"from": 9, "to": 22, "width": 0.706680417060852}, {"from": 9, "to": 24, "width": 0.7511553764343262}, {"from": 9, "to": 25, "width": 0.7013369202613831}, {"from": 9, "to": 26, "width": 0.7268657684326172}, {"from": 9, "to": 27, "width": 0.7782862782478333}, {"from": 9, "to": 29, "width": 0.7358975410461426}, {"from": 9, "to": 32, "width": 0.7089006900787354}, {"from": 9, "to": 33, "width": 0.7971483469009399}, {"from": 9, "to": 34, "width": 0.7680120468139648}, {"from": 9, "to": 35, "width": 0.7582164406776428}, {"from": 9, "to": 36, "width": 0.7352057695388794}, {"from": 9, "to": 38, "width": 0.7092440724372864}, {"from": 9, "to": 39, "width": 0.7927674651145935}, {"from": 9, "to": 41, "width": 0.7750102877616882}, {"from": 9, "to": 51, "width": 0.7469815015792847}, {"from": 9, "to": 60, "width": 0.7095065116882324}, {"from": 9, "to": 62, "width": 0.7077538371086121}, {"from": 9, "to": 73, "width": 0.7029353976249695}, {"from": 9, "to": 76, "width": 0.7133100628852844}, {"from": 9, "to": 92, "width": 0.7039484977722168}, {"from": 9, "to": 94, "width": 0.7414829134941101}, {"from": 9, "to": 97, "width": 0.7151567935943604}, {"from": 9, "to": 98, "width": 0.7257211208343506}, {"from": 9, "to": 99, "width": 0.7165654301643372}, {"from": 9, "to": 101, "width": 0.7567483186721802}, {"from": 9, "to": 102, "width": 0.7726874351501465}, {"from": 9, "to": 104, "width": 0.7435384392738342}, {"from": 9, "to": 105, "width": 0.7637956738471985}, {"from": 9, "to": 106, "width": 0.7045544385910034}, {"from": 9, "to": 107, "width": 0.7731949090957642}, {"from": 9, "to": 108, "width": 0.7508379817008972}, {"from": 9, "to": 109, "width": 0.7032395601272583}, {"from": 9, "to": 113, "width": 0.7073460817337036}, {"from": 9, "to": 116, "width": 0.729206919670105}, {"from": 9, "to": 117, "width": 0.7781555652618408}, {"from": 9, "to": 125, "width": 0.7248000502586365}, {"from": 9, "to": 126, "width": 0.728467583656311}, {"from": 9, "to": 130, "width": 0.7971483469009399}, {"from": 9, "to": 137, "width": 0.7646549344062805}, {"from": 10, "to": 14, "width": 0.7093130946159363}, {"from": 10, "to": 20, "width": 0.7554994821548462}, {"from": 10, "to": 21, "width": 0.7230967283248901}, {"from": 10, "to": 23, "width": 0.7633470296859741}, {"from": 10, "to": 24, "width": 0.7441942691802979}, {"from": 10, "to": 26, "width": 0.7098532915115356}, {"from": 10, "to": 30, "width": 0.7955590486526489}, {"from": 10, "to": 31, "width": 0.7528744339942932}, {"from": 10, "to": 36, "width": 0.7104085683822632}, {"from": 10, "to": 40, "width": 0.8305255174636841}, {"from": 10, "to": 41, "width": 0.7200958728790283}, {"from": 10, "to": 61, "width": 0.7083622217178345}, {"from": 11, "to": 12, "width": 0.7587212324142456}, {"from": 11, "to": 13, "width": 0.7179704904556274}, {"from": 11, "to": 14, "width": 0.8100261092185974}, {"from": 11, "to": 16, "width": 0.8205222487449646}, {"from": 11, "to": 17, "width": 0.7548440098762512}, {"from": 11, "to": 18, "width": 0.7202401161193848}, {"from": 11, "to": 20, "width": 0.8197950720787048}, {"from": 11, "to": 21, "width": 0.7361201047897339}, {"from": 11, "to": 23, "width": 0.8147492408752441}, {"from": 11, "to": 24, "width": 0.7651180028915405}, {"from": 11, "to": 26, "width": 0.7411131858825684}, {"from": 11, "to": 27, "width": 0.7883572578430176}, {"from": 11, "to": 29, "width": 0.7648031115531921}, {"from": 11, "to": 32, "width": 0.7216235399246216}, {"from": 11, "to": 33, "width": 0.7634176015853882}, {"from": 11, "to": 34, "width": 0.7414551973342896}, {"from": 11, "to": 35, "width": 0.749275803565979}, {"from": 11, "to": 36, "width": 0.7930260300636292}, {"from": 11, "to": 37, "width": 0.7654548287391663}, {"from": 11, "to": 38, "width": 0.7313724160194397}, {"from": 11, "to": 39, "width": 0.77801114320755}, {"from": 11, "to": 40, "width": 0.7529011368751526}, {"from": 11, "to": 41, "width": 0.8075565695762634}, {"from": 11, "to": 51, "width": 0.7848169803619385}, {"from": 11, "to": 53, "width": 0.7847195267677307}, {"from": 11, "to": 70, "width": 0.7268902063369751}, {"from": 11, "to": 117, "width": 0.7235410213470459}, {"from": 11, "to": 130, "width": 0.7634176015853882}, {"from": 12, "to": 13, "width": 0.7039941549301147}, {"from": 12, "to": 14, "width": 0.7917194962501526}, {"from": 12, "to": 16, "width": 0.812157154083252}, {"from": 12, "to": 17, "width": 0.7966991066932678}, {"from": 12, "to": 18, "width": 0.7870206832885742}, {"from": 12, "to": 20, "width": 0.7531975507736206}, {"from": 12, "to": 21, "width": 0.7033340930938721}, {"from": 12, "to": 23, "width": 0.7744835615158081}, {"from": 12, "to": 24, "width": 0.7661412358283997}, {"from": 12, "to": 26, "width": 0.7051122188568115}, {"from": 12, "to": 27, "width": 0.7314385175704956}, {"from": 12, "to": 29, "width": 0.772545337677002}, {"from": 12, "to": 33, "width": 0.7051004767417908}, {"from": 12, "to": 34, "width": 0.7531927824020386}, {"from": 12, "to": 35, "width": 0.7431861162185669}, {"from": 12, "to": 37, "width": 0.7441596984863281}, {"from": 12, "to": 39, "width": 0.7040073871612549}, {"from": 12, "to": 41, "width": 0.7154309749603271}, {"from": 12, "to": 51, "width": 0.8030725717544556}, {"from": 12, "to": 53, "width": 0.7363636493682861}, {"from": 12, "to": 63, "width": 0.7347662448883057}, {"from": 12, "to": 101, "width": 0.7067143321037292}, {"from": 12, "to": 104, "width": 0.730355978012085}, {"from": 12, "to": 111, "width": 0.7001317739486694}, {"from": 12, "to": 130, "width": 0.7051004767417908}, {"from": 12, "to": 137, "width": 0.7034064531326294}, {"from": 13, "to": 14, "width": 0.7693630456924438}, {"from": 13, "to": 15, "width": 0.7444485425949097}, {"from": 13, "to": 16, "width": 0.8076773881912231}, {"from": 13, "to": 19, "width": 0.740449845790863}, {"from": 13, "to": 20, "width": 0.7099161148071289}, {"from": 13, "to": 21, "width": 0.703892707824707}, {"from": 13, "to": 23, "width": 0.7804725766181946}, {"from": 13, "to": 24, "width": 0.7888586521148682}, {"from": 13, "to": 27, "width": 0.7606068849563599}, {"from": 13, "to": 28, "width": 0.7560504078865051}, {"from": 13, "to": 30, "width": 0.7158529758453369}, {"from": 13, "to": 33, "width": 0.7673055529594421}, {"from": 13, "to": 35, "width": 0.7381583452224731}, {"from": 13, "to": 37, "width": 0.7492882609367371}, {"from": 13, "to": 39, "width": 0.8211342096328735}, {"from": 13, "to": 41, "width": 0.7685737609863281}, {"from": 13, "to": 92, "width": 0.7344056367874146}, {"from": 13, "to": 108, "width": 0.7072964906692505}, {"from": 13, "to": 110, "width": 0.7010217308998108}, {"from": 13, "to": 126, "width": 0.725543200969696}, {"from": 13, "to": 130, "width": 0.7673055529594421}, {"from": 13, "to": 137, "width": 0.7098575830459595}, {"from": 14, "to": 15, "width": 0.7500277757644653}, {"from": 14, "to": 16, "width": 0.7985237836837769}, {"from": 14, "to": 17, "width": 0.7949515581130981}, {"from": 14, "to": 18, "width": 0.7693605422973633}, {"from": 14, "to": 20, "width": 0.806856095790863}, {"from": 14, "to": 21, "width": 0.7693920135498047}, {"from": 14, "to": 22, "width": 0.7785738706588745}, {"from": 14, "to": 23, "width": 0.8203597068786621}, {"from": 14, "to": 24, "width": 0.8389447927474976}, {"from": 14, "to": 25, "width": 0.7433778047561646}, {"from": 14, "to": 26, "width": 0.773078441619873}, {"from": 14, "to": 27, "width": 0.7940922975540161}, {"from": 14, "to": 28, "width": 0.7207256555557251}, {"from": 14, "to": 29, "width": 0.7642983198165894}, {"from": 14, "to": 32, "width": 0.7375521659851074}, {"from": 14, "to": 33, "width": 0.8038673400878906}, {"from": 14, "to": 34, "width": 0.7584890723228455}, {"from": 14, "to": 35, "width": 0.7895931005477905}, {"from": 14, "to": 36, "width": 0.7812485694885254}, {"from": 14, "to": 37, "width": 0.7827234268188477}, {"from": 14, "to": 38, "width": 0.705545961856842}, {"from": 14, "to": 39, "width": 0.8339873552322388}, {"from": 14, "to": 40, "width": 0.7731436491012573}, {"from": 14, "to": 41, "width": 0.8207974433898926}, {"from": 14, "to": 42, "width": 0.7207440137863159}, {"from": 14, "to": 47, "width": 0.7190291285514832}, {"from": 14, "to": 51, "width": 0.7699575424194336}, {"from": 14, "to": 53, "width": 0.7460117340087891}, {"from": 14, "to": 57, "width": 0.7044154405593872}, {"from": 14, "to": 61, "width": 0.7218362092971802}, {"from": 14, "to": 62, "width": 0.728849470615387}, {"from": 14, "to": 70, "width": 0.7345325350761414}, {"from": 14, "to": 71, "width": 0.705564558506012}, {"from": 14, "to": 73, "width": 0.713726282119751}, {"from": 14, "to": 76, "width": 0.7141819596290588}, {"from": 14, "to": 84, "width": 0.7365307807922363}, {"from": 14, "to": 91, "width": 0.702539324760437}, {"from": 14, "to": 92, "width": 0.7578310966491699}, {"from": 14, "to": 94, "width": 0.7517102956771851}, {"from": 14, "to": 97, "width": 0.7179142236709595}, {"from": 14, "to": 98, "width": 0.7460935711860657}, {"from": 14, "to": 101, "width": 0.7411388158798218}, {"from": 14, "to": 102, "width": 0.7612034678459167}, {"from": 14, "to": 104, "width": 0.7611854076385498}, {"from": 14, "to": 105, "width": 0.7045408487319946}, {"from": 14, "to": 107, "width": 0.8330807685852051}, {"from": 14, "to": 108, "width": 0.7511525750160217}, {"from": 14, "to": 109, "width": 0.7673131823539734}, {"from": 14, "to": 111, "width": 0.7012147903442383}, {"from": 14, "to": 113, "width": 0.7055795192718506}, {"from": 14, "to": 117, "width": 0.7433443665504456}, {"from": 14, "to": 118, "width": 0.7196216583251953}, {"from": 14, "to": 125, "width": 0.7711320519447327}, {"from": 14, "to": 126, "width": 0.7185033559799194}, {"from": 14, "to": 128, "width": 0.706429660320282}, {"from": 14, "to": 130, "width": 0.8038673400878906}, {"from": 14, "to": 137, "width": 0.7530331611633301}, {"from": 15, "to": 17, "width": 0.7533327341079712}, {"from": 15, "to": 19, "width": 0.7120320796966553}, {"from": 15, "to": 24, "width": 0.703200101852417}, {"from": 15, "to": 27, "width": 0.7572599649429321}, {"from": 15, "to": 28, "width": 0.8827694654464722}, {"from": 15, "to": 31, "width": 0.7026098966598511}, {"from": 15, "to": 33, "width": 0.8558905720710754}, {"from": 15, "to": 34, "width": 0.7020007371902466}, {"from": 15, "to": 35, "width": 0.7274254560470581}, {"from": 15, "to": 37, "width": 0.7173861265182495}, {"from": 15, "to": 39, "width": 0.7939051389694214}, {"from": 15, "to": 45, "width": 0.800780177116394}, {"from": 15, "to": 92, "width": 0.8487026691436768}, {"from": 15, "to": 99, "width": 0.7193163633346558}, {"from": 15, "to": 103, "width": 0.7320204973220825}, {"from": 15, "to": 104, "width": 0.7548514008522034}, {"from": 15, "to": 106, "width": 0.7467674016952515}, {"from": 15, "to": 107, "width": 0.7043454647064209}, {"from": 15, "to": 108, "width": 0.8116481900215149}, {"from": 15, "to": 109, "width": 0.7658061981201172}, {"from": 15, "to": 110, "width": 0.7997351884841919}, {"from": 15, "to": 114, "width": 0.8252699971199036}, {"from": 15, "to": 118, "width": 0.7148274779319763}, {"from": 15, "to": 126, "width": 0.8129287958145142}, {"from": 15, "to": 130, "width": 0.8558905720710754}, {"from": 16, "to": 17, "width": 0.7821803689002991}, {"from": 16, "to": 18, "width": 0.8042237162590027}, {"from": 16, "to": 20, "width": 0.776269793510437}, {"from": 16, "to": 21, "width": 0.720504641532898}, {"from": 16, "to": 23, "width": 0.8159088492393494}, {"from": 16, "to": 24, "width": 0.8146840333938599}, {"from": 16, "to": 26, "width": 0.7492773532867432}, {"from": 16, "to": 27, "width": 0.7736881971359253}, {"from": 16, "to": 29, "width": 0.7413919568061829}, {"from": 16, "to": 33, "width": 0.7396895885467529}, {"from": 16, "to": 34, "width": 0.7448389530181885}, {"from": 16, "to": 35, "width": 0.7496616840362549}, {"from": 16, "to": 36, "width": 0.7552689909934998}, {"from": 16, "to": 37, "width": 0.8282665014266968}, {"from": 16, "to": 39, "width": 0.7804564237594604}, {"from": 16, "to": 40, "width": 0.7354657649993896}, {"from": 16, "to": 41, "width": 0.8038502931594849}, {"from": 16, "to": 46, "width": 0.7043942213058472}, {"from": 16, "to": 47, "width": 0.7096092700958252}, {"from": 16, "to": 50, "width": 0.7196946740150452}, {"from": 16, "to": 51, "width": 0.8023680448532104}, {"from": 16, "to": 52, "width": 0.7339385151863098}, {"from": 16, "to": 57, "width": 0.7199057340621948}, {"from": 16, "to": 60, "width": 0.7318222522735596}, {"from": 16, "to": 63, "width": 0.7399356961250305}, {"from": 16, "to": 70, "width": 0.7327977418899536}, {"from": 16, "to": 71, "width": 0.7293624877929688}, {"from": 16, "to": 73, "width": 0.7148973345756531}, {"from": 16, "to": 76, "width": 0.7173283100128174}, {"from": 16, "to": 92, "width": 0.7200577259063721}, {"from": 16, "to": 99, "width": 0.7330560088157654}, {"from": 16, "to": 101, "width": 0.7636967897415161}, {"from": 16, "to": 102, "width": 0.719512403011322}, {"from": 16, "to": 104, "width": 0.7450990676879883}, {"from": 16, "to": 108, "width": 0.7489650249481201}, {"from": 16, "to": 117, "width": 0.7311152815818787}, {"from": 16, "to": 118, "width": 0.7242425084114075}, {"from": 16, "to": 126, "width": 0.70513516664505}, {"from": 16, "to": 130, "width": 0.7396895885467529}, {"from": 16, "to": 137, "width": 0.7636520862579346}, {"from": 17, "to": 18, "width": 0.7720142006874084}, {"from": 17, "to": 20, "width": 0.7648662328720093}, {"from": 17, "to": 22, "width": 0.7127567529678345}, {"from": 17, "to": 23, "width": 0.7345935106277466}, {"from": 17, "to": 24, "width": 0.7497940063476562}, {"from": 17, "to": 25, "width": 0.7387468218803406}, {"from": 17, "to": 26, "width": 0.7018551230430603}, {"from": 17, "to": 27, "width": 0.7811784148216248}, {"from": 17, "to": 28, "width": 0.7363529205322266}, {"from": 17, "to": 29, "width": 0.8176349401473999}, {"from": 17, "to": 33, "width": 0.8084169626235962}, {"from": 17, "to": 34, "width": 0.8230987787246704}, {"from": 17, "to": 35, "width": 0.7573865652084351}, {"from": 17, "to": 36, "width": 0.7473055124282837}, {"from": 17, "to": 37, "width": 0.7254103422164917}, {"from": 17, "to": 39, "width": 0.7828489542007446}, {"from": 17, "to": 41, "width": 0.7467318773269653}, {"from": 17, "to": 51, "width": 0.7133762836456299}, {"from": 17, "to": 92, "width": 0.7441861033439636}, {"from": 17, "to": 104, "width": 0.7213536500930786}, {"from": 17, "to": 108, "width": 0.7469275593757629}, {"from": 17, "to": 109, "width": 0.7020605802536011}, {"from": 17, "to": 117, "width": 0.7226296663284302}, {"from": 17, "to": 126, "width": 0.7268983125686646}, {"from": 17, "to": 130, "width": 0.8084169626235962}, {"from": 17, "to": 137, "width": 0.7235538363456726}, {"from": 18, "to": 19, "width": 0.7411841154098511}, {"from": 18, "to": 20, "width": 0.7233761548995972}, {"from": 18, "to": 21, "width": 0.7017538547515869}, {"from": 18, "to": 23, "width": 0.7921625375747681}, {"from": 18, "to": 24, "width": 0.8254200220108032}, {"from": 18, "to": 26, "width": 0.7182257771492004}, {"from": 18, "to": 27, "width": 0.7005598545074463}, {"from": 18, "to": 28, "width": 0.7096297740936279}, {"from": 18, "to": 33, "width": 0.776915431022644}, {"from": 18, "to": 34, "width": 0.7873731851577759}, {"from": 18, "to": 35, "width": 0.7774654030799866}, {"from": 18, "to": 39, "width": 0.7345215082168579}, {"from": 18, "to": 41, "width": 0.77007657289505}, {"from": 18, "to": 51, "width": 0.7446473240852356}, {"from": 18, "to": 53, "width": 0.7050777673721313}, {"from": 18, "to": 66, "width": 0.7088816165924072}, {"from": 18, "to": 92, "width": 0.7259031534194946}, {"from": 18, "to": 99, "width": 0.7315570116043091}, {"from": 18, "to": 101, "width": 0.7842787504196167}, {"from": 18, "to": 104, "width": 0.7833013534545898}, {"from": 18, "to": 107, "width": 0.7054111957550049}, {"from": 18, "to": 108, "width": 0.7547972202301025}, {"from": 18, "to": 109, "width": 0.711791455745697}, {"from": 18, "to": 126, "width": 0.7220399379730225}, {"from": 18, "to": 130, "width": 0.776915431022644}, {"from": 18, "to": 137, "width": 0.7372992634773254}, {"from": 19, "to": 28, "width": 0.7800450921058655}, {"from": 19, "to": 33, "width": 0.7220134735107422}, {"from": 19, "to": 45, "width": 0.7663102149963379}, {"from": 19, "to": 92, "width": 0.7096966505050659}, {"from": 19, "to": 96, "width": 0.7277437448501587}, {"from": 19, "to": 99, "width": 0.7294769883155823}, {"from": 19, "to": 101, "width": 0.7602788209915161}, {"from": 19, "to": 103, "width": 0.7164931893348694}, {"from": 19, "to": 104, "width": 0.7185378670692444}, {"from": 19, "to": 106, "width": 0.729141116142273}, {"from": 19, "to": 108, "width": 0.7578988075256348}, {"from": 19, "to": 109, "width": 0.7004125118255615}, {"from": 19, "to": 112, "width": 0.7223294377326965}, {"from": 19, "to": 114, "width": 0.7729551792144775}, {"from": 19, "to": 126, "width": 0.8276832103729248}, {"from": 19, "to": 130, "width": 0.7220134735107422}, {"from": 19, "to": 137, "width": 0.731784999370575}, {"from": 20, "to": 21, "width": 0.7894517183303833}, {"from": 20, "to": 23, "width": 0.8372209072113037}, {"from": 20, "to": 24, "width": 0.8490064144134521}, {"from": 20, "to": 26, "width": 0.7443767786026001}, {"from": 20, "to": 27, "width": 0.7662705183029175}, {"from": 20, "to": 29, "width": 0.7243116497993469}, {"from": 20, "to": 30, "width": 0.7584398984909058}, {"from": 20, "to": 32, "width": 0.7365157604217529}, {"from": 20, "to": 33, "width": 0.7545837163925171}, {"from": 20, "to": 34, "width": 0.7371554970741272}, {"from": 20, "to": 35, "width": 0.7743294835090637}, {"from": 20, "to": 36, "width": 0.8007247447967529}, {"from": 20, "to": 37, "width": 0.8054080009460449}, {"from": 20, "to": 38, "width": 0.7181435823440552}, {"from": 20, "to": 39, "width": 0.8001614809036255}, {"from": 20, "to": 40, "width": 0.8003069162368774}, {"from": 20, "to": 41, "width": 0.8069247007369995}, {"from": 20, "to": 51, "width": 0.7848918437957764}, {"from": 20, "to": 53, "width": 0.7903658151626587}, {"from": 20, "to": 57, "width": 0.7243439555168152}, {"from": 20, "to": 70, "width": 0.7257285714149475}, {"from": 20, "to": 92, "width": 0.7029728889465332}, {"from": 20, "to": 107, "width": 0.7161973714828491}, {"from": 20, "to": 117, "width": 0.7266672849655151}, {"from": 20, "to": 125, "width": 0.7208032608032227}, {"from": 20, "to": 130, "width": 0.7545837163925171}, {"from": 21, "to": 23, "width": 0.7810698747634888}, {"from": 21, "to": 24, "width": 0.7999916672706604}, {"from": 21, "to": 30, "width": 0.7000095844268799}, {"from": 21, "to": 32, "width": 0.721541166305542}, {"from": 21, "to": 35, "width": 0.7611143589019775}, {"from": 21, "to": 36, "width": 0.7143555879592896}, {"from": 21, "to": 39, "width": 0.7110833525657654}, {"from": 21, "to": 40, "width": 0.7764862775802612}, {"from": 21, "to": 41, "width": 0.8006447553634644}, {"from": 21, "to": 53, "width": 0.710320234298706}, {"from": 22, "to": 23, "width": 0.7019543647766113}, {"from": 22, "to": 25, "width": 0.8420895934104919}, {"from": 22, "to": 27, "width": 0.7019106149673462}, {"from": 22, "to": 29, "width": 0.7231832146644592}, {"from": 22, "to": 34, "width": 0.7031110525131226}, {"from": 22, "to": 53, "width": 0.7038016319274902}, {"from": 22, "to": 62, "width": 0.7406234741210938}, {"from": 22, "to": 84, "width": 0.7004778981208801}, {"from": 22, "to": 94, "width": 0.7838273048400879}, {"from": 22, "to": 98, "width": 0.7140631675720215}, {"from": 22, "to": 102, "width": 0.8317009806632996}, {"from": 22, "to": 107, "width": 0.7477693557739258}, {"from": 22, "to": 109, "width": 0.727611780166626}, {"from": 22, "to": 117, "width": 0.7734562754631042}, {"from": 22, "to": 125, "width": 0.7467001676559448}, {"from": 22, "to": 128, "width": 0.7271678447723389}, {"from": 23, "to": 24, "width": 0.8713740110397339}, {"from": 23, "to": 26, "width": 0.7181351184844971}, {"from": 23, "to": 27, "width": 0.776740312576294}, {"from": 23, "to": 29, "width": 0.7345000505447388}, {"from": 23, "to": 30, "width": 0.7331189513206482}, {"from": 23, "to": 32, "width": 0.7323161363601685}, {"from": 23, "to": 33, "width": 0.741989254951477}, {"from": 23, "to": 34, "width": 0.7304646372795105}, {"from": 23, "to": 35, "width": 0.8211981058120728}, {"from": 23, "to": 36, "width": 0.811687171459198}, {"from": 23, "to": 37, "width": 0.7858659029006958}, {"from": 23, "to": 39, "width": 0.7911030054092407}, {"from": 23, "to": 40, "width": 0.8246621489524841}, {"from": 23, "to": 41, "width": 0.8563686609268188}, {"from": 23, "to": 42, "width": 0.7004061341285706}, {"from": 23, "to": 51, "width": 0.7578585743904114}, {"from": 23, "to": 53, "width": 0.7939471006393433}, {"from": 23, "to": 57, "width": 0.705375075340271}, {"from": 23, "to": 70, "width": 0.7064053416252136}, {"from": 23, "to": 91, "width": 0.716344952583313}, {"from": 23, "to": 92, "width": 0.7113367915153503}, {"from": 23, "to": 117, "width": 0.7457684278488159}, {"from": 23, "to": 130, "width": 0.741989254951477}, {"from": 24, "to": 26, "width": 0.7572681903839111}, {"from": 24, "to": 27, "width": 0.7878319025039673}, {"from": 24, "to": 28, "width": 0.7167261242866516}, {"from": 24, "to": 29, "width": 0.709038496017456}, {"from": 24, "to": 30, "width": 0.7647783756256104}, {"from": 24, "to": 32, "width": 0.758598804473877}, {"from": 24, "to": 33, "width": 0.7928756475448608}, {"from": 24, "to": 34, "width": 0.7176827192306519}, {"from": 24, "to": 35, "width": 0.7921839952468872}, {"from": 24, "to": 36, "width": 0.7931828498840332}, {"from": 24, "to": 37, "width": 0.761247992515564}, {"from": 24, "to": 39, "width": 0.8127272129058838}, {"from": 24, "to": 40, "width": 0.7910486459732056}, {"from": 24, "to": 41, "width": 0.8896103501319885}, {"from": 24, "to": 51, "width": 0.750281572341919}, {"from": 24, "to": 53, "width": 0.7402260899543762}, {"from": 24, "to": 57, "width": 0.7261362671852112}, {"from": 24, "to": 61, "width": 0.7189293503761292}, {"from": 24, "to": 70, "width": 0.7474005818367004}, {"from": 24, "to": 92, "width": 0.7485005855560303}, {"from": 24, "to": 107, "width": 0.7470927834510803}, {"from": 24, "to": 108, "width": 0.7004989981651306}, {"from": 24, "to": 109, "width": 0.7262063026428223}, {"from": 24, "to": 117, "width": 0.7199831008911133}, {"from": 24, "to": 118, "width": 0.7254724502563477}, {"from": 24, "to": 130, "width": 0.7928756475448608}, {"from": 25, "to": 27, "width": 0.7469028234481812}, {"from": 25, "to": 29, "width": 0.7178125381469727}, {"from": 25, "to": 33, "width": 0.7523577809333801}, {"from": 25, "to": 34, "width": 0.739492654800415}, {"from": 25, "to": 39, "width": 0.7305335998535156}, {"from": 25, "to": 46, "width": 0.7316514849662781}, {"from": 25, "to": 53, "width": 0.7089517116546631}, {"from": 25, "to": 57, "width": 0.7157968878746033}, {"from": 25, "to": 62, "width": 0.749062180519104}, {"from": 25, "to": 93, "width": 0.7043721675872803}, {"from": 25, "to": 94, "width": 0.7465164661407471}, {"from": 25, "to": 98, "width": 0.7337755560874939}, {"from": 25, "to": 102, "width": 0.7209813594818115}, {"from": 25, "to": 107, "width": 0.7235212326049805}, {"from": 25, "to": 108, "width": 0.712143063545227}, {"from": 25, "to": 109, "width": 0.7616747617721558}, {"from": 25, "to": 117, "width": 0.7311801910400391}, {"from": 25, "to": 125, "width": 0.7778744697570801}, {"from": 25, "to": 128, "width": 0.762764573097229}, {"from": 25, "to": 130, "width": 0.7523577809333801}, {"from": 26, "to": 29, "width": 0.7218500971794128}, {"from": 26, "to": 30, "width": 0.7554823160171509}, {"from": 26, "to": 36, "width": 0.71949303150177}, {"from": 26, "to": 37, "width": 0.7094986438751221}, {"from": 26, "to": 38, "width": 0.745917558670044}, {"from": 26, "to": 39, "width": 0.7037116885185242}, {"from": 26, "to": 40, "width": 0.7170455455780029}, {"from": 26, "to": 41, "width": 0.765291690826416}, {"from": 26, "to": 42, "width": 0.7231661081314087}, {"from": 26, "to": 61, "width": 0.7329134941101074}, {"from": 26, "to": 64, "width": 0.708679735660553}, {"from": 26, "to": 70, "width": 0.7237869501113892}, {"from": 26, "to": 71, "width": 0.7035416960716248}, {"from": 26, "to": 81, "width": 0.7578638792037964}, {"from": 26, "to": 84, "width": 0.7054991722106934}, {"from": 26, "to": 98, "width": 0.716113269329071}, {"from": 26, "to": 101, "width": 0.7333468198776245}, {"from": 26, "to": 104, "width": 0.7155187726020813}, {"from": 26, "to": 109, "width": 0.7237651348114014}, {"from": 26, "to": 112, "width": 0.7117356657981873}, {"from": 26, "to": 126, "width": 0.7016311883926392}, {"from": 26, "to": 137, "width": 0.7385091781616211}, {"from": 27, "to": 28, "width": 0.7155090570449829}, {"from": 27, "to": 29, "width": 0.7368795871734619}, {"from": 27, "to": 32, "width": 0.7296705842018127}, {"from": 27, "to": 33, "width": 0.8201805949211121}, {"from": 27, "to": 34, "width": 0.7180981636047363}, {"from": 27, "to": 35, "width": 0.7432511448860168}, {"from": 27, "to": 36, "width": 0.7714935541152954}, {"from": 27, "to": 37, "width": 0.8075920939445496}, {"from": 27, "to": 39, "width": 0.8483361005783081}, {"from": 27, "to": 40, "width": 0.7239083647727966}, {"from": 27, "to": 41, "width": 0.7618690729141235}, {"from": 27, "to": 51, "width": 0.723699152469635}, {"from": 27, "to": 53, "width": 0.7310162782669067}, {"from": 27, "to": 70, "width": 0.7015038728713989}, {"from": 27, "to": 92, "width": 0.7306318283081055}, {"from": 27, "to": 108, "width": 0.7153022289276123}, {"from": 27, "to": 109, "width": 0.7311575412750244}, {"from": 27, "to": 118, "width": 0.7028524279594421}, {"from": 27, "to": 126, "width": 0.7035183906555176}, {"from": 27, "to": 130, "width": 0.8201805949211121}, {"from": 28, "to": 30, "width": 0.7036957740783691}, {"from": 28, "to": 31, "width": 0.7407073974609375}, {"from": 28, "to": 33, "width": 0.8075889945030212}, {"from": 28, "to": 34, "width": 0.7213273644447327}, {"from": 28, "to": 37, "width": 0.7004085183143616}, {"from": 28, "to": 39, "width": 0.7665424346923828}, {"from": 28, "to": 45, "width": 0.8185704946517944}, {"from": 28, "to": 92, "width": 0.8240429162979126}, {"from": 28, "to": 99, "width": 0.7594179511070251}, {"from": 28, "to": 101, "width": 0.7469233870506287}, {"from": 28, "to": 103, "width": 0.7575790286064148}, {"from": 28, "to": 104, "width": 0.7852850556373596}, {"from": 28, "to": 106, "width": 0.7532695531845093}, {"from": 28, "to": 108, "width": 0.811090350151062}, {"from": 28, "to": 109, "width": 0.765043318271637}, {"from": 28, "to": 110, "width": 0.8233364224433899}, {"from": 28, "to": 112, "width": 0.7302192449569702}, {"from": 28, "to": 114, "width": 0.8800221085548401}, {"from": 28, "to": 118, "width": 0.7626376748085022}, {"from": 28, "to": 124, "width": 0.7102415561676025}, {"from": 28, "to": 126, "width": 0.8872013688087463}, {"from": 28, "to": 130, "width": 0.8075889945030212}, {"from": 28, "to": 137, "width": 0.7562408447265625}, {"from": 29, "to": 33, "width": 0.7308111786842346}, {"from": 29, "to": 34, "width": 0.7666879892349243}, {"from": 29, "to": 35, "width": 0.7642414569854736}, {"from": 29, "to": 36, "width": 0.76316237449646}, {"from": 29, "to": 39, "width": 0.723983645439148}, {"from": 29, "to": 41, "width": 0.7018858194351196}, {"from": 29, "to": 46, "width": 0.7047324180603027}, {"from": 29, "to": 53, "width": 0.7395269274711609}, {"from": 29, "to": 104, "width": 0.7015920281410217}, {"from": 29, "to": 130, "width": 0.7308111786842346}, {"from": 30, "to": 31, "width": 0.7662537097930908}, {"from": 30, "to": 37, "width": 0.7024092674255371}, {"from": 30, "to": 39, "width": 0.7058293223381042}, {"from": 30, "to": 40, "width": 0.7336829900741577}, {"from": 30, "to": 61, "width": 0.7430009245872498}, {"from": 31, "to": 40, "width": 0.7344193458557129}, {"from": 31, "to": 61, "width": 0.7781165838241577}, {"from": 31, "to": 65, "width": 0.701677143573761}, {"from": 32, "to": 33, "width": 0.7457159757614136}, {"from": 32, "to": 35, "width": 0.7203941345214844}, {"from": 32, "to": 36, "width": 0.7303628325462341}, {"from": 32, "to": 39, "width": 0.8260805606842041}, {"from": 32, "to": 41, "width": 0.7277877330780029}, {"from": 32, "to": 57, "width": 0.7556284070014954}, {"from": 32, "to": 107, "width": 0.7082623243331909}, {"from": 32, "to": 130, "width": 0.7457159757614136}, {"from": 33, "to": 34, "width": 0.7843632698059082}, {"from": 33, "to": 35, "width": 0.7750949263572693}, {"from": 33, "to": 36, "width": 0.7059526443481445}, {"from": 33, "to": 37, "width": 0.7691972851753235}, {"from": 33, "to": 39, "width": 0.8537470698356628}, {"from": 33, "to": 41, "width": 0.7744739055633545}, {"from": 33, "to": 45, "width": 0.7146698236465454}, {"from": 33, "to": 92, "width": 0.8577396869659424}, {"from": 33, "to": 94, "width": 0.724089503288269}, {"from": 33, "to": 98, "width": 0.731959342956543}, {"from": 33, "to": 99, "width": 0.7357759475708008}, {"from": 33, "to": 101, "width": 0.7270970940589905}, {"from": 33, "to": 103, "width": 0.7247060537338257}, {"from": 33, "to": 104, "width": 0.7564657330513}, {"from": 33, "to": 106, "width": 0.7473931312561035}, {"from": 33, "to": 107, "width": 0.7458900213241577}, {"from": 33, "to": 108, "width": 0.8199609518051147}, {"from": 33, "to": 109, "width": 0.7706724405288696}, {"from": 33, "to": 110, "width": 0.7339702844619751}, {"from": 33, "to": 114, "width": 0.7435386776924133}, {"from": 33, "to": 117, "width": 0.7381438612937927}, {"from": 33, "to": 118, "width": 0.7591639757156372}, {"from": 33, "to": 125, "width": 0.7109418511390686}, {"from": 33, "to": 126, "width": 0.7644693851470947}, {"from": 33, "to": 128, "width": 0.7085476517677307}, {"from": 33, "to": 130, "width": 0.9999999403953552}, {"from": 34, "to": 35, "width": 0.7409730553627014}, {"from": 34, "to": 37, "width": 0.7215575575828552}, {"from": 34, "to": 39, "width": 0.7803937196731567}, {"from": 34, "to": 41, "width": 0.7275159358978271}, {"from": 34, "to": 53, "width": 0.7207347750663757}, {"from": 34, "to": 57, "width": 0.7117226123809814}, {"from": 34, "to": 92, "width": 0.7364982962608337}, {"from": 34, "to": 95, "width": 0.7038823962211609}, {"from": 34, "to": 99, "width": 0.7363744974136353}, {"from": 34, "to": 101, "width": 0.725605309009552}, {"from": 34, "to": 102, "width": 0.7178226709365845}, {"from": 34, "to": 104, "width": 0.721196174621582}, {"from": 34, "to": 106, "width": 0.7265157103538513}, {"from": 34, "to": 108, "width": 0.750363826751709}, {"from": 34, "to": 117, "width": 0.7503377795219421}, {"from": 34, "to": 130, "width": 0.7843632698059082}, {"from": 34, "to": 137, "width": 0.7309486865997314}, {"from": 35, "to": 36, "width": 0.7092491388320923}, {"from": 35, "to": 37, "width": 0.7033343315124512}, {"from": 35, "to": 39, "width": 0.7850416898727417}, {"from": 35, "to": 40, "width": 0.7249033451080322}, {"from": 35, "to": 41, "width": 0.7706677317619324}, {"from": 35, "to": 51, "width": 0.7256183624267578}, {"from": 35, "to": 53, "width": 0.7916741967201233}, {"from": 35, "to": 92, "width": 0.7001670598983765}, {"from": 35, "to": 107, "width": 0.7190695405006409}, {"from": 35, "to": 117, "width": 0.7333987951278687}, {"from": 35, "to": 130, "width": 0.7750949263572693}, {"from": 36, "to": 37, "width": 0.76368647813797}, {"from": 36, "to": 38, "width": 0.7153182625770569}, {"from": 36, "to": 39, "width": 0.719221830368042}, {"from": 36, "to": 40, "width": 0.7707943916320801}, {"from": 36, "to": 41, "width": 0.8073830604553223}, {"from": 36, "to": 51, "width": 0.7445356845855713}, {"from": 36, "to": 53, "width": 0.7259524464607239}, {"from": 36, "to": 70, "width": 0.7018516063690186}, {"from": 36, "to": 71, "width": 0.7039713859558105}, {"from": 36, "to": 130, "width": 0.7059526443481445}, {"from": 37, "to": 39, "width": 0.7840657830238342}, {"from": 37, "to": 40, "width": 0.7635947465896606}, {"from": 37, "to": 41, "width": 0.7636843919754028}, {"from": 37, "to": 42, "width": 0.7246273756027222}, {"from": 37, "to": 51, "width": 0.7553826570510864}, {"from": 37, "to": 70, "width": 0.7032469511032104}, {"from": 37, "to": 71, "width": 0.7080743312835693}, {"from": 37, "to": 84, "width": 0.7060942649841309}, {"from": 37, "to": 91, "width": 0.7220759391784668}, {"from": 37, "to": 92, "width": 0.7193785309791565}, {"from": 37, "to": 111, "width": 0.7098604440689087}, {"from": 37, "to": 117, "width": 0.704424262046814}, {"from": 37, "to": 122, "width": 0.7076138854026794}, {"from": 37, "to": 123, "width": 0.7379412055015564}, {"from": 37, "to": 125, "width": 0.7174172401428223}, {"from": 37, "to": 128, "width": 0.7009050846099854}, {"from": 37, "to": 130, "width": 0.7691972851753235}, {"from": 37, "to": 137, "width": 0.7188522815704346}, {"from": 38, "to": 42, "width": 0.755113422870636}, {"from": 38, "to": 84, "width": 0.7210618853569031}, {"from": 38, "to": 90, "width": 0.7084323167800903}, {"from": 39, "to": 40, "width": 0.7414834499359131}, {"from": 39, "to": 41, "width": 0.7675451040267944}, {"from": 39, "to": 53, "width": 0.7131122350692749}, {"from": 39, "to": 92, "width": 0.7990403175354004}, {"from": 39, "to": 99, "width": 0.7032654881477356}, {"from": 39, "to": 102, "width": 0.712950587272644}, {"from": 39, "to": 103, "width": 0.7068771123886108}, {"from": 39, "to": 106, "width": 0.7159041166305542}, {"from": 39, "to": 107, "width": 0.7721621990203857}, {"from": 39, "to": 108, "width": 0.7396770119667053}, {"from": 39, "to": 109, "width": 0.7523554563522339}, {"from": 39, "to": 114, "width": 0.7078502774238586}, {"from": 39, "to": 117, "width": 0.7189450263977051}, {"from": 39, "to": 118, "width": 0.7693045735359192}, {"from": 39, "to": 126, "width": 0.7250468730926514}, {"from": 39, "to": 130, "width": 0.8537470698356628}, {"from": 39, "to": 137, "width": 0.7004245519638062}, {"from": 40, "to": 41, "width": 0.8039836883544922}, {"from": 40, "to": 51, "width": 0.7109625339508057}, {"from": 40, "to": 53, "width": 0.712295413017273}, {"from": 40, "to": 61, "width": 0.7071850895881653}, {"from": 41, "to": 51, "width": 0.7390837669372559}, {"from": 41, "to": 53, "width": 0.7406355142593384}, {"from": 41, "to": 57, "width": 0.7214914560317993}, {"from": 41, "to": 70, "width": 0.710145115852356}, {"from": 41, "to": 92, "width": 0.7024832963943481}, {"from": 41, "to": 102, "width": 0.7119154930114746}, {"from": 41, "to": 108, "width": 0.7121406197547913}, {"from": 41, "to": 117, "width": 0.7478221654891968}, {"from": 41, "to": 130, "width": 0.7744739055633545}, {"from": 42, "to": 48, "width": 0.7118293642997742}, {"from": 42, "to": 70, "width": 0.7023767232894897}, {"from": 42, "to": 82, "width": 0.7687289714813232}, {"from": 42, "to": 84, "width": 0.8404994010925293}, {"from": 42, "to": 86, "width": 0.7681596279144287}, {"from": 42, "to": 87, "width": 0.7013818621635437}, {"from": 42, "to": 88, "width": 0.7215526103973389}, {"from": 42, "to": 90, "width": 0.7830960750579834}, {"from": 42, "to": 91, "width": 0.7918583750724792}, {"from": 43, "to": 82, "width": 0.7186394929885864}, {"from": 44, "to": 51, "width": 0.7094014286994934}, {"from": 44, "to": 55, "width": 0.7160318493843079}, {"from": 44, "to": 56, "width": 0.7105415463447571}, {"from": 45, "to": 92, "width": 0.7293002605438232}, {"from": 45, "to": 99, "width": 0.7515386343002319}, {"from": 45, "to": 101, "width": 0.7216913104057312}, {"from": 45, "to": 104, "width": 0.7513368725776672}, {"from": 45, "to": 106, "width": 0.7332254648208618}, {"from": 45, "to": 108, "width": 0.8040933609008789}, {"from": 45, "to": 109, "width": 0.7297396659851074}, {"from": 45, "to": 110, "width": 0.8153951168060303}, {"from": 45, "to": 112, "width": 0.7503383159637451}, {"from": 45, "to": 113, "width": 0.7038131356239319}, {"from": 45, "to": 114, "width": 0.8744550943374634}, {"from": 45, "to": 126, "width": 0.8579776883125305}, {"from": 45, "to": 130, "width": 0.7146698236465454}, {"from": 45, "to": 137, "width": 0.7142549753189087}, {"from": 46, "to": 48, "width": 0.7232282161712646}, {"from": 46, "to": 49, "width": 0.7247250080108643}, {"from": 46, "to": 51, "width": 0.7555637359619141}, {"from": 46, "to": 52, "width": 0.7536838054656982}, {"from": 46, "to": 53, "width": 0.7661967873573303}, {"from": 46, "to": 54, "width": 0.7264665365219116}, {"from": 46, "to": 57, "width": 0.778120756149292}, {"from": 46, "to": 62, "width": 0.7135295271873474}, {"from": 46, "to": 107, "width": 0.7009596228599548}, {"from": 46, "to": 116, "width": 0.707461953163147}, {"from": 46, "to": 125, "width": 0.729658305644989}, {"from": 46, "to": 127, "width": 0.7050009369850159}, {"from": 46, "to": 128, "width": 0.7582141160964966}, {"from": 47, "to": 51, "width": 0.7167136669158936}, {"from": 47, "to": 54, "width": 0.7014763951301575}, {"from": 47, "to": 73, "width": 0.7315487861633301}, {"from": 47, "to": 76, "width": 0.7748080492019653}, {"from": 47, "to": 116, "width": 0.7254194021224976}, {"from": 47, "to": 122, "width": 0.7206205725669861}, {"from": 47, "to": 127, "width": 0.7324820756912231}, {"from": 47, "to": 137, "width": 0.7464410066604614}, {"from": 48, "to": 57, "width": 0.803331732749939}, {"from": 48, "to": 84, "width": 0.7836121320724487}, {"from": 48, "to": 94, "width": 0.7667856812477112}, {"from": 48, "to": 95, "width": 0.7116173505783081}, {"from": 48, "to": 102, "width": 0.710058331489563}, {"from": 48, "to": 117, "width": 0.7524573802947998}, {"from": 48, "to": 125, "width": 0.7066118717193604}, {"from": 49, "to": 50, "width": 0.7994542121887207}, {"from": 49, "to": 51, "width": 0.8008384704589844}, {"from": 49, "to": 52, "width": 0.7406057119369507}, {"from": 49, "to": 54, "width": 0.8222202062606812}, {"from": 49, "to": 60, "width": 0.7083555459976196}, {"from": 49, "to": 116, "width": 0.7484586238861084}, {"from": 50, "to": 51, "width": 0.7907932996749878}, {"from": 50, "to": 54, "width": 0.7551262974739075}, {"from": 50, "to": 60, "width": 0.7333279252052307}, {"from": 50, "to": 76, "width": 0.7753260731697083}, {"from": 51, "to": 52, "width": 0.7262370586395264}, {"from": 51, "to": 53, "width": 0.800210177898407}, {"from": 51, "to": 54, "width": 0.7732577323913574}, {"from": 51, "to": 55, "width": 0.7187834978103638}, {"from": 51, "to": 56, "width": 0.718724250793457}, {"from": 51, "to": 57, "width": 0.735183596611023}, {"from": 51, "to": 60, "width": 0.7538054585456848}, {"from": 51, "to": 62, "width": 0.7047702074050903}, {"from": 51, "to": 63, "width": 0.729382336139679}, {"from": 51, "to": 70, "width": 0.7542262077331543}, {"from": 51, "to": 73, "width": 0.7213284969329834}, {"from": 51, "to": 76, "width": 0.7207605242729187}, {"from": 51, "to": 107, "width": 0.7143603563308716}, {"from": 51, "to": 116, "width": 0.7370860576629639}, {"from": 51, "to": 127, "width": 0.7086479663848877}, {"from": 52, "to": 54, "width": 0.7374472618103027}, {"from": 52, "to": 55, "width": 0.7528505921363831}, {"from": 52, "to": 57, "width": 0.7236196994781494}, {"from": 52, "to": 58, "width": 0.73881995677948}, {"from": 52, "to": 60, "width": 0.7241145372390747}, {"from": 52, "to": 64, "width": 0.7205634117126465}, {"from": 52, "to": 68, "width": 0.701379120349884}, {"from": 52, "to": 73, "width": 0.7010310888290405}, {"from": 52, "to": 76, "width": 0.7047104239463806}, {"from": 52, "to": 93, "width": 0.7298529148101807}, {"from": 52, "to": 95, "width": 0.7202593088150024}, {"from": 52, "to": 107, "width": 0.704167366027832}, {"from": 52, "to": 116, "width": 0.7188481092453003}, {"from": 52, "to": 127, "width": 0.7080790996551514}, {"from": 52, "to": 137, "width": 0.7048180103302002}, {"from": 53, "to": 57, "width": 0.7083780765533447}, {"from": 53, "to": 128, "width": 0.7515603303909302}, {"from": 54, "to": 60, "width": 0.7102195024490356}, {"from": 54, "to": 76, "width": 0.7001781463623047}, {"from": 54, "to": 95, "width": 0.7075076699256897}, {"from": 54, "to": 105, "width": 0.7216002941131592}, {"from": 54, "to": 116, "width": 0.8495575189590454}, {"from": 54, "to": 127, "width": 0.7617943286895752}, {"from": 55, "to": 56, "width": 0.761998176574707}, {"from": 55, "to": 63, "width": 0.7223145365715027}, {"from": 56, "to": 127, "width": 0.7178830504417419}, {"from": 57, "to": 58, "width": 0.7061018943786621}, {"from": 57, "to": 84, "width": 0.7015620470046997}, {"from": 57, "to": 94, "width": 0.7164757251739502}, {"from": 57, "to": 102, "width": 0.7530544996261597}, {"from": 57, "to": 117, "width": 0.7621805667877197}, {"from": 57, "to": 125, "width": 0.7123311758041382}, {"from": 58, "to": 60, "width": 0.7203693389892578}, {"from": 58, "to": 62, "width": 0.7367138266563416}, {"from": 58, "to": 63, "width": 0.7589501142501831}, {"from": 58, "to": 67, "width": 0.745527982711792}, {"from": 58, "to": 68, "width": 0.7735052108764648}, {"from": 58, "to": 71, "width": 0.7209477424621582}, {"from": 58, "to": 74, "width": 0.7624365091323853}, {"from": 58, "to": 76, "width": 0.7577157020568848}, {"from": 58, "to": 78, "width": 0.7310448884963989}, {"from": 58, "to": 97, "width": 0.729327917098999}, {"from": 58, "to": 101, "width": 0.7768788933753967}, {"from": 58, "to": 112, "width": 0.7042739987373352}, {"from": 58, "to": 122, "width": 0.7146980166435242}, {"from": 58, "to": 127, "width": 0.7218236923217773}, {"from": 58, "to": 137, "width": 0.7799807190895081}, {"from": 59, "to": 63, "width": 0.7271081209182739}, {"from": 59, "to": 80, "width": 0.7285193204879761}, {"from": 60, "to": 62, "width": 0.7431961297988892}, {"from": 60, "to": 63, "width": 0.7004119753837585}, {"from": 60, "to": 68, "width": 0.7317364811897278}, {"from": 60, "to": 70, "width": 0.7070682048797607}, {"from": 60, "to": 73, "width": 0.7271071672439575}, {"from": 60, "to": 76, "width": 0.7099461555480957}, {"from": 60, "to": 84, "width": 0.7091506123542786}, {"from": 60, "to": 90, "width": 0.7088344097137451}, {"from": 60, "to": 100, "width": 0.7001360058784485}, {"from": 60, "to": 101, "width": 0.704552412033081}, {"from": 60, "to": 102, "width": 0.7130482792854309}, {"from": 60, "to": 116, "width": 0.706976056098938}, {"from": 60, "to": 127, "width": 0.7104047536849976}, {"from": 60, "to": 137, "width": 0.7170730829238892}, {"from": 61, "to": 70, "width": 0.7152623534202576}, {"from": 61, "to": 71, "width": 0.7021052837371826}, {"from": 61, "to": 98, "width": 0.7113417983055115}, {"from": 62, "to": 63, "width": 0.736111044883728}, {"from": 62, "to": 67, "width": 0.7439368963241577}, {"from": 62, "to": 68, "width": 0.7769271731376648}, {"from": 62, "to": 70, "width": 0.7637348175048828}, {"from": 62, "to": 73, "width": 0.7117538452148438}, {"from": 62, "to": 93, "width": 0.772109866142273}, {"from": 62, "to": 94, "width": 0.8143763542175293}, {"from": 62, "to": 95, "width": 0.7267139554023743}, {"from": 62, "to": 97, "width": 0.7850549221038818}, {"from": 62, "to": 98, "width": 0.7864192128181458}, {"from": 62, "to": 101, "width": 0.7446064949035645}, {"from": 62, "to": 104, "width": 0.7453151941299438}, {"from": 62, "to": 105, "width": 0.7417538166046143}, {"from": 62, "to": 107, "width": 0.784820556640625}, {"from": 62, "to": 108, "width": 0.733405590057373}, {"from": 62, "to": 109, "width": 0.7445773482322693}, {"from": 62, "to": 111, "width": 0.7177084684371948}, {"from": 62, "to": 112, "width": 0.7129960656166077}, {"from": 62, "to": 113, "width": 0.7477098107337952}, {"from": 62, "to": 115, "width": 0.7294228672981262}, {"from": 62, "to": 116, "width": 0.7240085601806641}, {"from": 62, "to": 117, "width": 0.7087442874908447}, {"from": 62, "to": 122, "width": 0.7054359912872314}, {"from": 62, "to": 124, "width": 0.7397679686546326}, {"from": 62, "to": 125, "width": 0.7970428466796875}, {"from": 62, "to": 127, "width": 0.8050068616867065}, {"from": 62, "to": 128, "width": 0.7543681859970093}, {"from": 62, "to": 129, "width": 0.7366505861282349}, {"from": 62, "to": 137, "width": 0.7046990990638733}, {"from": 63, "to": 64, "width": 0.7529121041297913}, {"from": 63, "to": 67, "width": 0.7392628192901611}, {"from": 63, "to": 68, "width": 0.7961956262588501}, {"from": 63, "to": 70, "width": 0.7157988548278809}, {"from": 63, "to": 71, "width": 0.7553671002388}, {"from": 63, "to": 73, "width": 0.7448136806488037}, {"from": 63, "to": 74, "width": 0.7260719537734985}, {"from": 63, "to": 78, "width": 0.7847576141357422}, {"from": 63, "to": 97, "width": 0.7071855068206787}, {"from": 63, "to": 101, "width": 0.7301664352416992}, {"from": 63, "to": 104, "width": 0.7013918161392212}, {"from": 63, "to": 127, "width": 0.7473222017288208}, {"from": 63, "to": 137, "width": 0.748114824295044}, {"from": 64, "to": 68, "width": 0.7532979249954224}, {"from": 64, "to": 73, "width": 0.7365015149116516}, {"from": 64, "to": 78, "width": 0.749273955821991}, {"from": 64, "to": 80, "width": 0.7775669097900391}, {"from": 64, "to": 81, "width": 0.7364354729652405}, {"from": 64, "to": 93, "width": 0.7865698933601379}, {"from": 64, "to": 97, "width": 0.707575261592865}, {"from": 64, "to": 101, "width": 0.7160347104072571}, {"from": 64, "to": 104, "width": 0.7305784225463867}, {"from": 64, "to": 112, "width": 0.7135865092277527}, {"from": 64, "to": 116, "width": 0.7193141579627991}, {"from": 64, "to": 124, "width": 0.7491727471351624}, {"from": 64, "to": 126, "width": 0.7196391820907593}, {"from": 64, "to": 129, "width": 0.7023813724517822}, {"from": 64, "to": 132, "width": 0.7044243812561035}, {"from": 64, "to": 137, "width": 0.750015914440155}, {"from": 64, "to": 138, "width": 0.7421591281890869}, {"from": 66, "to": 67, "width": 0.7050133943557739}, {"from": 67, "to": 71, "width": 0.7142924070358276}, {"from": 67, "to": 73, "width": 0.7558610439300537}, {"from": 67, "to": 74, "width": 0.7039270997047424}, {"from": 67, "to": 78, "width": 0.710468053817749}, {"from": 67, "to": 101, "width": 0.7305218577384949}, {"from": 67, "to": 122, "width": 0.7151661515235901}, {"from": 67, "to": 137, "width": 0.770594596862793}, {"from": 68, "to": 73, "width": 0.7192343473434448}, {"from": 68, "to": 93, "width": 0.7567562460899353}, {"from": 68, "to": 94, "width": 0.7755048274993896}, {"from": 68, "to": 95, "width": 0.7302523851394653}, {"from": 68, "to": 97, "width": 0.8013384938240051}, {"from": 68, "to": 98, "width": 0.8019227981567383}, {"from": 68, "to": 100, "width": 0.7026875019073486}, {"from": 68, "to": 101, "width": 0.7710058093070984}, {"from": 68, "to": 104, "width": 0.7381540536880493}, {"from": 68, "to": 105, "width": 0.7477624416351318}, {"from": 68, "to": 116, "width": 0.7307285070419312}, {"from": 68, "to": 117, "width": 0.7001423239707947}, {"from": 68, "to": 122, "width": 0.7048980593681335}, {"from": 68, "to": 124, "width": 0.7460699081420898}, {"from": 68, "to": 125, "width": 0.740577220916748}, {"from": 68, "to": 127, "width": 0.8242208361625671}, {"from": 68, "to": 128, "width": 0.7050161361694336}, {"from": 68, "to": 129, "width": 0.7792807817459106}, {"from": 68, "to": 137, "width": 0.7102223634719849}, {"from": 69, "to": 74, "width": 0.701043426990509}, {"from": 70, "to": 98, "width": 0.7188480496406555}, {"from": 71, "to": 72, "width": 0.705062985420227}, {"from": 71, "to": 78, "width": 0.7028306126594543}, {"from": 71, "to": 102, "width": 0.7159470319747925}, {"from": 71, "to": 137, "width": 0.7551974654197693}, {"from": 73, "to": 76, "width": 0.7792521715164185}, {"from": 73, "to": 78, "width": 0.7007768750190735}, {"from": 73, "to": 86, "width": 0.7138175964355469}, {"from": 73, "to": 90, "width": 0.7045145630836487}, {"from": 73, "to": 101, "width": 0.7682740092277527}, {"from": 73, "to": 122, "width": 0.7220749258995056}, {"from": 73, "to": 137, "width": 0.8207584619522095}, {"from": 74, "to": 76, "width": 0.7049860954284668}, {"from": 74, "to": 77, "width": 0.7481885552406311}, {"from": 74, "to": 78, "width": 0.7229548692703247}, {"from": 74, "to": 79, "width": 0.7164911031723022}, {"from": 74, "to": 101, "width": 0.7052206993103027}, {"from": 74, "to": 122, "width": 0.7092055082321167}, {"from": 74, "to": 136, "width": 0.7040508985519409}, {"from": 74, "to": 137, "width": 0.7528213858604431}, {"from": 76, "to": 101, "width": 0.7828956842422485}, {"from": 76, "to": 105, "width": 0.7100271582603455}, {"from": 76, "to": 112, "width": 0.7069625854492188}, {"from": 76, "to": 127, "width": 0.7010798454284668}, {"from": 76, "to": 137, "width": 0.7966387271881104}, {"from": 77, "to": 79, "width": 0.7808279991149902}, {"from": 77, "to": 136, "width": 0.7293280363082886}, {"from": 78, "to": 97, "width": 0.710435152053833}, {"from": 78, "to": 101, "width": 0.7501058578491211}, {"from": 78, "to": 137, "width": 0.7689639329910278}, {"from": 79, "to": 136, "width": 0.7959653735160828}, {"from": 80, "to": 93, "width": 0.7656398415565491}, {"from": 80, "to": 104, "width": 0.7332523465156555}, {"from": 80, "to": 124, "width": 0.7337628602981567}, {"from": 80, "to": 129, "width": 0.7015713453292847}, {"from": 81, "to": 83, "width": 0.7340998649597168}, {"from": 81, "to": 86, "width": 0.7453242540359497}, {"from": 81, "to": 87, "width": 0.7591554522514343}, {"from": 81, "to": 89, "width": 0.726700484752655}, {"from": 81, "to": 90, "width": 0.7138795852661133}, {"from": 81, "to": 91, "width": 0.7593024969100952}, {"from": 82, "to": 86, "width": 0.7035567164421082}, {"from": 82, "to": 88, "width": 0.7261300683021545}, {"from": 82, "to": 89, "width": 0.7495569586753845}, {"from": 83, "to": 86, "width": 0.7276027798652649}, {"from": 83, "to": 87, "width": 0.7921380996704102}, {"from": 83, "to": 88, "width": 0.7127244472503662}, {"from": 83, "to": 90, "width": 0.724618673324585}, {"from": 83, "to": 91, "width": 0.7037667036056519}, {"from": 84, "to": 90, "width": 0.7610797882080078}, {"from": 84, "to": 91, "width": 0.7467551231384277}, {"from": 84, "to": 94, "width": 0.713555097579956}, {"from": 84, "to": 95, "width": 0.7052808403968811}, {"from": 85, "to": 120, "width": 0.745719313621521}, {"from": 85, "to": 141, "width": 0.7020924091339111}, {"from": 86, "to": 87, "width": 0.8011605739593506}, {"from": 86, "to": 88, "width": 0.7210111021995544}, {"from": 86, "to": 90, "width": 0.7818982601165771}, {"from": 86, "to": 91, "width": 0.7279808521270752}, {"from": 87, "to": 88, "width": 0.8512641787528992}, {"from": 87, "to": 90, "width": 0.8074416518211365}, {"from": 87, "to": 91, "width": 0.7302542924880981}, {"from": 88, "to": 90, "width": 0.8068811893463135}, {"from": 90, "to": 91, "width": 0.7660998702049255}, {"from": 92, "to": 94, "width": 0.7123510837554932}, {"from": 92, "to": 99, "width": 0.7551088929176331}, {"from": 92, "to": 101, "width": 0.7017346620559692}, {"from": 92, "to": 103, "width": 0.7756584882736206}, {"from": 92, "to": 104, "width": 0.7738005518913269}, {"from": 92, "to": 106, "width": 0.772347092628479}, {"from": 92, "to": 107, "width": 0.7199366092681885}, {"from": 92, "to": 108, "width": 0.8173576593399048}, {"from": 92, "to": 109, "width": 0.773980438709259}, {"from": 92, "to": 110, "width": 0.8404439687728882}, {"from": 92, "to": 111, "width": 0.7068144679069519}, {"from": 92, "to": 112, "width": 0.7007403373718262}, {"from": 92, "to": 114, "width": 0.7907136082649231}, {"from": 92, "to": 117, "width": 0.7175083756446838}, {"from": 92, "to": 118, "width": 0.7735318541526794}, {"from": 92, "to": 126, "width": 0.7778846025466919}, {"from": 92, "to": 130, "width": 0.8577396869659424}, {"from": 93, "to": 94, "width": 0.747325599193573}, {"from": 93, "to": 95, "width": 0.7683426141738892}, {"from": 93, "to": 96, "width": 0.7131489515304565}, {"from": 93, "to": 97, "width": 0.7859205007553101}, {"from": 93, "to": 98, "width": 0.7451993823051453}, {"from": 93, "to": 99, "width": 0.7311291694641113}, {"from": 93, "to": 101, "width": 0.7727802395820618}, {"from": 93, "to": 104, "width": 0.7948238849639893}, {"from": 93, "to": 105, "width": 0.7440242171287537}, {"from": 93, "to": 108, "width": 0.7753572463989258}, {"from": 93, "to": 112, "width": 0.7376694679260254}, {"from": 93, "to": 115, "width": 0.7330875396728516}, {"from": 93, "to": 116, "width": 0.7412961721420288}, {"from": 93, "to": 124, "width": 0.8580984473228455}, {"from": 93, "to": 125, "width": 0.7238091230392456}, {"from": 93, "to": 126, "width": 0.7463140487670898}, {"from": 93, "to": 127, "width": 0.7624117136001587}, {"from": 93, "to": 129, "width": 0.7708228826522827}, {"from": 93, "to": 137, "width": 0.719133734703064}, {"from": 94, "to": 95, "width": 0.778450071811676}, {"from": 94, "to": 96, "width": 0.706877589225769}, {"from": 94, "to": 97, "width": 0.8057806491851807}, {"from": 94, "to": 98, "width": 0.8432412147521973}, {"from": 94, "to": 99, "width": 0.7209391593933105}, {"from": 94, "to": 101, "width": 0.7836222052574158}, {"from": 94, "to": 102, "width": 0.7405092716217041}, {"from": 94, "to": 103, "width": 0.7093777656555176}, {"from": 94, "to": 104, "width": 0.7963903546333313}, {"from": 94, "to": 105, "width": 0.8011251091957092}, {"from": 94, "to": 107, "width": 0.8209438920021057}, {"from": 94, "to": 108, "width": 0.7716848254203796}, {"from": 94, "to": 109, "width": 0.8317272663116455}, {"from": 94, "to": 111, "width": 0.7345390319824219}, {"from": 94, "to": 112, "width": 0.7386332154273987}, {"from": 94, "to": 113, "width": 0.7427455186843872}, {"from": 94, "to": 115, "width": 0.735254168510437}, {"from": 94, "to": 116, "width": 0.7275734543800354}, {"from": 94, "to": 117, "width": 0.7698236703872681}, {"from": 94, "to": 124, "width": 0.7534219622612}, {"from": 94, "to": 125, "width": 0.851079523563385}, {"from": 94, "to": 126, "width": 0.7080521583557129}, {"from": 94, "to": 127, "width": 0.7811648845672607}, {"from": 94, "to": 128, "width": 0.7914223074913025}, {"from": 94, "to": 129, "width": 0.7063959836959839}, {"from": 94, "to": 130, "width": 0.724089503288269}, {"from": 95, "to": 96, "width": 0.7744712829589844}, {"from": 95, "to": 97, "width": 0.7138177752494812}, {"from": 95, "to": 98, "width": 0.7271198034286499}, {"from": 95, "to": 99, "width": 0.7833350300788879}, {"from": 95, "to": 101, "width": 0.7798477411270142}, {"from": 95, "to": 104, "width": 0.7618678212165833}, {"from": 95, "to": 105, "width": 0.7466657757759094}, {"from": 95, "to": 106, "width": 0.7131404876708984}, {"from": 95, "to": 108, "width": 0.7902716994285583}, {"from": 95, "to": 109, "width": 0.7631497383117676}, {"from": 95, "to": 112, "width": 0.7926624417304993}, {"from": 95, "to": 113, "width": 0.7409233450889587}, {"from": 95, "to": 115, "width": 0.7482531070709229}, {"from": 95, "to": 116, "width": 0.7367894053459167}, {"from": 95, "to": 117, "width": 0.7124854326248169}, {"from": 95, "to": 124, "width": 0.7603203058242798}, {"from": 95, "to": 125, "width": 0.7504894137382507}, {"from": 95, "to": 126, "width": 0.7103990316390991}, {"from": 95, "to": 127, "width": 0.7495739459991455}, {"from": 95, "to": 129, "width": 0.7753938436508179}, {"from": 95, "to": 137, "width": 0.7304009199142456}, {"from": 96, "to": 99, "width": 0.7786848545074463}, {"from": 96, "to": 101, "width": 0.7633273601531982}, {"from": 96, "to": 103, "width": 0.793022632598877}, {"from": 96, "to": 104, "width": 0.7241901755332947}, {"from": 96, "to": 105, "width": 0.7328457832336426}, {"from": 96, "to": 106, "width": 0.7045326232910156}, {"from": 96, "to": 108, "width": 0.7702864408493042}, {"from": 96, "to": 109, "width": 0.7269734144210815}, {"from": 96, "to": 112, "width": 0.7720139026641846}, {"from": 96, "to": 113, "width": 0.7013580203056335}, {"from": 96, "to": 114, "width": 0.7091261744499207}, {"from": 96, "to": 124, "width": 0.7481597065925598}, {"from": 96, "to": 126, "width": 0.7596681714057922}, {"from": 96, "to": 137, "width": 0.709896981716156}, {"from": 97, "to": 98, "width": 0.8154075145721436}, {"from": 97, "to": 100, "width": 0.7423717975616455}, {"from": 97, "to": 101, "width": 0.7869279384613037}, {"from": 97, "to": 104, "width": 0.792475163936615}, {"from": 97, "to": 105, "width": 0.7609844207763672}, {"from": 97, "to": 107, "width": 0.7362521886825562}, {"from": 97, "to": 108, "width": 0.7226247191429138}, {"from": 97, "to": 109, "width": 0.7465204000473022}, {"from": 97, "to": 111, "width": 0.7371490001678467}, {"from": 97, "to": 112, "width": 0.7474266290664673}, {"from": 97, "to": 113, "width": 0.7188489437103271}, {"from": 97, "to": 116, "width": 0.7298163175582886}, {"from": 97, "to": 122, "width": 0.7307863235473633}, {"from": 97, "to": 124, "width": 0.8174095153808594}, {"from": 97, "to": 125, "width": 0.7795004844665527}, {"from": 97, "to": 126, "width": 0.7343896627426147}, {"from": 97, "to": 127, "width": 0.8040249347686768}, {"from": 97, "to": 128, "width": 0.7789044976234436}, {"from": 97, "to": 129, "width": 0.7511597275733948}, {"from": 97, "to": 137, "width": 0.7447668313980103}, {"from": 98, "to": 101, "width": 0.7723061442375183}, {"from": 98, "to": 104, "width": 0.7860400080680847}, {"from": 98, "to": 105, "width": 0.7644340991973877}, {"from": 98, "to": 107, "width": 0.7588823437690735}, {"from": 98, "to": 108, "width": 0.7356704473495483}, {"from": 98, "to": 109, "width": 0.7915173172950745}, {"from": 98, "to": 111, "width": 0.7476900219917297}, {"from": 98, "to": 112, "width": 0.7225000858306885}, {"from": 98, "to": 116, "width": 0.7248072624206543}, {"from": 98, "to": 117, "width": 0.7477481365203857}, {"from": 98, "to": 124, "width": 0.7899052500724792}, {"from": 98, "to": 125, "width": 0.854816198348999}, {"from": 98, "to": 126, "width": 0.7078016400337219}, {"from": 98, "to": 127, "width": 0.8230017423629761}, {"from": 98, "to": 128, "width": 0.8108702898025513}, {"from": 98, "to": 129, "width": 0.789667010307312}, {"from": 98, "to": 130, "width": 0.731959342956543}, {"from": 99, "to": 101, "width": 0.807742714881897}, {"from": 99, "to": 103, "width": 0.730126142501831}, {"from": 99, "to": 104, "width": 0.7972953915596008}, {"from": 99, "to": 105, "width": 0.7231224179267883}, {"from": 99, "to": 106, "width": 0.8397080898284912}, {"from": 99, "to": 108, "width": 0.9203078746795654}, {"from": 99, "to": 109, "width": 0.7994146347045898}, {"from": 99, "to": 110, "width": 0.7588585615158081}, {"from": 99, "to": 112, "width": 0.8359150290489197}, {"from": 99, "to": 113, "width": 0.8105937242507935}, {"from": 99, "to": 114, "width": 0.8307064175605774}, {"from": 99, "to": 115, "width": 0.7632439136505127}, {"from": 99, "to": 118, "width": 0.7344117760658264}, {"from": 99, "to": 124, "width": 0.7302635908126831}, {"from": 99, "to": 126, "width": 0.833457350730896}, {"from": 99, "to": 130, "width": 0.7357759475708008}, {"from": 99, "to": 137, "width": 0.7750790119171143}, {"from": 100, "to": 124, "width": 0.7086647748947144}, {"from": 100, "to": 127, "width": 0.7524768114089966}, {"from": 100, "to": 129, "width": 0.7112369537353516}, {"from": 101, "to": 103, "width": 0.7153466939926147}, {"from": 101, "to": 104, "width": 0.8828290700912476}, {"from": 101, "to": 105, "width": 0.7778351306915283}, {"from": 101, "to": 106, "width": 0.7606770396232605}, {"from": 101, "to": 107, "width": 0.7146788835525513}, {"from": 101, "to": 108, "width": 0.8380860090255737}, {"from": 101, "to": 109, "width": 0.7581243515014648}, {"from": 101, "to": 112, "width": 0.8211244344711304}, {"from": 101, "to": 113, "width": 0.7814621925354004}, {"from": 101, "to": 114, "width": 0.7221502065658569}, {"from": 101, "to": 115, "width": 0.7507532835006714}, {"from": 101, "to": 116, "width": 0.7040233612060547}, {"from": 101, "to": 117, "width": 0.7169176340103149}, {"from": 101, "to": 122, "width": 0.71685791015625}, {"from": 101, "to": 124, "width": 0.7765820622444153}, {"from": 101, "to": 125, "width": 0.7139793038368225}, {"from": 101, "to": 126, "width": 0.8257356882095337}, {"from": 101, "to": 127, "width": 0.766205370426178}, {"from": 101, "to": 129, "width": 0.7923606634140015}, {"from": 101, "to": 130, "width": 0.7270970940589905}, {"from": 101, "to": 137, "width": 0.8773361444473267}, {"from": 102, "to": 107, "width": 0.7499619126319885}, {"from": 102, "to": 108, "width": 0.7093400359153748}, {"from": 102, "to": 117, "width": 0.8461048603057861}, {"from": 102, "to": 125, "width": 0.7053548693656921}, {"from": 103, "to": 104, "width": 0.7355199456214905}, {"from": 103, "to": 105, "width": 0.7227270007133484}, {"from": 103, "to": 108, "width": 0.7514415979385376}, {"from": 103, "to": 110, "width": 0.7477633953094482}, {"from": 103, "to": 114, "width": 0.7430648803710938}, {"from": 103, "to": 118, "width": 0.7395881414413452}, {"from": 103, "to": 124, "width": 0.7466030120849609}, {"from": 103, "to": 126, "width": 0.7402058839797974}, {"from": 103, "to": 130, "width": 0.7247060537338257}, {"from": 104, "to": 105, "width": 0.7461163997650146}, {"from": 104, "to": 106, "width": 0.7452203035354614}, {"from": 104, "to": 107, "width": 0.7396352291107178}, {"from": 104, "to": 108, "width": 0.8512634634971619}, {"from": 104, "to": 109, "width": 0.8186218738555908}, {"from": 104, "to": 110, "width": 0.7245113253593445}, {"from": 104, "to": 111, "width": 0.7261234521865845}, {"from": 104, "to": 112, "width": 0.8419405221939087}, {"from": 104, "to": 113, "width": 0.7957305908203125}, {"from": 104, "to": 114, "width": 0.7571151256561279}, {"from": 104, "to": 115, "width": 0.7483978271484375}, {"from": 104, "to": 116, "width": 0.7336270213127136}, {"from": 104, "to": 117, "width": 0.7267707586288452}, {"from": 104, "to": 124, "width": 0.8076871037483215}, {"from": 104, "to": 125, "width": 0.7087482213973999}, {"from": 104, "to": 126, "width": 0.8503503203392029}, {"from": 104, "to": 127, "width": 0.7514175176620483}, {"from": 104, "to": 129, "width": 0.7833113670349121}, {"from": 104, "to": 130, "width": 0.7564657330513}, {"from": 104, "to": 137, "width": 0.7653106451034546}, {"from": 105, "to": 107, "width": 0.7413761615753174}, {"from": 105, "to": 108, "width": 0.7660897970199585}, {"from": 105, "to": 109, "width": 0.7523418664932251}, {"from": 105, "to": 112, "width": 0.7262831330299377}, {"from": 105, "to": 113, "width": 0.7267591953277588}, {"from": 105, "to": 115, "width": 0.7614654302597046}, {"from": 105, "to": 116, "width": 0.8024418354034424}, {"from": 105, "to": 122, "width": 0.7274957299232483}, {"from": 105, "to": 124, "width": 0.7493842840194702}, {"from": 105, "to": 125, "width": 0.7633288502693176}, {"from": 105, "to": 126, "width": 0.7555273175239563}, {"from": 105, "to": 127, "width": 0.7900547385215759}, {"from": 105, "to": 129, "width": 0.7419529557228088}, {"from": 105, "to": 137, "width": 0.7463515996932983}, {"from": 106, "to": 108, "width": 0.856594979763031}, {"from": 106, "to": 109, "width": 0.7657994627952576}, {"from": 106, "to": 110, "width": 0.7939355373382568}, {"from": 106, "to": 112, "width": 0.8014197945594788}, {"from": 106, "to": 113, "width": 0.8391780853271484}, {"from": 106, "to": 114, "width": 0.762406587600708}, {"from": 106, "to": 118, "width": 0.7967337369918823}, {"from": 106, "to": 126, "width": 0.7912477254867554}, {"from": 106, "to": 130, "width": 0.7473931312561035}, {"from": 106, "to": 137, "width": 0.7668715715408325}, {"from": 107, "to": 108, "width": 0.7114542722702026}, {"from": 107, "to": 109, "width": 0.7727623581886292}, {"from": 107, "to": 111, "width": 0.733327329158783}, {"from": 107, "to": 117, "width": 0.7154920697212219}, {"from": 107, "to": 118, "width": 0.7195197939872742}, {"from": 107, "to": 125, "width": 0.8112356662750244}, {"from": 107, "to": 127, "width": 0.7156270742416382}, {"from": 107, "to": 128, "width": 0.7660192847251892}, {"from": 107, "to": 130, "width": 0.7458900213241577}, {"from": 107, "to": 137, "width": 0.7077345252037048}, {"from": 108, "to": 109, "width": 0.8440232276916504}, {"from": 108, "to": 110, "width": 0.7949497699737549}, {"from": 108, "to": 112, "width": 0.8431126475334167}, {"from": 108, "to": 113, "width": 0.8436501026153564}, {"from": 108, "to": 114, "width": 0.8509750366210938}, {"from": 108, "to": 115, "width": 0.7702246904373169}, {"from": 108, "to": 117, "width": 0.7167390584945679}, {"from": 108, "to": 118, "width": 0.762645959854126}, {"from": 108, "to": 124, "width": 0.8017162084579468}, {"from": 108, "to": 125, "width": 0.7086632251739502}, {"from": 108, "to": 126, "width": 0.8825198411941528}, {"from": 108, "to": 127, "width": 0.7003074884414673}, {"from": 108, "to": 129, "width": 0.7183736562728882}, {"from": 108, "to": 130, "width": 0.8199609518051147}, {"from": 108, "to": 137, "width": 0.7892476320266724}, {"from": 109, "to": 110, "width": 0.7198808789253235}, {"from": 109, "to": 111, "width": 0.7181538343429565}, {"from": 109, "to": 112, "width": 0.8243402242660522}, {"from": 109, "to": 113, "width": 0.8072310090065002}, {"from": 109, "to": 114, "width": 0.7614383697509766}, {"from": 109, "to": 115, "width": 0.7375098466873169}, {"from": 109, "to": 118, "width": 0.7395812273025513}, {"from": 109, "to": 124, "width": 0.701221227645874}, {"from": 109, "to": 125, "width": 0.7806636095046997}, {"from": 109, "to": 126, "width": 0.8080894947052002}, {"from": 109, "to": 127, "width": 0.7156380414962769}, {"from": 109, "to": 128, "width": 0.7551510334014893}, {"from": 109, "to": 130, "width": 0.7706724405288696}, {"from": 109, "to": 137, "width": 0.7081249952316284}, {"from": 110, "to": 112, "width": 0.7507020235061646}, {"from": 110, "to": 114, "width": 0.8328678607940674}, {"from": 110, "to": 118, "width": 0.7770920395851135}, {"from": 110, "to": 126, "width": 0.8060896992683411}, {"from": 110, "to": 130, "width": 0.7339702844619751}, {"from": 111, "to": 124, "width": 0.7514179944992065}, {"from": 111, "to": 125, "width": 0.7832058668136597}, {"from": 111, "to": 127, "width": 0.7234912514686584}, {"from": 111, "to": 128, "width": 0.7695050239562988}, {"from": 112, "to": 113, "width": 0.865892231464386}, {"from": 112, "to": 114, "width": 0.753362774848938}, {"from": 112, "to": 115, "width": 0.7177491188049316}, {"from": 112, "to": 124, "width": 0.7293418645858765}, {"from": 112, "to": 126, "width": 0.8482687473297119}, {"from": 112, "to": 127, "width": 0.7237813472747803}, {"from": 112, "to": 129, "width": 0.729101300239563}, {"from": 112, "to": 137, "width": 0.7657794952392578}, {"from": 112, "to": 142, "width": 0.7023961544036865}, {"from": 113, "to": 115, "width": 0.7579208612442017}, {"from": 113, "to": 126, "width": 0.7874259352684021}, {"from": 113, "to": 137, "width": 0.7369340658187866}, {"from": 114, "to": 118, "width": 0.7174714803695679}, {"from": 114, "to": 124, "width": 0.713852047920227}, {"from": 114, "to": 126, "width": 0.9051536321640015}, {"from": 114, "to": 130, "width": 0.7435386776924133}, {"from": 114, "to": 137, "width": 0.7185484170913696}, {"from": 115, "to": 124, "width": 0.724079430103302}, {"from": 115, "to": 127, "width": 0.7530165910720825}, {"from": 115, "to": 129, "width": 0.7703731656074524}, {"from": 116, "to": 122, "width": 0.7016687393188477}, {"from": 116, "to": 125, "width": 0.7123210430145264}, {"from": 116, "to": 127, "width": 0.8061168789863586}, {"from": 116, "to": 128, "width": 0.7095562219619751}, {"from": 116, "to": 129, "width": 0.7311620116233826}, {"from": 117, "to": 125, "width": 0.729362964630127}, {"from": 117, "to": 130, "width": 0.7381438612937927}, {"from": 118, "to": 126, "width": 0.740505576133728}, {"from": 118, "to": 130, "width": 0.7591639757156372}, {"from": 118, "to": 137, "width": 0.7001991868019104}, {"from": 119, "to": 120, "width": 0.8341625332832336}, {"from": 119, "to": 121, "width": 0.9638517498970032}, {"from": 120, "to": 121, "width": 0.8162637948989868}, {"from": 122, "to": 125, "width": 0.7228603363037109}, {"from": 122, "to": 127, "width": 0.7635119557380676}, {"from": 122, "to": 128, "width": 0.7219244837760925}, {"from": 122, "to": 137, "width": 0.7673418521881104}, {"from": 123, "to": 128, "width": 0.7010921835899353}, {"from": 124, "to": 125, "width": 0.7266421914100647}, {"from": 124, "to": 126, "width": 0.7614327669143677}, {"from": 124, "to": 127, "width": 0.7625898122787476}, {"from": 124, "to": 129, "width": 0.7889226078987122}, {"from": 124, "to": 137, "width": 0.7203210592269897}, {"from": 125, "to": 127, "width": 0.7956493496894836}, {"from": 125, "to": 128, "width": 0.8674617409706116}, {"from": 125, "to": 129, "width": 0.7174007892608643}, {"from": 125, "to": 130, "width": 0.7109418511390686}, {"from": 126, "to": 130, "width": 0.7644693851470947}, {"from": 126, "to": 137, "width": 0.7931251525878906}, {"from": 127, "to": 128, "width": 0.7991901636123657}, {"from": 127, "to": 129, "width": 0.8320393562316895}, {"from": 127, "to": 137, "width": 0.7091888785362244}, {"from": 128, "to": 130, "width": 0.7085476517677307}, {"from": 131, "to": 132, "width": 0.7723121643066406}, {"from": 131, "to": 133, "width": 0.827274739742279}, {"from": 131, "to": 134, "width": 0.7581425905227661}, {"from": 131, "to": 139, "width": 0.8184090852737427}, {"from": 131, "to": 140, "width": 0.873248815536499}, {"from": 131, "to": 141, "width": 0.7611837983131409}, {"from": 131, "to": 142, "width": 0.7533776164054871}, {"from": 132, "to": 134, "width": 0.7158634662628174}, {"from": 132, "to": 138, "width": 0.7479819059371948}, {"from": 132, "to": 140, "width": 0.7611550092697144}, {"from": 132, "to": 141, "width": 0.8031483292579651}, {"from": 133, "to": 134, "width": 0.7603884935379028}, {"from": 133, "to": 139, "width": 0.8101636171340942}, {"from": 133, "to": 140, "width": 0.880033016204834}, {"from": 133, "to": 142, "width": 0.831248939037323}, {"from": 134, "to": 139, "width": 0.7783105373382568}, {"from": 134, "to": 140, "width": 0.7827219367027283}, {"from": 135, "to": 136, "width": 0.7335671186447144}, {"from": 135, "to": 138, "width": 0.74492347240448}, {"from": 139, "to": 140, "width": 0.8381903171539307}, {"from": 139, "to": 142, "width": 0.794669508934021}, {"from": 140, "to": 141, "width": 0.7519651651382446}, {"from": 140, "to": 142, "width": 0.7978265285491943}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"physics": {"barnesHut": {"avoidOverlap": 0.02}, "minVelocity": 0.75}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>